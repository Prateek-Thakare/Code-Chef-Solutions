{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object-detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prateek-Thakare/Code-Chef-Solutions/blob/master/Object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5INwLNEAhZXT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "088d5b55-01a6-43f5-dc95-ee46c0d1f94b"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Ld66vfTVNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91203790-bc3f-413e-cdab-cb342a860331"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/content/drive/sclera/models/research/object_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRpj0mVqSFT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ce07b0f-5eae-484c-fe37-7d43f6a47f08"
      },
      "source": [
        "!python train.py --logtosdterr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0707 16:45:27.456230 140263261390720 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0707 16:45:27.707548 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/mobilenet_v1.py:432: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0707 16:45:27.711444 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/slim-0.1-py3.6.egg/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0707 16:45:27.723149 140263261390720 deprecation_wrapper.py:119] From train.py:55: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0707 16:45:27.723304 140263261390720 deprecation_wrapper.py:119] From train.py:55: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0707 16:45:27.723788 140263261390720 deprecation_wrapper.py:119] From train.py:184: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0707 16:45:27.724204 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/absl/app.py:251: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W0707 16:45:27.724349 140263261390720 deprecation_wrapper.py:119] From train.py:90: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0707 16:45:27.724751 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/config_util.py:98: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0707 16:45:28.118793 140263261390720 deprecation_wrapper.py:119] From train.py:95: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W0707 16:45:28.145848 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W0707 16:45:28.151978 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:177: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0707 16:45:28.153169 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/data_decoders/tf_example_decoder.py:192: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0707 16:45:28.187771 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0707 16:45:28.203613 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0707 16:45:28.203758 140263261390720 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "W0707 16:45:28.213094 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0707 16:45:28.213269 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0707 16:45:28.247067 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0707 16:45:28.456629 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:43: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0707 16:45:28.463485 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0707 16:45:28.468346 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:512: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0707 16:45:28.523857 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:188: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0707 16:45:28.535909 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/box_list_ops.py:201: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0707 16:45:29.434726 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/batcher.py:96: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0707 16:45:29.439153 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0707 16:45:29.440325 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0707 16:45:29.449656 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0707 16:45:29.750208 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/preprocessor.py:2515: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0707 16:45:32.401260 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0707 16:45:32.401466 140263261390720 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0707 16:45:32.439488 140263261390720 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0707 16:45:32.476696 140263261390720 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0707 16:45:32.514005 140263261390720 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0707 16:45:32.551126 140263261390720 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0707 16:45:32.728863 140263261390720 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0707 16:45:34.375708 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:172: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0707 16:45:34.377176 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:178: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0707 16:45:34.650868 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:208: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W0707 16:45:34.651551 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:95: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0707 16:45:34.651786 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/learning_schedules.py:61: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W0707 16:45:34.661990 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0707 16:45:36.206981 140263261390720 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0707 16:45:37.540936 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0707 16:45:40.596487 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:353: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0707 16:45:40.816665 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:355: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W0707 16:45:40.819041 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:359: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W0707 16:45:40.822545 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:368: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0707 16:45:40.829053 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0707 16:45:41.561172 140263261390720 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/utils/variables_helper.py:134: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0707 16:45:41.566545 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.566716 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.566793 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.566892 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.566956 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.567023 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/BoxEncodingPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.567098 140263261390720 variables_helper.py:149] Variable [BoxPredictor_0/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[6]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.567164 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.567225 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.567284 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.567351 140263261390720 variables_helper.py:149] Variable [BoxPredictor_0/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 273]], model variable shape: [[1, 1, 512, 6]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.567411 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.567470 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.567528 140263261390720 variables_helper.py:152] Variable [BoxPredictor_0/ClassPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.567598 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.567660 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.567718 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.567783 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.567861 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.567923 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/BoxEncodingPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.568024 140263261390720 variables_helper.py:149] Variable [BoxPredictor_1/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.568086 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.568147 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.568218 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.568301 140263261390720 variables_helper.py:149] Variable [BoxPredictor_1/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1024, 546]], model variable shape: [[1, 1, 1024, 12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.568362 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.568422 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.568481 140263261390720 variables_helper.py:152] Variable [BoxPredictor_1/ClassPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.568546 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.568613 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.568673 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.568750 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.568806 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.568900 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/BoxEncodingPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.568967 140263261390720 variables_helper.py:149] Variable [BoxPredictor_2/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.569028 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.569088 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.569147 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.569216 140263261390720 variables_helper.py:149] Variable [BoxPredictor_2/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.569309 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.569367 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.569445 140263261390720 variables_helper.py:152] Variable [BoxPredictor_2/ClassPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.569512 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.569574 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.569642 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.569711 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.569773 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.569847 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/BoxEncodingPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.569917 140263261390720 variables_helper.py:149] Variable [BoxPredictor_3/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.569991 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.570050 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.570110 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.570178 140263261390720 variables_helper.py:149] Variable [BoxPredictor_3/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.570275 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.570338 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.570399 140263261390720 variables_helper.py:152] Variable [BoxPredictor_3/ClassPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.570479 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.570538 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.570602 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.570669 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.570729 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.570788 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/BoxEncodingPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.570867 140263261390720 variables_helper.py:149] Variable [BoxPredictor_4/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.570945 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.571027 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.571122 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.571196 140263261390720 variables_helper.py:149] Variable [BoxPredictor_4/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.571259 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.571321 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.571383 140263261390720 variables_helper.py:152] Variable [BoxPredictor_4/ClassPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.571450 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.571512 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.571573 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.571646 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.571707 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.571769 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/BoxEncodingPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.571850 140263261390720 variables_helper.py:149] Variable [BoxPredictor_5/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.571913 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/biases/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.571975 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/biases/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.572036 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/biases/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.572106 140263261390720 variables_helper.py:149] Variable [BoxPredictor_5/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 12]]. This variable will not be initialized from the checkpoint.\n",
            "W0707 16:45:41.572181 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.572241 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.572300 140263261390720 variables_helper.py:152] Variable [BoxPredictor_5/ClassPredictor/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.572365 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.572426 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.572486 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.572549 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.572615 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.572674 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.572746 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.572806 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.572878 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_0/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.572944 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.573699 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.573810 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.573921 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.573997 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.574070 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.574167 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.574264 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.574343 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.574457 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.574551 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.574660 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.574746 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.574874 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.574971 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.575067 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.575152 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.575224 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_10_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.575315 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.575380 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.575446 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.575551 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.575641 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.575714 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.575810 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.575923 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.576007 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.576097 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.576186 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.576267 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.576366 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.576446 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.576523 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.576663 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.576763 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.576861 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_11_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.576956 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.577037 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.577115 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.577249 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.577332 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.577417 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.577518 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.577616 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.577702 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.577795 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.577913 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.577986 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.578085 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.578179 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.578255 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.578349 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.578427 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.578502 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_12_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.578591 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.578698 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.578809 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.578928 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.579012 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.579123 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.579222 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.579304 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.579380 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.579466 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.579610 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.579697 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.579797 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.579901 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.579982 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.580079 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.580162 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.580260 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.580352 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.580433 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.580512 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.580609 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.580693 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.580772 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.580896 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.580987 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.581065 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.581154 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.581235 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.581314 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.581398 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.581477 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.581555 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.581667 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.581749 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.581845 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.581939 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.582019 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.582120 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.582211 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.582295 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.582376 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.582480 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.582569 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.582670 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.582764 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.582874 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.582964 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.583065 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.583148 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.583227 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.583324 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.583405 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.583483 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.583570 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.583665 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.583745 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.583868 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.583972 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.584049 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.584147 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.584231 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.584310 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.584394 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.584474 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.584565 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.584659 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.584754 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.584852 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.584956 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.585040 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.585120 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.585208 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.585290 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.585366 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.585470 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.585557 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.585654 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.585759 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.585879 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.585963 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.586047 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.586130 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.586208 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.586294 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.586374 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.586452 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.586547 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.586665 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.586755 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.586852 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.586955 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.587034 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.587120 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.587202 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.587279 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.587372 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.587454 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.587531 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.587637 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.587721 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.587801 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.587911 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.587993 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.588072 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.588169 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.588251 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.588328 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_1_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.588436 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.588529 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.588618 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.588709 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.588789 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.588891 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.588990 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.589074 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.589154 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.589239 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.589318 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.589396 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.589491 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.589596 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.589677 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.589776 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.589914 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.590016 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_2_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.590108 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.590195 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.590278 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.590368 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.590454 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.590538 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.590650 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.590751 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.590850 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.590942 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.591023 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.591104 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.591201 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.591275 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.591347 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.591438 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.591516 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.591600 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_3_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.591686 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.591763 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.591872 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.591971 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.592068 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.592148 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.592243 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.592324 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.592403 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.592488 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.592567 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.592661 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.592747 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.592844 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.592929 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.593026 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.593109 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.593187 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_4_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.593273 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.593354 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.593432 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.593516 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.593617 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.593694 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.593817 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.593917 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.594012 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.594099 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.594179 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.594259 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.594344 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.594423 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.594499 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.594607 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.594695 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.594771 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_5_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.594887 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.594976 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.595055 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.595142 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.595224 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.595302 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.595397 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.595479 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.595558 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.595659 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.595741 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.595846 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.595933 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.596010 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.596082 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.596195 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.596278 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.596363 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_6_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.596443 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.596518 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.596624 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.596711 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.596792 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.596892 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.596990 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.597072 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.597152 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.597238 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.597315 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.597393 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.597479 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.597558 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.597646 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.597744 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.597846 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.597937 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_7_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.598040 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.598148 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.598228 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.598323 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.598399 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.598492 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.598603 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.598689 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.598768 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.598874 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.598960 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.599039 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.599145 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.599227 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.599304 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.599399 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.599481 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.599559 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_8_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.599658 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.599740 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.599819 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.599928 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.600026 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.600110 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.600213 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.600299 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.600382 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.600474 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.600559 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.600658 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.600753 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.600857 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.600945 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.601047 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0707 16:45:41.601137 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/RMSProp] is not available in checkpoint\n",
            "W0707 16:45:41.601230 140263261390720 variables_helper.py:152] Variable [FeatureExtractor/MobilenetV1/Conv2d_9_pointwise/weights/RMSProp_1] is not available in checkpoint\n",
            "W0707 16:45:41.601324 140263261390720 variables_helper.py:152] Variable [global_step] is not available in checkpoint\n",
            "W0707 16:45:42.209941 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-07-07 16:45:43.041889: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-07 16:45:43.044232: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xff53c00 executing computations on platform Host. Devices:\n",
            "2019-07-07 16:45:43.044270: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-07 16:45:43.067892: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-07 16:45:43.312617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-07 16:45:43.313217: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xff53340 executing computations on platform CUDA. Devices:\n",
            "2019-07-07 16:45:43.313248: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-07-07 16:45:43.317800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-07 16:45:43.318225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-07 16:45:43.335929: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-07 16:45:43.507462: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-07 16:45:43.585243: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-07 16:45:43.608809: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-07 16:45:43.789755: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-07 16:45:43.895853: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-07 16:45:44.211669: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-07 16:45:44.211971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-07 16:45:44.212584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-07 16:45:44.212988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-07 16:45:44.216555: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-07 16:45:44.217963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-07 16:45:44.217995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-07 16:45:44.218008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-07 16:45:44.219919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-07 16:45:44.220467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-07 16:45:44.220842: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-07 16:45:44.220888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0707 16:45:44.225206 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0707 16:45:44.227090 140263261390720 saver.py:1280] Restoring parameters from training/model.ckpt-0\n",
            "2019-07-07 16:45:45.278561: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "W0707 16:45:45.797474 140263261390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0707 16:45:45.799610 140263261390720 session_manager.py:500] Running local_init_op.\n",
            "I0707 16:45:46.122521 140263261390720 session_manager.py:502] Done running local_init_op.\n",
            "I0707 16:45:52.774894 140263261390720 learning.py:754] Starting Session.\n",
            "I0707 16:45:52.976428 140260304557824 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0707 16:45:52.979647 140263261390720 learning.py:768] Starting Queues.\n",
            "I0707 16:45:57.480632 140260296165120 supervisor.py:1099] global_step/sec: 0\n",
            "2019-07-07 16:46:01.495236: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "I0707 16:46:06.680134 140260287772416 supervisor.py:1050] Recording summary at step 0.\n",
            "I0707 16:46:07.143550 140263261390720 learning.py:507] global step 1: loss = 12.2223 (13.925 sec/step)\n",
            "I0707 16:46:07.709245 140263261390720 learning.py:507] global step 2: loss = 13.0472 (0.359 sec/step)\n",
            "I0707 16:46:08.040934 140263261390720 learning.py:507] global step 3: loss = 12.0015 (0.330 sec/step)\n",
            "I0707 16:46:08.424995 140263261390720 learning.py:507] global step 4: loss = 12.7327 (0.382 sec/step)\n",
            "I0707 16:46:08.795619 140263261390720 learning.py:507] global step 5: loss = 12.1046 (0.369 sec/step)\n",
            "I0707 16:46:09.159075 140263261390720 learning.py:507] global step 6: loss = 11.6655 (0.362 sec/step)\n",
            "I0707 16:46:09.485675 140263261390720 learning.py:507] global step 7: loss = 10.9566 (0.324 sec/step)\n",
            "I0707 16:46:09.821711 140263261390720 learning.py:507] global step 8: loss = 9.9653 (0.334 sec/step)\n",
            "I0707 16:46:10.140397 140263261390720 learning.py:507] global step 9: loss = 10.4046 (0.317 sec/step)\n",
            "I0707 16:46:10.461185 140263261390720 learning.py:507] global step 10: loss = 9.0754 (0.319 sec/step)\n",
            "I0707 16:46:10.787453 140263261390720 learning.py:507] global step 11: loss = 8.7579 (0.325 sec/step)\n",
            "I0707 16:46:11.240361 140263261390720 learning.py:507] global step 12: loss = 9.1199 (0.451 sec/step)\n",
            "I0707 16:46:11.595595 140263261390720 learning.py:507] global step 13: loss = 8.9752 (0.353 sec/step)\n",
            "I0707 16:46:11.969416 140263261390720 learning.py:507] global step 14: loss = 8.6893 (0.372 sec/step)\n",
            "I0707 16:46:12.306802 140263261390720 learning.py:507] global step 15: loss = 8.6200 (0.336 sec/step)\n",
            "I0707 16:46:12.613401 140263261390720 learning.py:507] global step 16: loss = 8.3291 (0.304 sec/step)\n",
            "I0707 16:46:12.933328 140263261390720 learning.py:507] global step 17: loss = 8.2141 (0.318 sec/step)\n",
            "I0707 16:46:13.367658 140263261390720 learning.py:507] global step 18: loss = 9.4543 (0.433 sec/step)\n",
            "I0707 16:46:13.694143 140263261390720 learning.py:507] global step 19: loss = 8.8126 (0.325 sec/step)\n",
            "I0707 16:46:14.152215 140263261390720 learning.py:507] global step 20: loss = 8.4617 (0.456 sec/step)\n",
            "I0707 16:46:14.507055 140263261390720 learning.py:507] global step 21: loss = 8.2313 (0.353 sec/step)\n",
            "I0707 16:46:14.847517 140263261390720 learning.py:507] global step 22: loss = 7.7728 (0.339 sec/step)\n",
            "I0707 16:46:15.205764 140263261390720 learning.py:507] global step 23: loss = 8.4038 (0.356 sec/step)\n",
            "I0707 16:46:15.506006 140263261390720 learning.py:507] global step 24: loss = 7.7751 (0.298 sec/step)\n",
            "I0707 16:46:15.843207 140263261390720 learning.py:507] global step 25: loss = 7.6842 (0.335 sec/step)\n",
            "I0707 16:46:16.372524 140263261390720 learning.py:507] global step 26: loss = 7.5632 (0.527 sec/step)\n",
            "I0707 16:46:16.689262 140263261390720 learning.py:507] global step 27: loss = 8.2092 (0.315 sec/step)\n",
            "I0707 16:46:17.021070 140263261390720 learning.py:507] global step 28: loss = 8.0951 (0.330 sec/step)\n",
            "I0707 16:46:17.401717 140263261390720 learning.py:507] global step 29: loss = 8.0403 (0.379 sec/step)\n",
            "I0707 16:46:17.722606 140263261390720 learning.py:507] global step 30: loss = 7.7777 (0.319 sec/step)\n",
            "I0707 16:46:18.085364 140263261390720 learning.py:507] global step 31: loss = 7.4623 (0.361 sec/step)\n",
            "I0707 16:46:18.472629 140263261390720 learning.py:507] global step 32: loss = 7.7472 (0.385 sec/step)\n",
            "I0707 16:46:18.822612 140263261390720 learning.py:507] global step 33: loss = 8.2972 (0.348 sec/step)\n",
            "I0707 16:46:19.169252 140263261390720 learning.py:507] global step 34: loss = 8.2547 (0.345 sec/step)\n",
            "I0707 16:46:19.516209 140263261390720 learning.py:507] global step 35: loss = 8.1148 (0.345 sec/step)\n",
            "I0707 16:46:19.834061 140263261390720 learning.py:507] global step 36: loss = 8.4642 (0.316 sec/step)\n",
            "I0707 16:46:20.143996 140263261390720 learning.py:507] global step 37: loss = 7.6411 (0.308 sec/step)\n",
            "I0707 16:46:20.495196 140263261390720 learning.py:507] global step 38: loss = 6.8625 (0.350 sec/step)\n",
            "I0707 16:46:20.834842 140263261390720 learning.py:507] global step 39: loss = 7.9054 (0.338 sec/step)\n",
            "I0707 16:46:21.319117 140263261390720 learning.py:507] global step 40: loss = 6.8284 (0.482 sec/step)\n",
            "I0707 16:46:21.662463 140263261390720 learning.py:507] global step 41: loss = 7.3500 (0.341 sec/step)\n",
            "I0707 16:46:21.974841 140263261390720 learning.py:507] global step 42: loss = 7.5233 (0.310 sec/step)\n",
            "I0707 16:46:22.301944 140263261390720 learning.py:507] global step 43: loss = 7.6887 (0.325 sec/step)\n",
            "I0707 16:46:22.635565 140263261390720 learning.py:507] global step 44: loss = 7.3004 (0.332 sec/step)\n",
            "I0707 16:46:22.961531 140263261390720 learning.py:507] global step 45: loss = 7.6734 (0.324 sec/step)\n",
            "2019-07-07 16:46:23.069238: W tensorflow/core/framework/allocator.cc:107] Allocation of 1695621600 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1695629312 bytes == 0xff332000 @  0x7f9195999b6b 0x7f91959b9379 0x7f916f8d6337 0x7f916f8892af 0x7f916f58030b 0x7f916f54df86 0x7f916f54e875 0x7f9177eb44d8 0x7f9177f7130c 0x7f9177f718d8 0x7f9177e4625f 0x7f9177ecc0c3 0x7f9177ecb30b 0x7f916f7fd651 0x7f916f7fe8df 0x7f916f8a4fc9 0x7f916f8a1ea8 0x7f919429966f 0x7f919537b6db 0x7f91956b488f\n",
            "I0707 16:46:23.640661 140263261390720 learning.py:507] global step 46: loss = 7.8036 (0.677 sec/step)\n",
            "I0707 16:46:24.159467 140263261390720 learning.py:507] global step 47: loss = 7.4167 (0.517 sec/step)\n",
            "I0707 16:46:24.482871 140263261390720 learning.py:507] global step 48: loss = 7.1660 (0.322 sec/step)\n",
            "I0707 16:46:24.909328 140263261390720 learning.py:507] global step 49: loss = 6.8724 (0.425 sec/step)\n",
            "I0707 16:46:25.227789 140263261390720 learning.py:507] global step 50: loss = 8.0504 (0.316 sec/step)\n",
            "I0707 16:46:25.581594 140263261390720 learning.py:507] global step 51: loss = 7.4951 (0.351 sec/step)\n",
            "I0707 16:46:26.334927 140263261390720 learning.py:507] global step 52: loss = 7.4749 (0.751 sec/step)\n",
            "I0707 16:46:26.672724 140263261390720 learning.py:507] global step 53: loss = 7.4481 (0.336 sec/step)\n",
            "I0707 16:46:26.987086 140263261390720 learning.py:507] global step 54: loss = 7.2251 (0.313 sec/step)\n",
            "I0707 16:46:27.427179 140263261390720 learning.py:507] global step 55: loss = 7.2478 (0.438 sec/step)\n",
            "I0707 16:46:27.748011 140263261390720 learning.py:507] global step 56: loss = 8.0508 (0.319 sec/step)\n",
            "I0707 16:46:28.092042 140263261390720 learning.py:507] global step 57: loss = 7.9886 (0.342 sec/step)\n",
            "I0707 16:46:28.476049 140263261390720 learning.py:507] global step 58: loss = 7.5468 (0.382 sec/step)\n",
            "I0707 16:46:28.831191 140263261390720 learning.py:507] global step 59: loss = 7.2063 (0.353 sec/step)\n",
            "I0707 16:46:29.190723 140263261390720 learning.py:507] global step 60: loss = 6.8265 (0.358 sec/step)\n",
            "I0707 16:46:29.622661 140263261390720 learning.py:507] global step 61: loss = 7.3218 (0.430 sec/step)\n",
            "I0707 16:46:29.924968 140263261390720 learning.py:507] global step 62: loss = 6.6787 (0.301 sec/step)\n",
            "I0707 16:46:30.260210 140263261390720 learning.py:507] global step 63: loss = 7.1364 (0.333 sec/step)\n",
            "I0707 16:46:30.571514 140263261390720 learning.py:507] global step 64: loss = 7.2545 (0.310 sec/step)\n",
            "I0707 16:46:30.911071 140263261390720 learning.py:507] global step 65: loss = 7.4005 (0.337 sec/step)\n",
            "I0707 16:46:31.256013 140263261390720 learning.py:507] global step 66: loss = 7.3789 (0.341 sec/step)\n",
            "I0707 16:46:31.583228 140263261390720 learning.py:507] global step 67: loss = 7.3028 (0.325 sec/step)\n",
            "I0707 16:46:31.915971 140263261390720 learning.py:507] global step 68: loss = 7.4019 (0.331 sec/step)\n",
            "I0707 16:46:32.246474 140263261390720 learning.py:507] global step 69: loss = 6.6214 (0.329 sec/step)\n",
            "I0707 16:46:32.565723 140263261390720 learning.py:507] global step 70: loss = 7.8354 (0.318 sec/step)\n",
            "I0707 16:46:32.878214 140263261390720 learning.py:507] global step 71: loss = 7.2342 (0.311 sec/step)\n",
            "I0707 16:46:33.182302 140263261390720 learning.py:507] global step 72: loss = 6.3588 (0.302 sec/step)\n",
            "I0707 16:46:33.506051 140263261390720 learning.py:507] global step 73: loss = 6.7288 (0.322 sec/step)\n",
            "I0707 16:46:33.841533 140263261390720 learning.py:507] global step 74: loss = 7.0386 (0.334 sec/step)\n",
            "I0707 16:46:34.229485 140263261390720 learning.py:507] global step 75: loss = 6.7047 (0.386 sec/step)\n",
            "I0707 16:46:34.551600 140263261390720 learning.py:507] global step 76: loss = 7.0093 (0.320 sec/step)\n",
            "I0707 16:46:34.894953 140263261390720 learning.py:507] global step 77: loss = 7.0839 (0.341 sec/step)\n",
            "I0707 16:46:35.206383 140263261390720 learning.py:507] global step 78: loss = 7.0093 (0.310 sec/step)\n",
            "I0707 16:46:35.543921 140263261390720 learning.py:507] global step 79: loss = 6.3281 (0.336 sec/step)\n",
            "I0707 16:46:35.871583 140263261390720 learning.py:507] global step 80: loss = 7.3332 (0.326 sec/step)\n",
            "I0707 16:46:36.216552 140263261390720 learning.py:507] global step 81: loss = 7.0941 (0.343 sec/step)\n",
            "I0707 16:46:36.565756 140263261390720 learning.py:507] global step 82: loss = 6.0760 (0.348 sec/step)\n",
            "I0707 16:46:36.928369 140263261390720 learning.py:507] global step 83: loss = 6.9806 (0.361 sec/step)\n",
            "I0707 16:46:37.247459 140263261390720 learning.py:507] global step 84: loss = 7.0980 (0.317 sec/step)\n",
            "I0707 16:46:37.606459 140263261390720 learning.py:507] global step 85: loss = 6.6780 (0.357 sec/step)\n",
            "I0707 16:46:37.935129 140263261390720 learning.py:507] global step 86: loss = 6.5434 (0.327 sec/step)\n",
            "I0707 16:46:38.262926 140263261390720 learning.py:507] global step 87: loss = 6.2701 (0.326 sec/step)\n",
            "I0707 16:46:38.620605 140263261390720 learning.py:507] global step 88: loss = 6.7093 (0.356 sec/step)\n",
            "I0707 16:46:38.940214 140263261390720 learning.py:507] global step 89: loss = 7.4257 (0.318 sec/step)\n",
            "I0707 16:46:39.299734 140263261390720 learning.py:507] global step 90: loss = 6.4505 (0.358 sec/step)\n",
            "I0707 16:46:39.621293 140263261390720 learning.py:507] global step 91: loss = 6.9097 (0.320 sec/step)\n",
            "I0707 16:46:39.980152 140263261390720 learning.py:507] global step 92: loss = 6.8601 (0.357 sec/step)\n",
            "I0707 16:46:40.341900 140263261390720 learning.py:507] global step 93: loss = 6.4605 (0.360 sec/step)\n",
            "I0707 16:46:40.653214 140263261390720 learning.py:507] global step 94: loss = 6.8872 (0.310 sec/step)\n",
            "I0707 16:46:40.959243 140263261390720 learning.py:507] global step 95: loss = 6.7677 (0.304 sec/step)\n",
            "I0707 16:46:41.313037 140263261390720 learning.py:507] global step 96: loss = 6.4012 (0.352 sec/step)\n",
            "I0707 16:46:41.628953 140263261390720 learning.py:507] global step 97: loss = 6.8320 (0.314 sec/step)\n",
            "I0707 16:46:41.957626 140263261390720 learning.py:507] global step 98: loss = 7.0859 (0.327 sec/step)\n",
            "I0707 16:46:42.281033 140263261390720 learning.py:507] global step 99: loss = 6.2226 (0.319 sec/step)\n",
            "I0707 16:46:42.599409 140263261390720 learning.py:507] global step 100: loss = 6.1500 (0.316 sec/step)\n",
            "I0707 16:46:42.968720 140263261390720 learning.py:507] global step 101: loss = 6.9019 (0.367 sec/step)\n",
            "I0707 16:46:43.345943 140263261390720 learning.py:507] global step 102: loss = 6.6704 (0.375 sec/step)\n",
            "I0707 16:46:43.691581 140263261390720 learning.py:507] global step 103: loss = 6.7597 (0.344 sec/step)\n",
            "I0707 16:46:44.000599 140263261390720 learning.py:507] global step 104: loss = 6.6299 (0.307 sec/step)\n",
            "I0707 16:46:44.368153 140263261390720 learning.py:507] global step 105: loss = 6.2829 (0.366 sec/step)\n",
            "I0707 16:46:44.695541 140263261390720 learning.py:507] global step 106: loss = 6.7334 (0.325 sec/step)\n",
            "I0707 16:46:45.119167 140263261390720 learning.py:507] global step 107: loss = 6.8278 (0.422 sec/step)\n",
            "I0707 16:46:45.466589 140263261390720 learning.py:507] global step 108: loss = 6.1395 (0.346 sec/step)\n",
            "I0707 16:46:45.867999 140263261390720 learning.py:507] global step 109: loss = 6.1132 (0.399 sec/step)\n",
            "I0707 16:46:46.267505 140263261390720 learning.py:507] global step 110: loss = 6.3393 (0.398 sec/step)\n",
            "I0707 16:46:46.591293 140263261390720 learning.py:507] global step 111: loss = 6.1761 (0.322 sec/step)\n",
            "I0707 16:46:46.905160 140263261390720 learning.py:507] global step 112: loss = 6.4877 (0.312 sec/step)\n",
            "I0707 16:46:47.484934 140263261390720 learning.py:507] global step 113: loss = 6.9139 (0.578 sec/step)\n",
            "I0707 16:46:47.821251 140263261390720 learning.py:507] global step 114: loss = 7.2584 (0.334 sec/step)\n",
            "I0707 16:46:48.135419 140263261390720 learning.py:507] global step 115: loss = 6.8649 (0.312 sec/step)\n",
            "I0707 16:46:48.582051 140263261390720 learning.py:507] global step 116: loss = 7.5105 (0.445 sec/step)\n",
            "I0707 16:46:48.886793 140263261390720 learning.py:507] global step 117: loss = 7.7445 (0.303 sec/step)\n",
            "I0707 16:46:49.338818 140263261390720 learning.py:507] global step 118: loss = 7.0786 (0.450 sec/step)\n",
            "I0707 16:46:49.655794 140263261390720 learning.py:507] global step 119: loss = 6.5441 (0.315 sec/step)\n",
            "I0707 16:46:49.985496 140263261390720 learning.py:507] global step 120: loss = 6.6164 (0.328 sec/step)\n",
            "I0707 16:46:50.442472 140263261390720 learning.py:507] global step 121: loss = 6.5781 (0.455 sec/step)\n",
            "I0707 16:46:50.808027 140263261390720 learning.py:507] global step 122: loss = 6.5927 (0.364 sec/step)\n",
            "I0707 16:46:51.127351 140263261390720 learning.py:507] global step 123: loss = 6.5469 (0.317 sec/step)\n",
            "I0707 16:46:51.457522 140263261390720 learning.py:507] global step 124: loss = 6.9270 (0.328 sec/step)\n",
            "I0707 16:46:51.764657 140263261390720 learning.py:507] global step 125: loss = 6.3965 (0.305 sec/step)\n",
            "I0707 16:46:52.202841 140263261390720 learning.py:507] global step 126: loss = 7.0117 (0.436 sec/step)\n",
            "I0707 16:46:52.573551 140263261390720 learning.py:507] global step 127: loss = 6.0092 (0.369 sec/step)\n",
            "I0707 16:46:52.909282 140263261390720 learning.py:507] global step 128: loss = 5.9301 (0.334 sec/step)\n",
            "I0707 16:46:53.366518 140263261390720 learning.py:507] global step 129: loss = 6.9730 (0.453 sec/step)\n",
            "I0707 16:46:53.675409 140263261390720 learning.py:507] global step 130: loss = 6.3813 (0.307 sec/step)\n",
            "I0707 16:46:54.017537 140263261390720 learning.py:507] global step 131: loss = 6.5306 (0.341 sec/step)\n",
            "I0707 16:46:54.334489 140263261390720 learning.py:507] global step 132: loss = 5.5362 (0.315 sec/step)\n",
            "I0707 16:46:54.673311 140263261390720 learning.py:507] global step 133: loss = 5.9878 (0.337 sec/step)\n",
            "I0707 16:46:55.010391 140263261390720 learning.py:507] global step 134: loss = 6.0179 (0.335 sec/step)\n",
            "I0707 16:46:55.380973 140263261390720 learning.py:507] global step 135: loss = 6.7139 (0.368 sec/step)\n",
            "I0707 16:46:55.731342 140263261390720 learning.py:507] global step 136: loss = 6.9687 (0.348 sec/step)\n",
            "I0707 16:46:56.073308 140263261390720 learning.py:507] global step 137: loss = 6.4258 (0.340 sec/step)\n",
            "I0707 16:46:56.492857 140263261390720 learning.py:507] global step 138: loss = 6.0634 (0.417 sec/step)\n",
            "I0707 16:46:56.829898 140263261390720 learning.py:507] global step 139: loss = 5.9058 (0.335 sec/step)\n",
            "I0707 16:46:57.169665 140263261390720 learning.py:507] global step 140: loss = 6.9508 (0.338 sec/step)\n",
            "I0707 16:46:57.617181 140263261390720 learning.py:507] global step 141: loss = 7.0997 (0.446 sec/step)\n",
            "I0707 16:46:57.935896 140263261390720 learning.py:507] global step 142: loss = 7.2425 (0.317 sec/step)\n",
            "I0707 16:46:58.248210 140263261390720 learning.py:507] global step 143: loss = 6.0147 (0.310 sec/step)\n",
            "I0707 16:46:58.644864 140263261390720 learning.py:507] global step 144: loss = 6.0697 (0.395 sec/step)\n",
            "I0707 16:46:58.945625 140263261390720 learning.py:507] global step 145: loss = 6.5770 (0.299 sec/step)\n",
            "I0707 16:46:59.349273 140263261390720 learning.py:507] global step 146: loss = 6.2836 (0.402 sec/step)\n",
            "I0707 16:46:59.967235 140263261390720 learning.py:507] global step 147: loss = 6.0737 (0.616 sec/step)\n",
            "I0707 16:47:00.326673 140263261390720 learning.py:507] global step 148: loss = 5.6235 (0.358 sec/step)\n",
            "I0707 16:47:00.638485 140263261390720 learning.py:507] global step 149: loss = 5.8540 (0.310 sec/step)\n",
            "I0707 16:47:00.990770 140263261390720 learning.py:507] global step 150: loss = 5.9835 (0.350 sec/step)\n",
            "I0707 16:47:01.323241 140263261390720 learning.py:507] global step 151: loss = 6.8697 (0.330 sec/step)\n",
            "I0707 16:47:01.788294 140263261390720 learning.py:507] global step 152: loss = 6.7144 (0.463 sec/step)\n",
            "I0707 16:47:02.147663 140263261390720 learning.py:507] global step 153: loss = 6.8534 (0.358 sec/step)\n",
            "I0707 16:47:02.488514 140263261390720 learning.py:507] global step 154: loss = 6.7050 (0.339 sec/step)\n",
            "I0707 16:47:02.904135 140263261390720 learning.py:507] global step 155: loss = 6.5356 (0.414 sec/step)\n",
            "I0707 16:47:03.262185 140263261390720 learning.py:507] global step 156: loss = 6.0457 (0.356 sec/step)\n",
            "I0707 16:47:03.602491 140263261390720 learning.py:507] global step 157: loss = 5.7957 (0.338 sec/step)\n",
            "2019-07-07 16:47:03.608058: W tensorflow/core/framework/allocator.cc:107] Allocation of 1164268896 exceeds 10% of system memory.\n",
            "I0707 16:47:04.084077 140263261390720 learning.py:507] global step 158: loss = 6.4688 (0.480 sec/step)\n",
            "I0707 16:47:04.402409 140263261390720 learning.py:507] global step 159: loss = 6.2286 (0.316 sec/step)\n",
            "I0707 16:47:04.734109 140263261390720 learning.py:507] global step 160: loss = 6.1088 (0.329 sec/step)\n",
            "I0707 16:47:05.112853 140263261390720 learning.py:507] global step 161: loss = 6.0625 (0.376 sec/step)\n",
            "I0707 16:47:05.450707 140263261390720 learning.py:507] global step 162: loss = 6.2128 (0.336 sec/step)\n",
            "I0707 16:47:05.765366 140263261390720 learning.py:507] global step 163: loss = 5.8697 (0.312 sec/step)\n",
            "I0707 16:47:06.436369 140263261390720 learning.py:507] global step 164: loss = 7.0232 (0.667 sec/step)\n",
            "I0707 16:47:06.780144 140263261390720 learning.py:507] global step 165: loss = 5.4575 (0.337 sec/step)\n",
            "I0707 16:47:07.089128 140263261390720 learning.py:507] global step 166: loss = 6.1705 (0.306 sec/step)\n",
            "I0707 16:47:07.420275 140263261390720 learning.py:507] global step 167: loss = 5.4168 (0.329 sec/step)\n",
            "I0707 16:47:07.760987 140263261390720 learning.py:507] global step 168: loss = 6.6851 (0.339 sec/step)\n",
            "I0707 16:47:08.091118 140263261390720 learning.py:507] global step 169: loss = 6.7184 (0.328 sec/step)\n",
            "I0707 16:47:08.517369 140263261390720 learning.py:507] global step 170: loss = 5.8178 (0.424 sec/step)\n",
            "I0707 16:47:08.841924 140263261390720 learning.py:507] global step 171: loss = 5.6720 (0.323 sec/step)\n",
            "I0707 16:47:09.203545 140263261390720 learning.py:507] global step 172: loss = 6.7430 (0.360 sec/step)\n",
            "I0707 16:47:09.530986 140263261390720 learning.py:507] global step 173: loss = 5.7101 (0.326 sec/step)\n",
            "I0707 16:47:09.873495 140263261390720 learning.py:507] global step 174: loss = 5.3643 (0.341 sec/step)\n",
            "I0707 16:47:10.295941 140263261390720 learning.py:507] global step 175: loss = 6.7516 (0.421 sec/step)\n",
            "I0707 16:47:10.614495 140263261390720 learning.py:507] global step 176: loss = 6.6616 (0.317 sec/step)\n",
            "I0707 16:47:10.950742 140263261390720 learning.py:507] global step 177: loss = 6.4748 (0.333 sec/step)\n",
            "I0707 16:47:11.289155 140263261390720 learning.py:507] global step 178: loss = 5.7297 (0.336 sec/step)\n",
            "I0707 16:47:11.616968 140263261390720 learning.py:507] global step 179: loss = 6.1119 (0.326 sec/step)\n",
            "I0707 16:47:11.983601 140263261390720 learning.py:507] global step 180: loss = 5.6287 (0.365 sec/step)\n",
            "I0707 16:47:12.506812 140263261390720 learning.py:507] global step 181: loss = 6.2405 (0.521 sec/step)\n",
            "I0707 16:47:12.865679 140263261390720 learning.py:507] global step 182: loss = 6.2551 (0.357 sec/step)\n",
            "I0707 16:47:13.185522 140263261390720 learning.py:507] global step 183: loss = 6.2823 (0.318 sec/step)\n",
            "I0707 16:47:13.504308 140263261390720 learning.py:507] global step 184: loss = 6.3234 (0.317 sec/step)\n",
            "I0707 16:47:13.867106 140263261390720 learning.py:507] global step 185: loss = 5.9108 (0.361 sec/step)\n",
            "I0707 16:47:14.223985 140263261390720 learning.py:507] global step 186: loss = 6.5231 (0.355 sec/step)\n",
            "I0707 16:47:14.647487 140263261390720 learning.py:507] global step 187: loss = 6.8921 (0.422 sec/step)\n",
            "I0707 16:47:15.031745 140263261390720 learning.py:507] global step 188: loss = 5.7926 (0.382 sec/step)\n",
            "I0707 16:47:15.391604 140263261390720 learning.py:507] global step 189: loss = 6.5747 (0.358 sec/step)\n",
            "I0707 16:47:15.712729 140263261390720 learning.py:507] global step 190: loss = 6.7594 (0.319 sec/step)\n",
            "I0707 16:47:16.117872 140263261390720 learning.py:507] global step 191: loss = 6.4368 (0.403 sec/step)\n",
            "I0707 16:47:16.495472 140263261390720 learning.py:507] global step 192: loss = 6.4721 (0.376 sec/step)\n",
            "I0707 16:47:16.847122 140263261390720 learning.py:507] global step 193: loss = 6.4145 (0.350 sec/step)\n",
            "I0707 16:47:17.235810 140263261390720 learning.py:507] global step 194: loss = 5.9134 (0.387 sec/step)\n",
            "I0707 16:47:17.607769 140263261390720 learning.py:507] global step 195: loss = 6.1060 (0.370 sec/step)\n",
            "I0707 16:47:17.936995 140263261390720 learning.py:507] global step 196: loss = 7.0636 (0.327 sec/step)\n",
            "I0707 16:47:18.284329 140263261390720 learning.py:507] global step 197: loss = 5.2176 (0.346 sec/step)\n",
            "I0707 16:47:18.661112 140263261390720 learning.py:507] global step 198: loss = 5.7468 (0.375 sec/step)\n",
            "I0707 16:47:19.019167 140263261390720 learning.py:507] global step 199: loss = 6.2784 (0.356 sec/step)\n",
            "I0707 16:47:19.394333 140263261390720 learning.py:507] global step 200: loss = 6.1250 (0.373 sec/step)\n",
            "I0707 16:47:19.701594 140263261390720 learning.py:507] global step 201: loss = 5.5867 (0.305 sec/step)\n",
            "I0707 16:47:20.066505 140263261390720 learning.py:507] global step 202: loss = 5.5186 (0.363 sec/step)\n",
            "I0707 16:47:20.380747 140263261390720 learning.py:507] global step 203: loss = 6.2713 (0.312 sec/step)\n",
            "I0707 16:47:20.734407 140263261390720 learning.py:507] global step 204: loss = 5.5094 (0.352 sec/step)\n",
            "I0707 16:47:21.063744 140263261390720 learning.py:507] global step 205: loss = 5.8048 (0.327 sec/step)\n",
            "I0707 16:47:21.408420 140263261390720 learning.py:507] global step 206: loss = 5.5770 (0.342 sec/step)\n",
            "I0707 16:47:21.737055 140263261390720 learning.py:507] global step 207: loss = 5.9060 (0.327 sec/step)\n",
            "I0707 16:47:22.202383 140263261390720 learning.py:507] global step 208: loss = 6.0250 (0.463 sec/step)\n",
            "I0707 16:47:22.532153 140263261390720 learning.py:507] global step 209: loss = 6.6040 (0.328 sec/step)\n",
            "I0707 16:47:22.860063 140263261390720 learning.py:507] global step 210: loss = 5.9446 (0.326 sec/step)\n",
            "I0707 16:47:23.184763 140263261390720 learning.py:507] global step 211: loss = 6.9429 (0.323 sec/step)\n",
            "I0707 16:47:23.537917 140263261390720 learning.py:507] global step 212: loss = 7.1983 (0.351 sec/step)\n",
            "I0707 16:47:23.877265 140263261390720 learning.py:507] global step 213: loss = 6.4147 (0.337 sec/step)\n",
            "I0707 16:47:24.296398 140263261390720 learning.py:507] global step 214: loss = 6.0186 (0.417 sec/step)\n",
            "I0707 16:47:24.626795 140263261390720 learning.py:507] global step 215: loss = 6.2734 (0.329 sec/step)\n",
            "I0707 16:47:25.023326 140263261390720 learning.py:507] global step 216: loss = 6.0912 (0.395 sec/step)\n",
            "I0707 16:47:25.353510 140263261390720 learning.py:507] global step 217: loss = 7.1509 (0.328 sec/step)\n",
            "I0707 16:47:25.680438 140263261390720 learning.py:507] global step 218: loss = 5.9883 (0.325 sec/step)\n",
            "I0707 16:47:26.071167 140263261390720 learning.py:507] global step 219: loss = 6.6850 (0.389 sec/step)\n",
            "I0707 16:47:26.457760 140263261390720 learning.py:507] global step 220: loss = 6.1645 (0.385 sec/step)\n",
            "I0707 16:47:26.764586 140263261390720 learning.py:507] global step 221: loss = 7.0834 (0.305 sec/step)\n",
            "I0707 16:47:27.112506 140263261390720 learning.py:507] global step 222: loss = 5.6323 (0.346 sec/step)\n",
            "I0707 16:47:27.456122 140263261390720 learning.py:507] global step 223: loss = 5.9466 (0.341 sec/step)\n",
            "I0707 16:47:27.806819 140263261390720 learning.py:507] global step 224: loss = 6.8118 (0.347 sec/step)\n",
            "I0707 16:47:28.176737 140263261390720 learning.py:507] global step 225: loss = 5.5889 (0.368 sec/step)\n",
            "I0707 16:47:28.560955 140263261390720 learning.py:507] global step 226: loss = 6.2950 (0.382 sec/step)\n",
            "I0707 16:47:28.951874 140263261390720 learning.py:507] global step 227: loss = 5.7533 (0.389 sec/step)\n",
            "I0707 16:47:29.288195 140263261390720 learning.py:507] global step 228: loss = 5.3845 (0.334 sec/step)\n",
            "I0707 16:47:29.615545 140263261390720 learning.py:507] global step 229: loss = 6.4974 (0.326 sec/step)\n",
            "I0707 16:47:30.064627 140263261390720 learning.py:507] global step 230: loss = 6.3203 (0.447 sec/step)\n",
            "I0707 16:47:30.452600 140263261390720 learning.py:507] global step 231: loss = 6.0666 (0.386 sec/step)\n",
            "I0707 16:47:30.807538 140263261390720 learning.py:507] global step 232: loss = 6.5435 (0.353 sec/step)\n",
            "I0707 16:47:31.124943 140263261390720 learning.py:507] global step 233: loss = 5.4304 (0.316 sec/step)\n",
            "I0707 16:47:31.449394 140263261390720 learning.py:507] global step 234: loss = 5.9164 (0.322 sec/step)\n",
            "I0707 16:47:31.842418 140263261390720 learning.py:507] global step 235: loss = 6.1026 (0.391 sec/step)\n",
            "I0707 16:47:32.276710 140263261390720 learning.py:507] global step 236: loss = 7.4278 (0.433 sec/step)\n",
            "I0707 16:47:32.622668 140263261390720 learning.py:507] global step 237: loss = 7.6739 (0.344 sec/step)\n",
            "I0707 16:47:33.017645 140263261390720 learning.py:507] global step 238: loss = 5.7870 (0.389 sec/step)\n",
            "I0707 16:47:33.539079 140263261390720 learning.py:507] global step 239: loss = 5.0230 (0.520 sec/step)\n",
            "I0707 16:47:33.887504 140263261390720 learning.py:507] global step 240: loss = 5.3785 (0.347 sec/step)\n",
            "I0707 16:47:34.243526 140263261390720 learning.py:507] global step 241: loss = 7.0301 (0.353 sec/step)\n",
            "I0707 16:47:34.660816 140263261390720 learning.py:507] global step 242: loss = 6.2058 (0.415 sec/step)\n",
            "I0707 16:47:34.984903 140263261390720 learning.py:507] global step 243: loss = 5.2508 (0.322 sec/step)\n",
            "I0707 16:47:35.325178 140263261390720 learning.py:507] global step 244: loss = 5.3548 (0.338 sec/step)\n",
            "I0707 16:47:35.656557 140263261390720 learning.py:507] global step 245: loss = 6.2275 (0.330 sec/step)\n",
            "I0707 16:47:36.149993 140263261390720 learning.py:507] global step 246: loss = 5.0101 (0.492 sec/step)\n",
            "I0707 16:47:36.583040 140263261390720 learning.py:507] global step 247: loss = 5.5773 (0.431 sec/step)\n",
            "I0707 16:47:36.894139 140263261390720 learning.py:507] global step 248: loss = 4.9914 (0.309 sec/step)\n",
            "I0707 16:47:37.226053 140263261390720 learning.py:507] global step 249: loss = 5.9018 (0.330 sec/step)\n",
            "I0707 16:47:37.591718 140263261390720 learning.py:507] global step 250: loss = 5.0843 (0.364 sec/step)\n",
            "I0707 16:47:37.934102 140263261390720 learning.py:507] global step 251: loss = 5.4446 (0.341 sec/step)\n",
            "I0707 16:47:38.245159 140263261390720 learning.py:507] global step 252: loss = 5.7734 (0.309 sec/step)\n",
            "I0707 16:47:38.684263 140263261390720 learning.py:507] global step 253: loss = 6.1172 (0.437 sec/step)\n",
            "I0707 16:47:39.043725 140263261390720 learning.py:507] global step 254: loss = 5.7039 (0.358 sec/step)\n",
            "I0707 16:47:39.498506 140263261390720 learning.py:507] global step 255: loss = 6.3634 (0.453 sec/step)\n",
            "I0707 16:47:39.875191 140263261390720 learning.py:507] global step 256: loss = 4.6104 (0.375 sec/step)\n",
            "I0707 16:47:40.215718 140263261390720 learning.py:507] global step 257: loss = 5.7162 (0.339 sec/step)\n",
            "I0707 16:47:40.602922 140263261390720 learning.py:507] global step 258: loss = 5.4761 (0.385 sec/step)\n",
            "I0707 16:47:40.953528 140263261390720 learning.py:507] global step 259: loss = 5.6087 (0.349 sec/step)\n",
            "I0707 16:47:41.275807 140263261390720 learning.py:507] global step 260: loss = 6.6171 (0.321 sec/step)\n",
            "I0707 16:47:41.670754 140263261390720 learning.py:507] global step 261: loss = 6.0778 (0.392 sec/step)\n",
            "I0707 16:47:42.048987 140263261390720 learning.py:507] global step 262: loss = 5.8910 (0.376 sec/step)\n",
            "I0707 16:47:42.389255 140263261390720 learning.py:507] global step 263: loss = 6.9990 (0.338 sec/step)\n",
            "I0707 16:47:42.854193 140263261390720 learning.py:507] global step 264: loss = 5.4448 (0.463 sec/step)\n",
            "I0707 16:47:43.291633 140263261390720 learning.py:507] global step 265: loss = 5.5905 (0.436 sec/step)\n",
            "I0707 16:47:43.627939 140263261390720 learning.py:507] global step 266: loss = 5.6599 (0.334 sec/step)\n",
            "I0707 16:47:44.004725 140263261390720 learning.py:507] global step 267: loss = 4.9670 (0.375 sec/step)\n",
            "I0707 16:47:44.314815 140263261390720 learning.py:507] global step 268: loss = 5.1152 (0.308 sec/step)\n",
            "I0707 16:47:44.664606 140263261390720 learning.py:507] global step 269: loss = 6.8648 (0.348 sec/step)\n",
            "I0707 16:47:45.037960 140263261390720 learning.py:507] global step 270: loss = 6.5215 (0.371 sec/step)\n",
            "I0707 16:47:45.346767 140263261390720 learning.py:507] global step 271: loss = 7.1095 (0.307 sec/step)\n",
            "I0707 16:47:45.676553 140263261390720 learning.py:507] global step 272: loss = 6.7106 (0.328 sec/step)\n",
            "I0707 16:47:46.149721 140263261390720 learning.py:507] global step 273: loss = 5.1046 (0.471 sec/step)\n",
            "I0707 16:47:46.470282 140263261390720 learning.py:507] global step 274: loss = 4.9286 (0.317 sec/step)\n",
            "2019-07-07 16:47:46.520227: W tensorflow/core/framework/allocator.cc:107] Allocation of 1695621600 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1695629312 bytes == 0x1151ec000 @  0x7f9195999b6b 0x7f91959b9379 0x7f916f8d6337 0x7f916f8892af 0x7f916f58030b 0x7f916f54df86 0x7f916f54e875 0x7f9177eb44d8 0x7f9177f7130c 0x7f9177f718d8 0x7f9177e4625f 0x7f9177ecc0c3 0x7f9177ecb30b 0x7f916f7fd651 0x7f916f7fe8df 0x7f916f8a4fc9 0x7f916f8a1ea8 0x7f919429966f 0x7f919537b6db 0x7f91956b488f\n",
            "I0707 16:47:47.051195 140263261390720 learning.py:507] global step 275: loss = 5.6633 (0.579 sec/step)\n",
            "I0707 16:47:47.380033 140263261390720 learning.py:507] global step 276: loss = 5.1233 (0.327 sec/step)\n",
            "I0707 16:47:47.685385 140263261390720 learning.py:507] global step 277: loss = 5.7312 (0.304 sec/step)\n",
            "I0707 16:47:47.988901 140263261390720 learning.py:507] global step 278: loss = 5.6626 (0.302 sec/step)\n",
            "I0707 16:47:48.342448 140263261390720 learning.py:507] global step 279: loss = 5.3750 (0.352 sec/step)\n",
            "I0707 16:47:48.679226 140263261390720 learning.py:507] global step 280: loss = 5.1599 (0.335 sec/step)\n",
            "I0707 16:47:49.425155 140263261390720 learning.py:507] global step 281: loss = 6.5154 (0.744 sec/step)\n",
            "I0707 16:47:49.780698 140263261390720 learning.py:507] global step 282: loss = 6.2853 (0.354 sec/step)\n",
            "I0707 16:47:50.100531 140263261390720 learning.py:507] global step 283: loss = 5.3583 (0.318 sec/step)\n",
            "I0707 16:47:50.448815 140263261390720 learning.py:507] global step 284: loss = 4.9501 (0.346 sec/step)\n",
            "I0707 16:47:50.797468 140263261390720 learning.py:507] global step 285: loss = 6.4092 (0.347 sec/step)\n",
            "I0707 16:47:51.140680 140263261390720 learning.py:507] global step 286: loss = 7.5773 (0.341 sec/step)\n",
            "I0707 16:47:51.535142 140263261390720 learning.py:507] global step 287: loss = 4.9042 (0.393 sec/step)\n",
            "I0707 16:47:51.908102 140263261390720 learning.py:507] global step 288: loss = 5.0027 (0.371 sec/step)\n",
            "I0707 16:47:52.245193 140263261390720 learning.py:507] global step 289: loss = 5.9935 (0.335 sec/step)\n",
            "I0707 16:47:52.624342 140263261390720 learning.py:507] global step 290: loss = 6.9371 (0.378 sec/step)\n",
            "I0707 16:47:52.972299 140263261390720 learning.py:507] global step 291: loss = 6.5985 (0.346 sec/step)\n",
            "I0707 16:47:53.789529 140260287772416 supervisor.py:1050] Recording summary at step 292.\n",
            "I0707 16:47:53.806451 140263261390720 learning.py:507] global step 292: loss = 5.2460 (0.831 sec/step)\n",
            "I0707 16:47:54.154448 140263261390720 learning.py:507] global step 293: loss = 5.5048 (0.346 sec/step)\n",
            "I0707 16:47:54.492403 140263261390720 learning.py:507] global step 294: loss = 6.5439 (0.336 sec/step)\n",
            "I0707 16:47:54.825443 140260296165120 supervisor.py:1099] global_step/sec: 2.50544\n",
            "I0707 16:47:54.838813 140263261390720 learning.py:507] global step 295: loss = 6.2809 (0.345 sec/step)\n",
            "I0707 16:47:55.154415 140263261390720 learning.py:507] global step 296: loss = 4.9620 (0.314 sec/step)\n",
            "I0707 16:47:55.486715 140263261390720 learning.py:507] global step 297: loss = 6.5897 (0.330 sec/step)\n",
            "I0707 16:47:56.017402 140263261390720 learning.py:507] global step 298: loss = 5.8232 (0.529 sec/step)\n",
            "I0707 16:47:56.357259 140263261390720 learning.py:507] global step 299: loss = 5.3291 (0.337 sec/step)\n",
            "I0707 16:47:56.670196 140263261390720 learning.py:507] global step 300: loss = 5.7491 (0.311 sec/step)\n",
            "I0707 16:47:56.992782 140263261390720 learning.py:507] global step 301: loss = 6.6765 (0.321 sec/step)\n",
            "I0707 16:47:57.299199 140263261390720 learning.py:507] global step 302: loss = 5.2727 (0.305 sec/step)\n",
            "I0707 16:47:57.641412 140263261390720 learning.py:507] global step 303: loss = 5.5587 (0.341 sec/step)\n",
            "I0707 16:47:57.967035 140263261390720 learning.py:507] global step 304: loss = 5.2394 (0.324 sec/step)\n",
            "I0707 16:47:58.428151 140263261390720 learning.py:507] global step 305: loss = 5.8029 (0.459 sec/step)\n",
            "I0707 16:47:58.747206 140263261390720 learning.py:507] global step 306: loss = 5.6588 (0.317 sec/step)\n",
            "I0707 16:47:59.056775 140263261390720 learning.py:507] global step 307: loss = 4.8250 (0.308 sec/step)\n",
            "I0707 16:47:59.420621 140263261390720 learning.py:507] global step 308: loss = 6.6203 (0.360 sec/step)\n",
            "I0707 16:47:59.766180 140263261390720 learning.py:507] global step 309: loss = 5.6202 (0.340 sec/step)\n",
            "I0707 16:48:00.101196 140263261390720 learning.py:507] global step 310: loss = 5.0979 (0.333 sec/step)\n",
            "I0707 16:48:00.422624 140263261390720 learning.py:507] global step 311: loss = 6.0979 (0.319 sec/step)\n",
            "I0707 16:48:00.742101 140263261390720 learning.py:507] global step 312: loss = 6.3584 (0.318 sec/step)\n",
            "I0707 16:48:01.110436 140263261390720 learning.py:507] global step 313: loss = 5.5642 (0.367 sec/step)\n",
            "I0707 16:48:01.630725 140263261390720 learning.py:507] global step 314: loss = 6.0960 (0.518 sec/step)\n",
            "I0707 16:48:01.935923 140263261390720 learning.py:507] global step 315: loss = 5.6884 (0.303 sec/step)\n",
            "I0707 16:48:02.325350 140263261390720 learning.py:507] global step 316: loss = 7.3308 (0.387 sec/step)\n",
            "I0707 16:48:02.663197 140263261390720 learning.py:507] global step 317: loss = 6.0018 (0.336 sec/step)\n",
            "I0707 16:48:02.968098 140263261390720 learning.py:507] global step 318: loss = 6.1146 (0.303 sec/step)\n",
            "I0707 16:48:03.423056 140263261390720 learning.py:507] global step 319: loss = 5.6679 (0.453 sec/step)\n",
            "I0707 16:48:03.763621 140263261390720 learning.py:507] global step 320: loss = 5.7619 (0.339 sec/step)\n",
            "I0707 16:48:04.075694 140263261390720 learning.py:507] global step 321: loss = 6.5696 (0.310 sec/step)\n",
            "I0707 16:48:04.397499 140263261390720 learning.py:507] global step 322: loss = 5.4539 (0.320 sec/step)\n",
            "I0707 16:48:04.732181 140263261390720 learning.py:507] global step 323: loss = 5.9351 (0.333 sec/step)\n",
            "I0707 16:48:05.166242 140263261390720 learning.py:507] global step 324: loss = 5.8874 (0.432 sec/step)\n",
            "I0707 16:48:05.474849 140263261390720 learning.py:507] global step 325: loss = 5.8532 (0.307 sec/step)\n",
            "I0707 16:48:05.793597 140263261390720 learning.py:507] global step 326: loss = 6.0719 (0.317 sec/step)\n",
            "I0707 16:48:06.106719 140263261390720 learning.py:507] global step 327: loss = 5.3642 (0.311 sec/step)\n",
            "I0707 16:48:06.418521 140263261390720 learning.py:507] global step 328: loss = 5.3827 (0.310 sec/step)\n",
            "I0707 16:48:06.749488 140263261390720 learning.py:507] global step 329: loss = 6.1407 (0.329 sec/step)\n",
            "I0707 16:48:07.107721 140263261390720 learning.py:507] global step 330: loss = 4.7746 (0.356 sec/step)\n",
            "I0707 16:48:07.416132 140263261390720 learning.py:507] global step 331: loss = 6.2697 (0.307 sec/step)\n",
            "I0707 16:48:07.736520 140263261390720 learning.py:507] global step 332: loss = 5.9945 (0.319 sec/step)\n",
            "I0707 16:48:08.077406 140263261390720 learning.py:507] global step 333: loss = 5.2200 (0.339 sec/step)\n",
            "I0707 16:48:08.534526 140263261390720 learning.py:507] global step 334: loss = 5.7075 (0.452 sec/step)\n",
            "I0707 16:48:08.866946 140263261390720 learning.py:507] global step 335: loss = 5.5724 (0.330 sec/step)\n",
            "I0707 16:48:09.182059 140263261390720 learning.py:507] global step 336: loss = 4.5293 (0.313 sec/step)\n",
            "I0707 16:48:09.520814 140263261390720 learning.py:507] global step 337: loss = 6.8214 (0.337 sec/step)\n",
            "I0707 16:48:09.853438 140263261390720 learning.py:507] global step 338: loss = 5.5486 (0.331 sec/step)\n",
            "I0707 16:48:10.172801 140263261390720 learning.py:507] global step 339: loss = 5.0445 (0.318 sec/step)\n",
            "I0707 16:48:10.689720 140263261390720 learning.py:507] global step 340: loss = 6.5404 (0.515 sec/step)\n",
            "I0707 16:48:11.139705 140263261390720 learning.py:507] global step 341: loss = 5.3128 (0.448 sec/step)\n",
            "I0707 16:48:11.446300 140263261390720 learning.py:507] global step 342: loss = 5.3166 (0.305 sec/step)\n",
            "I0707 16:48:11.773943 140263261390720 learning.py:507] global step 343: loss = 7.0714 (0.325 sec/step)\n",
            "I0707 16:48:12.161136 140263261390720 learning.py:507] global step 344: loss = 5.8384 (0.386 sec/step)\n",
            "I0707 16:48:12.537988 140263261390720 learning.py:507] global step 345: loss = 5.8661 (0.375 sec/step)\n",
            "I0707 16:48:12.868625 140263261390720 learning.py:507] global step 346: loss = 5.5169 (0.329 sec/step)\n",
            "I0707 16:48:13.219950 140263261390720 learning.py:507] global step 347: loss = 7.0386 (0.350 sec/step)\n",
            "I0707 16:48:13.526468 140263261390720 learning.py:507] global step 348: loss = 6.8492 (0.305 sec/step)\n",
            "I0707 16:48:13.927104 140263261390720 learning.py:507] global step 349: loss = 6.3801 (0.399 sec/step)\n",
            "I0707 16:48:14.284295 140263261390720 learning.py:507] global step 350: loss = 5.3860 (0.356 sec/step)\n",
            "I0707 16:48:14.619495 140263261390720 learning.py:507] global step 351: loss = 6.9851 (0.333 sec/step)\n",
            "I0707 16:48:14.939885 140263261390720 learning.py:507] global step 352: loss = 5.5837 (0.319 sec/step)\n",
            "I0707 16:48:15.251091 140263261390720 learning.py:507] global step 353: loss = 6.1161 (0.310 sec/step)\n",
            "I0707 16:48:15.555809 140263261390720 learning.py:507] global step 354: loss = 6.5297 (0.303 sec/step)\n",
            "I0707 16:48:15.877577 140263261390720 learning.py:507] global step 355: loss = 4.0468 (0.320 sec/step)\n",
            "I0707 16:48:16.190252 140263261390720 learning.py:507] global step 356: loss = 5.5193 (0.311 sec/step)\n",
            "I0707 16:48:16.512451 140263261390720 learning.py:507] global step 357: loss = 5.9459 (0.320 sec/step)\n",
            "I0707 16:48:16.846731 140263261390720 learning.py:507] global step 358: loss = 5.7393 (0.333 sec/step)\n",
            "I0707 16:48:17.195959 140263261390720 learning.py:507] global step 359: loss = 5.9091 (0.347 sec/step)\n",
            "I0707 16:48:17.509125 140263261390720 learning.py:507] global step 360: loss = 6.0680 (0.311 sec/step)\n",
            "I0707 16:48:17.847325 140263261390720 learning.py:507] global step 361: loss = 8.4636 (0.336 sec/step)\n",
            "I0707 16:48:18.182625 140263261390720 learning.py:507] global step 362: loss = 6.2668 (0.333 sec/step)\n",
            "I0707 16:48:18.470893 140263261390720 learning.py:507] global step 363: loss = 5.4225 (0.287 sec/step)\n",
            "I0707 16:48:18.796152 140263261390720 learning.py:507] global step 364: loss = 6.7622 (0.323 sec/step)\n",
            "I0707 16:48:19.174000 140263261390720 learning.py:507] global step 365: loss = 4.6836 (0.376 sec/step)\n",
            "I0707 16:48:19.536077 140263261390720 learning.py:507] global step 366: loss = 5.9933 (0.360 sec/step)\n",
            "I0707 16:48:19.946727 140263261390720 learning.py:507] global step 367: loss = 5.5537 (0.409 sec/step)\n",
            "I0707 16:48:20.296812 140263261390720 learning.py:507] global step 368: loss = 5.7518 (0.348 sec/step)\n",
            "I0707 16:48:20.703312 140263261390720 learning.py:507] global step 369: loss = 5.1173 (0.405 sec/step)\n",
            "I0707 16:48:21.026992 140263261390720 learning.py:507] global step 370: loss = 6.6356 (0.322 sec/step)\n",
            "I0707 16:48:21.401592 140263261390720 learning.py:507] global step 371: loss = 5.0647 (0.373 sec/step)\n",
            "I0707 16:48:21.730940 140263261390720 learning.py:507] global step 372: loss = 5.3141 (0.327 sec/step)\n",
            "I0707 16:48:22.097657 140263261390720 learning.py:507] global step 373: loss = 4.2253 (0.365 sec/step)\n",
            "I0707 16:48:22.403044 140263261390720 learning.py:507] global step 374: loss = 5.7482 (0.304 sec/step)\n",
            "I0707 16:48:22.887676 140263261390720 learning.py:507] global step 375: loss = 6.2774 (0.483 sec/step)\n",
            "I0707 16:48:23.259975 140263261390720 learning.py:507] global step 376: loss = 4.5137 (0.370 sec/step)\n",
            "I0707 16:48:23.574964 140263261390720 learning.py:507] global step 377: loss = 5.0424 (0.313 sec/step)\n",
            "I0707 16:48:23.888245 140263261390720 learning.py:507] global step 378: loss = 5.8995 (0.312 sec/step)\n",
            "I0707 16:48:24.302008 140263261390720 learning.py:507] global step 379: loss = 4.1656 (0.412 sec/step)\n",
            "I0707 16:48:24.611349 140263261390720 learning.py:507] global step 380: loss = 6.4473 (0.308 sec/step)\n",
            "I0707 16:48:24.974814 140263261390720 learning.py:507] global step 381: loss = 5.1239 (0.361 sec/step)\n",
            "I0707 16:48:25.334784 140263261390720 learning.py:507] global step 382: loss = 5.6293 (0.358 sec/step)\n",
            "I0707 16:48:25.637166 140263261390720 learning.py:507] global step 383: loss = 5.0381 (0.301 sec/step)\n",
            "I0707 16:48:25.964877 140263261390720 learning.py:507] global step 384: loss = 6.3457 (0.326 sec/step)\n",
            "I0707 16:48:26.382034 140263261390720 learning.py:507] global step 385: loss = 5.4139 (0.415 sec/step)\n",
            "I0707 16:48:26.706439 140263261390720 learning.py:507] global step 386: loss = 6.0830 (0.322 sec/step)\n",
            "I0707 16:48:27.019070 140263261390720 learning.py:507] global step 387: loss = 5.2684 (0.311 sec/step)\n",
            "I0707 16:48:27.337512 140263261390720 learning.py:507] global step 388: loss = 4.6094 (0.317 sec/step)\n",
            "I0707 16:48:27.650905 140263261390720 learning.py:507] global step 389: loss = 5.2570 (0.312 sec/step)\n",
            "I0707 16:48:27.974961 140263261390720 learning.py:507] global step 390: loss = 5.9389 (0.322 sec/step)\n",
            "I0707 16:48:28.318852 140263261390720 learning.py:507] global step 391: loss = 5.1913 (0.342 sec/step)\n",
            "I0707 16:48:28.655328 140263261390720 learning.py:507] global step 392: loss = 5.6072 (0.334 sec/step)\n",
            "I0707 16:48:28.982466 140263261390720 learning.py:507] global step 393: loss = 5.2769 (0.325 sec/step)\n",
            "I0707 16:48:29.291980 140263261390720 learning.py:507] global step 394: loss = 4.7629 (0.307 sec/step)\n",
            "I0707 16:48:29.615166 140263261390720 learning.py:507] global step 395: loss = 5.1006 (0.321 sec/step)\n",
            "I0707 16:48:29.963320 140263261390720 learning.py:507] global step 396: loss = 5.6181 (0.346 sec/step)\n",
            "I0707 16:48:30.269163 140263261390720 learning.py:507] global step 397: loss = 4.7511 (0.304 sec/step)\n",
            "I0707 16:48:30.623854 140263261390720 learning.py:507] global step 398: loss = 6.3160 (0.353 sec/step)\n",
            "I0707 16:48:30.984868 140263261390720 learning.py:507] global step 399: loss = 6.1109 (0.359 sec/step)\n",
            "I0707 16:48:31.339236 140263261390720 learning.py:507] global step 400: loss = 5.5411 (0.353 sec/step)\n",
            "I0707 16:48:31.712904 140263261390720 learning.py:507] global step 401: loss = 6.2337 (0.372 sec/step)\n",
            "I0707 16:48:32.067866 140263261390720 learning.py:507] global step 402: loss = 6.3173 (0.353 sec/step)\n",
            "I0707 16:48:32.400422 140263261390720 learning.py:507] global step 403: loss = 5.0983 (0.331 sec/step)\n",
            "I0707 16:48:32.739725 140263261390720 learning.py:507] global step 404: loss = 6.1127 (0.337 sec/step)\n",
            "I0707 16:48:33.060592 140263261390720 learning.py:507] global step 405: loss = 5.6283 (0.319 sec/step)\n",
            "I0707 16:48:33.478276 140263261390720 learning.py:507] global step 406: loss = 5.2391 (0.413 sec/step)\n",
            "I0707 16:48:33.805424 140263261390720 learning.py:507] global step 407: loss = 6.8111 (0.325 sec/step)\n",
            "I0707 16:48:34.119580 140263261390720 learning.py:507] global step 408: loss = 6.3882 (0.313 sec/step)\n",
            "I0707 16:48:34.464779 140263261390720 learning.py:507] global step 409: loss = 6.4236 (0.343 sec/step)\n",
            "I0707 16:48:34.811230 140263261390720 learning.py:507] global step 410: loss = 6.1489 (0.345 sec/step)\n",
            "I0707 16:48:35.137473 140263261390720 learning.py:507] global step 411: loss = 5.7099 (0.325 sec/step)\n",
            "I0707 16:48:35.464882 140263261390720 learning.py:507] global step 412: loss = 6.6601 (0.326 sec/step)\n",
            "I0707 16:48:35.796870 140263261390720 learning.py:507] global step 413: loss = 5.4653 (0.330 sec/step)\n",
            "I0707 16:48:36.115428 140263261390720 learning.py:507] global step 414: loss = 5.1891 (0.317 sec/step)\n",
            "I0707 16:48:36.443226 140263261390720 learning.py:507] global step 415: loss = 5.0817 (0.326 sec/step)\n",
            "I0707 16:48:36.792763 140263261390720 learning.py:507] global step 416: loss = 5.5718 (0.348 sec/step)\n",
            "I0707 16:48:37.132048 140263261390720 learning.py:507] global step 417: loss = 5.5159 (0.337 sec/step)\n",
            "I0707 16:48:37.496753 140263261390720 learning.py:507] global step 418: loss = 4.7649 (0.363 sec/step)\n",
            "I0707 16:48:37.805746 140263261390720 learning.py:507] global step 419: loss = 5.4268 (0.307 sec/step)\n",
            "I0707 16:48:38.145785 140263261390720 learning.py:507] global step 420: loss = 6.5882 (0.338 sec/step)\n",
            "I0707 16:48:38.469589 140263261390720 learning.py:507] global step 421: loss = 5.6867 (0.322 sec/step)\n",
            "I0707 16:48:38.816195 140263261390720 learning.py:507] global step 422: loss = 4.7418 (0.344 sec/step)\n",
            "I0707 16:48:39.127426 140263261390720 learning.py:507] global step 423: loss = 4.4712 (0.309 sec/step)\n",
            "I0707 16:48:39.477021 140263261390720 learning.py:507] global step 424: loss = 5.4066 (0.348 sec/step)\n",
            "I0707 16:48:39.804267 140263261390720 learning.py:507] global step 425: loss = 6.0724 (0.325 sec/step)\n",
            "I0707 16:48:40.114627 140263261390720 learning.py:507] global step 426: loss = 4.7189 (0.309 sec/step)\n",
            "I0707 16:48:40.454718 140263261390720 learning.py:507] global step 427: loss = 6.4138 (0.338 sec/step)\n",
            "I0707 16:48:40.806731 140263261390720 learning.py:507] global step 428: loss = 6.3819 (0.350 sec/step)\n",
            "I0707 16:48:41.113844 140263261390720 learning.py:507] global step 429: loss = 5.2081 (0.305 sec/step)\n",
            "I0707 16:48:41.591697 140263261390720 learning.py:507] global step 430: loss = 7.0564 (0.476 sec/step)\n",
            "I0707 16:48:41.919004 140263261390720 learning.py:507] global step 431: loss = 6.1908 (0.325 sec/step)\n",
            "I0707 16:48:42.227680 140263261390720 learning.py:507] global step 432: loss = 6.3752 (0.307 sec/step)\n",
            "I0707 16:48:42.577619 140263261390720 learning.py:507] global step 433: loss = 4.9483 (0.348 sec/step)\n",
            "I0707 16:48:42.966316 140263261390720 learning.py:507] global step 434: loss = 6.2855 (0.387 sec/step)\n",
            "I0707 16:48:43.327488 140263261390720 learning.py:507] global step 435: loss = 4.3920 (0.359 sec/step)\n",
            "I0707 16:48:43.645843 140263261390720 learning.py:507] global step 436: loss = 6.1667 (0.317 sec/step)\n",
            "I0707 16:48:43.952425 140263261390720 learning.py:507] global step 437: loss = 5.0211 (0.305 sec/step)\n",
            "I0707 16:48:44.295731 140263261390720 learning.py:507] global step 438: loss = 5.8959 (0.341 sec/step)\n",
            "I0707 16:48:44.740310 140263261390720 learning.py:507] global step 439: loss = 5.9404 (0.443 sec/step)\n",
            "I0707 16:48:45.070674 140263261390720 learning.py:507] global step 440: loss = 5.5566 (0.328 sec/step)\n",
            "I0707 16:48:45.381639 140263261390720 learning.py:507] global step 441: loss = 5.9551 (0.309 sec/step)\n",
            "I0707 16:48:45.689010 140263261390720 learning.py:507] global step 442: loss = 5.2317 (0.306 sec/step)\n",
            "I0707 16:48:45.996345 140263261390720 learning.py:507] global step 443: loss = 6.5235 (0.306 sec/step)\n",
            "I0707 16:48:46.304852 140263261390720 learning.py:507] global step 444: loss = 5.0265 (0.307 sec/step)\n",
            "I0707 16:48:46.626907 140263261390720 learning.py:507] global step 445: loss = 6.7264 (0.320 sec/step)\n",
            "I0707 16:48:46.967336 140263261390720 learning.py:507] global step 446: loss = 5.9494 (0.339 sec/step)\n",
            "I0707 16:48:47.411864 140263261390720 learning.py:507] global step 447: loss = 4.6507 (0.443 sec/step)\n",
            "I0707 16:48:47.752785 140263261390720 learning.py:507] global step 448: loss = 6.1945 (0.339 sec/step)\n",
            "I0707 16:48:48.067631 140263261390720 learning.py:507] global step 449: loss = 7.1877 (0.313 sec/step)\n",
            "I0707 16:48:48.385574 140263261390720 learning.py:507] global step 450: loss = 4.8895 (0.316 sec/step)\n",
            "I0707 16:48:48.701503 140263261390720 learning.py:507] global step 451: loss = 5.2407 (0.314 sec/step)\n",
            "I0707 16:48:49.017024 140263261390720 learning.py:507] global step 452: loss = 6.3458 (0.312 sec/step)\n",
            "I0707 16:48:49.358302 140263261390720 learning.py:507] global step 453: loss = 6.0787 (0.339 sec/step)\n",
            "I0707 16:48:49.678475 140263261390720 learning.py:507] global step 454: loss = 4.6303 (0.318 sec/step)\n",
            "I0707 16:48:50.152632 140263261390720 learning.py:507] global step 455: loss = 5.4846 (0.472 sec/step)\n",
            "I0707 16:48:50.476227 140263261390720 learning.py:507] global step 456: loss = 6.3801 (0.322 sec/step)\n",
            "I0707 16:48:50.788274 140263261390720 learning.py:507] global step 457: loss = 6.1122 (0.310 sec/step)\n",
            "I0707 16:48:51.107907 140263261390720 learning.py:507] global step 458: loss = 7.1166 (0.318 sec/step)\n",
            "2019-07-07 16:48:51.183779: W tensorflow/core/framework/allocator.cc:107] Allocation of 1418195424 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1418199040 bytes == 0x1092f6000 @  0x7f9195999b6b 0x7f91959b9379 0x7f916f8d6337 0x7f916f8892af 0x7f916f58030b 0x7f916f54df86 0x7f916f54e875 0x7f9177eb44d8 0x7f9177f7130c 0x7f9177f718d8 0x7f9177e4625f 0x7f9177ecc0c3 0x7f9177ecb30b 0x7f916f7fd651 0x7f916f7fe8df 0x7f916f8a4fc9 0x7f916f8a1ea8 0x7f919429966f 0x7f919537b6db 0x7f91956b488f\n",
            "I0707 16:48:51.667229 140263261390720 learning.py:507] global step 459: loss = 4.7750 (0.558 sec/step)\n",
            "I0707 16:48:51.976118 140263261390720 learning.py:507] global step 460: loss = 6.1126 (0.307 sec/step)\n",
            "I0707 16:48:52.317317 140263261390720 learning.py:507] global step 461: loss = 6.3549 (0.340 sec/step)\n",
            "I0707 16:48:52.621783 140263261390720 learning.py:507] global step 462: loss = 4.8361 (0.303 sec/step)\n",
            "I0707 16:48:52.924468 140263261390720 learning.py:507] global step 463: loss = 5.0912 (0.301 sec/step)\n",
            "I0707 16:48:53.269591 140263261390720 learning.py:507] global step 464: loss = 5.1450 (0.343 sec/step)\n",
            "I0707 16:48:53.902620 140263261390720 learning.py:507] global step 465: loss = 5.6376 (0.631 sec/step)\n",
            "I0707 16:48:54.245508 140263261390720 learning.py:507] global step 466: loss = 5.2603 (0.341 sec/step)\n",
            "I0707 16:48:54.580221 140263261390720 learning.py:507] global step 467: loss = 5.4024 (0.333 sec/step)\n",
            "I0707 16:48:54.908187 140263261390720 learning.py:507] global step 468: loss = 4.2766 (0.326 sec/step)\n",
            "I0707 16:48:55.233729 140263261390720 learning.py:507] global step 469: loss = 5.8249 (0.323 sec/step)\n",
            "I0707 16:48:55.551721 140263261390720 learning.py:507] global step 470: loss = 5.9777 (0.316 sec/step)\n",
            "I0707 16:48:55.871938 140263261390720 learning.py:507] global step 471: loss = 5.2825 (0.319 sec/step)\n",
            "I0707 16:48:56.224534 140263261390720 learning.py:507] global step 472: loss = 4.7547 (0.351 sec/step)\n",
            "I0707 16:48:56.543177 140263261390720 learning.py:507] global step 473: loss = 5.3052 (0.317 sec/step)\n",
            "I0707 16:48:56.889346 140263261390720 learning.py:507] global step 474: loss = 5.1497 (0.345 sec/step)\n",
            "I0707 16:48:57.383279 140263261390720 learning.py:507] global step 475: loss = 5.4238 (0.492 sec/step)\n",
            "I0707 16:48:57.716510 140263261390720 learning.py:507] global step 476: loss = 5.1082 (0.331 sec/step)\n",
            "I0707 16:48:58.042020 140263261390720 learning.py:507] global step 477: loss = 6.9511 (0.324 sec/step)\n",
            "I0707 16:48:58.382188 140263261390720 learning.py:507] global step 478: loss = 5.8356 (0.338 sec/step)\n",
            "I0707 16:48:58.692581 140263261390720 learning.py:507] global step 479: loss = 5.5517 (0.308 sec/step)\n",
            "I0707 16:48:59.015083 140263261390720 learning.py:507] global step 480: loss = 5.7451 (0.321 sec/step)\n",
            "I0707 16:48:59.584666 140263261390720 learning.py:507] global step 481: loss = 5.6658 (0.565 sec/step)\n",
            "I0707 16:48:59.882375 140263261390720 learning.py:507] global step 482: loss = 4.8358 (0.296 sec/step)\n",
            "I0707 16:49:00.246674 140263261390720 learning.py:507] global step 483: loss = 5.0376 (0.363 sec/step)\n",
            "I0707 16:49:00.591351 140263261390720 learning.py:507] global step 484: loss = 6.8748 (0.343 sec/step)\n",
            "I0707 16:49:00.940804 140263261390720 learning.py:507] global step 485: loss = 5.9421 (0.348 sec/step)\n",
            "I0707 16:49:01.236651 140263261390720 learning.py:507] global step 486: loss = 7.0200 (0.294 sec/step)\n",
            "I0707 16:49:01.569435 140263261390720 learning.py:507] global step 487: loss = 6.2293 (0.331 sec/step)\n",
            "I0707 16:49:01.893786 140263261390720 learning.py:507] global step 488: loss = 5.8996 (0.323 sec/step)\n",
            "I0707 16:49:02.302779 140263261390720 learning.py:507] global step 489: loss = 4.7795 (0.407 sec/step)\n",
            "I0707 16:49:02.611901 140263261390720 learning.py:507] global step 490: loss = 5.5052 (0.307 sec/step)\n",
            "I0707 16:49:02.922430 140263261390720 learning.py:507] global step 491: loss = 6.1403 (0.308 sec/step)\n",
            "I0707 16:49:03.216359 140263261390720 learning.py:507] global step 492: loss = 5.7924 (0.292 sec/step)\n",
            "I0707 16:49:03.588681 140263261390720 learning.py:507] global step 493: loss = 4.4545 (0.371 sec/step)\n",
            "I0707 16:49:03.939006 140263261390720 learning.py:507] global step 494: loss = 4.2215 (0.349 sec/step)\n",
            "I0707 16:49:04.247098 140263261390720 learning.py:507] global step 495: loss = 5.6271 (0.306 sec/step)\n",
            "I0707 16:49:04.580605 140263261390720 learning.py:507] global step 496: loss = 5.0575 (0.332 sec/step)\n",
            "I0707 16:49:04.889402 140263261390720 learning.py:507] global step 497: loss = 4.7554 (0.307 sec/step)\n",
            "I0707 16:49:05.299525 140263261390720 learning.py:507] global step 498: loss = 6.0081 (0.408 sec/step)\n",
            "I0707 16:49:05.629125 140263261390720 learning.py:507] global step 499: loss = 4.9564 (0.328 sec/step)\n",
            "I0707 16:49:05.958241 140263261390720 learning.py:507] global step 500: loss = 5.7545 (0.327 sec/step)\n",
            "I0707 16:49:06.280444 140263261390720 learning.py:507] global step 501: loss = 4.2799 (0.320 sec/step)\n",
            "I0707 16:49:06.603410 140263261390720 learning.py:507] global step 502: loss = 4.2662 (0.321 sec/step)\n",
            "I0707 16:49:06.955899 140263261390720 learning.py:507] global step 503: loss = 5.8436 (0.351 sec/step)\n",
            "I0707 16:49:07.333105 140263261390720 learning.py:507] global step 504: loss = 5.4325 (0.376 sec/step)\n",
            "I0707 16:49:07.659252 140263261390720 learning.py:507] global step 505: loss = 6.2063 (0.324 sec/step)\n",
            "I0707 16:49:07.964872 140263261390720 learning.py:507] global step 506: loss = 6.5685 (0.304 sec/step)\n",
            "I0707 16:49:08.360193 140263261390720 learning.py:507] global step 507: loss = 4.4558 (0.393 sec/step)\n",
            "I0707 16:49:08.685012 140263261390720 learning.py:507] global step 508: loss = 4.3866 (0.323 sec/step)\n",
            "I0707 16:49:09.021795 140263261390720 learning.py:507] global step 509: loss = 5.3150 (0.335 sec/step)\n",
            "I0707 16:49:09.354735 140263261390720 learning.py:507] global step 510: loss = 5.3010 (0.331 sec/step)\n",
            "I0707 16:49:09.783166 140263261390720 learning.py:507] global step 511: loss = 5.1630 (0.427 sec/step)\n",
            "I0707 16:49:10.211112 140263261390720 learning.py:507] global step 512: loss = 6.1787 (0.426 sec/step)\n",
            "I0707 16:49:10.556018 140263261390720 learning.py:507] global step 513: loss = 5.6335 (0.343 sec/step)\n",
            "I0707 16:49:10.978540 140263261390720 learning.py:507] global step 514: loss = 5.1423 (0.421 sec/step)\n",
            "I0707 16:49:11.324632 140263261390720 learning.py:507] global step 515: loss = 5.6391 (0.344 sec/step)\n",
            "I0707 16:49:11.633213 140263261390720 learning.py:507] global step 516: loss = 6.4052 (0.307 sec/step)\n",
            "I0707 16:49:11.988609 140263261390720 learning.py:507] global step 517: loss = 5.0661 (0.354 sec/step)\n",
            "I0707 16:49:12.300879 140263261390720 learning.py:507] global step 518: loss = 4.9944 (0.310 sec/step)\n",
            "I0707 16:49:12.704216 140263261390720 learning.py:507] global step 519: loss = 5.5765 (0.401 sec/step)\n",
            "I0707 16:49:13.015746 140263261390720 learning.py:507] global step 520: loss = 5.5271 (0.310 sec/step)\n",
            "I0707 16:49:13.455043 140263261390720 learning.py:507] global step 521: loss = 6.2206 (0.437 sec/step)\n",
            "I0707 16:49:13.840287 140263261390720 learning.py:507] global step 522: loss = 4.5597 (0.383 sec/step)\n",
            "I0707 16:49:14.222312 140263261390720 learning.py:507] global step 523: loss = 4.9512 (0.380 sec/step)\n",
            "I0707 16:49:14.524804 140263261390720 learning.py:507] global step 524: loss = 5.2948 (0.301 sec/step)\n",
            "I0707 16:49:14.857846 140263261390720 learning.py:507] global step 525: loss = 5.0475 (0.331 sec/step)\n",
            "I0707 16:49:15.248746 140263261390720 learning.py:507] global step 526: loss = 4.6564 (0.389 sec/step)\n",
            "I0707 16:49:15.590440 140263261390720 learning.py:507] global step 527: loss = 4.7050 (0.340 sec/step)\n",
            "I0707 16:49:15.906121 140263261390720 learning.py:507] global step 528: loss = 4.4635 (0.314 sec/step)\n",
            "I0707 16:49:16.255517 140263261390720 learning.py:507] global step 529: loss = 4.3237 (0.348 sec/step)\n",
            "I0707 16:49:16.573980 140263261390720 learning.py:507] global step 530: loss = 6.7672 (0.317 sec/step)\n",
            "I0707 16:49:16.969656 140263261390720 learning.py:507] global step 531: loss = 4.4604 (0.394 sec/step)\n",
            "I0707 16:49:17.393472 140263261390720 learning.py:507] global step 532: loss = 5.2356 (0.422 sec/step)\n",
            "I0707 16:49:17.706252 140263261390720 learning.py:507] global step 533: loss = 5.9105 (0.311 sec/step)\n",
            "I0707 16:49:18.050186 140263261390720 learning.py:507] global step 534: loss = 7.3837 (0.342 sec/step)\n",
            "I0707 16:49:18.392721 140263261390720 learning.py:507] global step 535: loss = 5.4500 (0.341 sec/step)\n",
            "I0707 16:49:18.703379 140263261390720 learning.py:507] global step 536: loss = 6.1745 (0.309 sec/step)\n",
            "I0707 16:49:19.032124 140263261390720 learning.py:507] global step 537: loss = 5.7760 (0.327 sec/step)\n",
            "I0707 16:49:19.372633 140263261390720 learning.py:507] global step 538: loss = 4.8288 (0.339 sec/step)\n",
            "I0707 16:49:19.786661 140263261390720 learning.py:507] global step 539: loss = 6.6764 (0.412 sec/step)\n",
            "I0707 16:49:20.110771 140263261390720 learning.py:507] global step 540: loss = 5.8576 (0.322 sec/step)\n",
            "I0707 16:49:20.532516 140263261390720 learning.py:507] global step 541: loss = 5.1262 (0.420 sec/step)\n",
            "I0707 16:49:20.872086 140263261390720 learning.py:507] global step 542: loss = 5.5137 (0.338 sec/step)\n",
            "I0707 16:49:21.203588 140263261390720 learning.py:507] global step 543: loss = 7.4601 (0.329 sec/step)\n",
            "I0707 16:49:21.614128 140263261390720 learning.py:507] global step 544: loss = 5.8147 (0.408 sec/step)\n",
            "I0707 16:49:22.085349 140263261390720 learning.py:507] global step 545: loss = 5.4958 (0.470 sec/step)\n",
            "I0707 16:49:22.391793 140263261390720 learning.py:507] global step 546: loss = 6.0587 (0.305 sec/step)\n",
            "I0707 16:49:22.838171 140263261390720 learning.py:507] global step 547: loss = 5.2816 (0.445 sec/step)\n",
            "I0707 16:49:23.244270 140263261390720 learning.py:507] global step 548: loss = 5.4320 (0.404 sec/step)\n",
            "I0707 16:49:23.575965 140263261390720 learning.py:507] global step 549: loss = 4.3898 (0.330 sec/step)\n",
            "I0707 16:49:23.891609 140263261390720 learning.py:507] global step 550: loss = 5.8235 (0.314 sec/step)\n",
            "I0707 16:49:24.226123 140263261390720 learning.py:507] global step 551: loss = 6.1275 (0.333 sec/step)\n",
            "I0707 16:49:24.596072 140263261390720 learning.py:507] global step 552: loss = 5.2067 (0.368 sec/step)\n",
            "I0707 16:49:24.892454 140263261390720 learning.py:507] global step 553: loss = 4.3634 (0.295 sec/step)\n",
            "I0707 16:49:25.288004 140263261390720 learning.py:507] global step 554: loss = 5.3462 (0.394 sec/step)\n",
            "I0707 16:49:25.634767 140263261390720 learning.py:507] global step 555: loss = 5.1887 (0.345 sec/step)\n",
            "I0707 16:49:25.935177 140263261390720 learning.py:507] global step 556: loss = 5.4424 (0.298 sec/step)\n",
            "I0707 16:49:26.276403 140263261390720 learning.py:507] global step 557: loss = 5.8312 (0.340 sec/step)\n",
            "I0707 16:49:26.616531 140263261390720 learning.py:507] global step 558: loss = 4.0113 (0.338 sec/step)\n",
            "I0707 16:49:27.032185 140263261390720 learning.py:507] global step 559: loss = 5.6374 (0.414 sec/step)\n",
            "I0707 16:49:27.473729 140263261390720 learning.py:507] global step 560: loss = 5.6933 (0.440 sec/step)\n",
            "I0707 16:49:27.785620 140263261390720 learning.py:507] global step 561: loss = 4.7531 (0.310 sec/step)\n",
            "I0707 16:49:28.084178 140263261390720 learning.py:507] global step 562: loss = 4.9692 (0.297 sec/step)\n",
            "I0707 16:49:28.407737 140263261390720 learning.py:507] global step 563: loss = 4.6072 (0.322 sec/step)\n",
            "I0707 16:49:28.712302 140263261390720 learning.py:507] global step 564: loss = 5.5612 (0.303 sec/step)\n",
            "I0707 16:49:29.001683 140263261390720 learning.py:507] global step 565: loss = 5.4682 (0.287 sec/step)\n",
            "I0707 16:49:29.312170 140263261390720 learning.py:507] global step 566: loss = 6.3756 (0.309 sec/step)\n",
            "I0707 16:49:29.637453 140263261390720 learning.py:507] global step 567: loss = 3.8170 (0.324 sec/step)\n",
            "I0707 16:49:30.050070 140263261390720 learning.py:507] global step 568: loss = 5.9659 (0.411 sec/step)\n",
            "I0707 16:49:30.376932 140263261390720 learning.py:507] global step 569: loss = 5.7543 (0.325 sec/step)\n",
            "I0707 16:49:30.686919 140263261390720 learning.py:507] global step 570: loss = 4.7044 (0.309 sec/step)\n",
            "I0707 16:49:30.993936 140263261390720 learning.py:507] global step 571: loss = 6.0910 (0.305 sec/step)\n",
            "I0707 16:49:31.408305 140263261390720 learning.py:507] global step 572: loss = 6.3165 (0.413 sec/step)\n",
            "I0707 16:49:31.715994 140263261390720 learning.py:507] global step 573: loss = 5.2712 (0.306 sec/step)\n",
            "I0707 16:49:32.011585 140263261390720 learning.py:507] global step 574: loss = 4.3757 (0.294 sec/step)\n",
            "I0707 16:49:32.339396 140263261390720 learning.py:507] global step 575: loss = 5.4448 (0.326 sec/step)\n",
            "I0707 16:49:32.631918 140263261390720 learning.py:507] global step 576: loss = 8.0809 (0.291 sec/step)\n",
            "I0707 16:49:32.930750 140263261390720 learning.py:507] global step 577: loss = 4.9687 (0.297 sec/step)\n",
            "I0707 16:49:33.429961 140263261390720 learning.py:507] global step 578: loss = 6.9475 (0.498 sec/step)\n",
            "I0707 16:49:33.740273 140263261390720 learning.py:507] global step 579: loss = 5.3909 (0.309 sec/step)\n",
            "I0707 16:49:34.028595 140263261390720 learning.py:507] global step 580: loss = 5.1573 (0.287 sec/step)\n",
            "I0707 16:49:34.355993 140263261390720 learning.py:507] global step 581: loss = 4.5382 (0.326 sec/step)\n",
            "I0707 16:49:34.691393 140263261390720 learning.py:507] global step 582: loss = 5.6497 (0.334 sec/step)\n",
            "I0707 16:49:35.011922 140263261390720 learning.py:507] global step 583: loss = 4.5623 (0.319 sec/step)\n",
            "I0707 16:49:35.342363 140263261390720 learning.py:507] global step 584: loss = 4.9945 (0.329 sec/step)\n",
            "I0707 16:49:35.686011 140263261390720 learning.py:507] global step 585: loss = 5.1743 (0.342 sec/step)\n",
            "I0707 16:49:36.105000 140263261390720 learning.py:507] global step 586: loss = 6.1806 (0.417 sec/step)\n",
            "I0707 16:49:36.455739 140263261390720 learning.py:507] global step 587: loss = 5.4432 (0.349 sec/step)\n",
            "I0707 16:49:36.802517 140263261390720 learning.py:507] global step 588: loss = 5.3488 (0.345 sec/step)\n",
            "I0707 16:49:37.109662 140263261390720 learning.py:507] global step 589: loss = 5.3310 (0.305 sec/step)\n",
            "I0707 16:49:37.413589 140263261390720 learning.py:507] global step 590: loss = 6.6746 (0.302 sec/step)\n",
            "I0707 16:49:37.778216 140263261390720 learning.py:507] global step 591: loss = 4.8837 (0.363 sec/step)\n",
            "I0707 16:49:38.094476 140263261390720 learning.py:507] global step 592: loss = 5.5961 (0.315 sec/step)\n",
            "I0707 16:49:38.450286 140263261390720 learning.py:507] global step 593: loss = 6.0061 (0.354 sec/step)\n",
            "I0707 16:49:38.760843 140263261390720 learning.py:507] global step 594: loss = 6.7213 (0.309 sec/step)\n",
            "I0707 16:49:39.137791 140263261390720 learning.py:507] global step 595: loss = 5.1206 (0.375 sec/step)\n",
            "I0707 16:49:39.449568 140263261390720 learning.py:507] global step 596: loss = 4.8513 (0.310 sec/step)\n",
            "I0707 16:49:39.770310 140263261390720 learning.py:507] global step 597: loss = 4.2179 (0.319 sec/step)\n",
            "I0707 16:49:40.095387 140263261390720 learning.py:507] global step 598: loss = 5.7856 (0.323 sec/step)\n",
            "I0707 16:49:40.417474 140263261390720 learning.py:507] global step 599: loss = 4.6384 (0.319 sec/step)\n",
            "I0707 16:49:40.742399 140263261390720 learning.py:507] global step 600: loss = 5.3596 (0.323 sec/step)\n",
            "I0707 16:49:41.077815 140263261390720 learning.py:507] global step 601: loss = 5.0450 (0.334 sec/step)\n",
            "I0707 16:49:41.392594 140263261390720 learning.py:507] global step 602: loss = 5.5317 (0.313 sec/step)\n",
            "I0707 16:49:41.781988 140263261390720 learning.py:507] global step 603: loss = 5.2326 (0.388 sec/step)\n",
            "I0707 16:49:42.100522 140263261390720 learning.py:507] global step 604: loss = 5.5352 (0.316 sec/step)\n",
            "I0707 16:49:42.402252 140263261390720 learning.py:507] global step 605: loss = 6.5174 (0.300 sec/step)\n",
            "I0707 16:49:42.783507 140263261390720 learning.py:507] global step 606: loss = 5.2494 (0.380 sec/step)\n",
            "I0707 16:49:43.094760 140263261390720 learning.py:507] global step 607: loss = 4.3110 (0.310 sec/step)\n",
            "I0707 16:49:43.400408 140263261390720 learning.py:507] global step 608: loss = 5.6909 (0.304 sec/step)\n",
            "I0707 16:49:43.711967 140263261390720 learning.py:507] global step 609: loss = 6.9149 (0.309 sec/step)\n",
            "I0707 16:49:44.034975 140263261390720 learning.py:507] global step 610: loss = 4.8130 (0.321 sec/step)\n",
            "I0707 16:49:44.345981 140263261390720 learning.py:507] global step 611: loss = 4.9812 (0.309 sec/step)\n",
            "I0707 16:49:44.672614 140263261390720 learning.py:507] global step 612: loss = 4.7719 (0.325 sec/step)\n",
            "I0707 16:49:45.023532 140263261390720 learning.py:507] global step 613: loss = 7.3228 (0.349 sec/step)\n",
            "I0707 16:49:45.341459 140263261390720 learning.py:507] global step 614: loss = 4.6892 (0.316 sec/step)\n",
            "I0707 16:49:45.688676 140263261390720 learning.py:507] global step 615: loss = 6.4794 (0.345 sec/step)\n",
            "I0707 16:49:46.027235 140263261390720 learning.py:507] global step 616: loss = 6.2125 (0.336 sec/step)\n",
            "I0707 16:49:46.323065 140263261390720 learning.py:507] global step 617: loss = 4.8077 (0.294 sec/step)\n",
            "I0707 16:49:46.666676 140263261390720 learning.py:507] global step 618: loss = 4.6772 (0.342 sec/step)\n",
            "I0707 16:49:46.998785 140263261390720 learning.py:507] global step 619: loss = 4.9997 (0.330 sec/step)\n",
            "I0707 16:49:47.325779 140263261390720 learning.py:507] global step 620: loss = 7.0987 (0.325 sec/step)\n",
            "I0707 16:49:47.673653 140263261390720 learning.py:507] global step 621: loss = 4.7716 (0.346 sec/step)\n",
            "I0707 16:49:48.017866 140263261390720 learning.py:507] global step 622: loss = 5.1935 (0.342 sec/step)\n",
            "I0707 16:49:48.372417 140263261390720 learning.py:507] global step 623: loss = 5.2373 (0.353 sec/step)\n",
            "I0707 16:49:48.716918 140263261390720 learning.py:507] global step 624: loss = 6.4361 (0.343 sec/step)\n",
            "I0707 16:49:49.033786 140263261390720 learning.py:507] global step 625: loss = 6.5863 (0.315 sec/step)\n",
            "I0707 16:49:49.382673 140263261390720 learning.py:507] global step 626: loss = 5.9612 (0.347 sec/step)\n",
            "I0707 16:49:49.698732 140263261390720 learning.py:507] global step 627: loss = 4.8020 (0.314 sec/step)\n",
            "I0707 16:49:50.013972 140263261390720 learning.py:507] global step 628: loss = 6.1956 (0.313 sec/step)\n",
            "I0707 16:49:50.423356 140263261390720 learning.py:507] global step 629: loss = 5.2791 (0.408 sec/step)\n",
            "I0707 16:49:50.730746 140263261390720 learning.py:507] global step 630: loss = 4.3810 (0.305 sec/step)\n",
            "I0707 16:49:51.038269 140263261390720 learning.py:507] global step 631: loss = 5.6366 (0.306 sec/step)\n",
            "I0707 16:49:51.506439 140263261390720 learning.py:507] global step 632: loss = 5.1809 (0.466 sec/step)\n",
            "I0707 16:49:51.817050 140263261390720 learning.py:507] global step 633: loss = 5.3898 (0.309 sec/step)\n",
            "I0707 16:49:52.146537 140263261390720 learning.py:507] global step 634: loss = 4.9244 (0.328 sec/step)\n",
            "I0707 16:49:52.500693 140263261390720 learning.py:507] global step 635: loss = 4.6375 (0.352 sec/step)\n",
            "I0707 16:49:52.814792 140263261390720 learning.py:507] global step 636: loss = 5.2324 (0.312 sec/step)\n",
            "I0707 16:49:53.359489 140263261390720 learning.py:507] global step 637: loss = 5.8657 (0.532 sec/step)\n",
            "I0707 16:49:53.599106 140260287772416 supervisor.py:1050] Recording summary at step 637.\n",
            "I0707 16:49:53.894425 140263261390720 learning.py:507] global step 638: loss = 5.4644 (0.462 sec/step)\n",
            "I0707 16:49:54.346158 140263261390720 learning.py:507] global step 639: loss = 5.9755 (0.450 sec/step)\n",
            "I0707 16:49:54.691508 140263261390720 learning.py:507] global step 640: loss = 4.0503 (0.344 sec/step)\n",
            "I0707 16:49:54.837910 140260296165120 supervisor.py:1099] global_step/sec: 2.88303\n",
            "I0707 16:49:55.015457 140263261390720 learning.py:507] global step 641: loss = 4.5587 (0.320 sec/step)\n",
            "I0707 16:49:55.423315 140263261390720 learning.py:507] global step 642: loss = 6.3678 (0.406 sec/step)\n",
            "I0707 16:49:55.830752 140263261390720 learning.py:507] global step 643: loss = 4.4736 (0.406 sec/step)\n",
            "I0707 16:49:56.169998 140263261390720 learning.py:507] global step 644: loss = 4.3215 (0.338 sec/step)\n",
            "I0707 16:49:56.495969 140263261390720 learning.py:507] global step 645: loss = 4.1336 (0.324 sec/step)\n",
            "I0707 16:49:56.812276 140263261390720 learning.py:507] global step 646: loss = 4.7017 (0.315 sec/step)\n",
            "I0707 16:49:57.108896 140263261390720 learning.py:507] global step 647: loss = 4.7041 (0.295 sec/step)\n",
            "I0707 16:49:57.523390 140263261390720 learning.py:507] global step 648: loss = 4.7408 (0.413 sec/step)\n",
            "I0707 16:49:57.853102 140263261390720 learning.py:507] global step 649: loss = 5.8480 (0.325 sec/step)\n",
            "I0707 16:49:58.342224 140263261390720 learning.py:507] global step 650: loss = 4.5356 (0.486 sec/step)\n",
            "I0707 16:49:58.650671 140263261390720 learning.py:507] global step 651: loss = 5.5750 (0.307 sec/step)\n",
            "I0707 16:49:58.951907 140263261390720 learning.py:507] global step 652: loss = 5.6542 (0.300 sec/step)\n",
            "I0707 16:49:59.298293 140263261390720 learning.py:507] global step 653: loss = 4.9305 (0.345 sec/step)\n",
            "I0707 16:49:59.642178 140263261390720 learning.py:507] global step 654: loss = 5.3158 (0.342 sec/step)\n",
            "I0707 16:49:59.952190 140263261390720 learning.py:507] global step 655: loss = 5.1966 (0.308 sec/step)\n",
            "I0707 16:50:00.303180 140263261390720 learning.py:507] global step 656: loss = 4.5308 (0.349 sec/step)\n",
            "I0707 16:50:00.629506 140263261390720 learning.py:507] global step 657: loss = 5.2165 (0.325 sec/step)\n",
            "I0707 16:50:01.023738 140263261390720 learning.py:507] global step 658: loss = 6.3076 (0.393 sec/step)\n",
            "2019-07-07 16:50:01.039044: W tensorflow/core/framework/allocator.cc:107] Allocation of 1252491264 exceeds 10% of system memory.\n",
            "I0707 16:50:01.520890 140263261390720 learning.py:507] global step 659: loss = 6.2580 (0.496 sec/step)\n",
            "I0707 16:50:01.832138 140263261390720 learning.py:507] global step 660: loss = 4.8263 (0.310 sec/step)\n",
            "I0707 16:50:02.239381 140263261390720 learning.py:507] global step 661: loss = 4.3486 (0.404 sec/step)\n",
            "I0707 16:50:02.576164 140263261390720 learning.py:507] global step 662: loss = 5.2136 (0.333 sec/step)\n",
            "I0707 16:50:02.893107 140263261390720 learning.py:507] global step 663: loss = 4.6619 (0.315 sec/step)\n",
            "I0707 16:50:03.190631 140263261390720 learning.py:507] global step 664: loss = 4.5827 (0.296 sec/step)\n",
            "I0707 16:50:03.796743 140263261390720 learning.py:507] global step 665: loss = 4.5749 (0.604 sec/step)\n",
            "I0707 16:50:04.110308 140263261390720 learning.py:507] global step 666: loss = 5.6142 (0.312 sec/step)\n",
            "I0707 16:50:04.444408 140263261390720 learning.py:507] global step 667: loss = 6.6378 (0.332 sec/step)\n",
            "I0707 16:50:04.730146 140263261390720 learning.py:507] global step 668: loss = 4.7137 (0.284 sec/step)\n",
            "I0707 16:50:05.169851 140263261390720 learning.py:507] global step 669: loss = 4.1375 (0.438 sec/step)\n",
            "I0707 16:50:05.538535 140263261390720 learning.py:507] global step 670: loss = 6.0628 (0.367 sec/step)\n",
            "I0707 16:50:05.824966 140263261390720 learning.py:507] global step 671: loss = 5.1104 (0.285 sec/step)\n",
            "I0707 16:50:06.141637 140263261390720 learning.py:507] global step 672: loss = 4.4921 (0.315 sec/step)\n",
            "I0707 16:50:06.457201 140263261390720 learning.py:507] global step 673: loss = 4.6642 (0.314 sec/step)\n",
            "I0707 16:50:06.772582 140263261390720 learning.py:507] global step 674: loss = 6.5733 (0.314 sec/step)\n",
            "I0707 16:50:07.126183 140263261390720 learning.py:507] global step 675: loss = 5.0225 (0.352 sec/step)\n",
            "I0707 16:50:07.553152 140263261390720 learning.py:507] global step 676: loss = 5.5073 (0.425 sec/step)\n",
            "I0707 16:50:07.841595 140263261390720 learning.py:507] global step 677: loss = 5.4979 (0.287 sec/step)\n",
            "tcmalloc: large alloc 1695629312 bytes == 0xff332000 @  0x7f9195999b6b 0x7f91959b9379 0x7f916f8d6337 0x7f916f8892af 0x7f916f58030b 0x7f916f54df86 0x7f916f54e875 0x7f9177eb44d8 0x7f9177f7130c 0x7f9177f718d8 0x7f9177e4625f 0x7f9177ecc0c3 0x7f9177ecb30b 0x7f916f7fd651 0x7f916f7fe8df 0x7f916f8a4fc9 0x7f916f8a1ea8 0x7f919429966f 0x7f919537b6db 0x7f91956b488f\n",
            "I0707 16:50:08.441455 140263261390720 learning.py:507] global step 678: loss = 5.6682 (0.598 sec/step)\n",
            "I0707 16:50:08.729530 140263261390720 learning.py:507] global step 679: loss = 5.1654 (0.286 sec/step)\n",
            "I0707 16:50:09.047741 140263261390720 learning.py:507] global step 680: loss = 6.7249 (0.317 sec/step)\n",
            "I0707 16:50:09.345963 140263261390720 learning.py:507] global step 681: loss = 4.5419 (0.297 sec/step)\n",
            "I0707 16:50:09.665017 140263261390720 learning.py:507] global step 682: loss = 5.4729 (0.317 sec/step)\n",
            "I0707 16:50:09.979249 140263261390720 learning.py:507] global step 683: loss = 4.7427 (0.313 sec/step)\n",
            "I0707 16:50:10.705908 140263261390720 learning.py:507] global step 684: loss = 5.8480 (0.725 sec/step)\n",
            "I0707 16:50:11.063486 140263261390720 learning.py:507] global step 685: loss = 5.9174 (0.356 sec/step)\n",
            "I0707 16:50:11.393208 140263261390720 learning.py:507] global step 686: loss = 5.8712 (0.328 sec/step)\n",
            "I0707 16:50:11.706612 140263261390720 learning.py:507] global step 687: loss = 4.9004 (0.312 sec/step)\n",
            "I0707 16:50:12.047660 140263261390720 learning.py:507] global step 688: loss = 5.9295 (0.339 sec/step)\n",
            "I0707 16:50:12.491048 140263261390720 learning.py:507] global step 689: loss = 4.7185 (0.442 sec/step)\n",
            "I0707 16:50:12.824975 140263261390720 learning.py:507] global step 690: loss = 6.6796 (0.332 sec/step)\n",
            "I0707 16:50:13.317361 140263261390720 learning.py:507] global step 691: loss = 5.5351 (0.491 sec/step)\n",
            "I0707 16:50:13.666676 140263261390720 learning.py:507] global step 692: loss = 6.1153 (0.348 sec/step)\n",
            "I0707 16:50:13.990846 140263261390720 learning.py:507] global step 693: loss = 5.3622 (0.323 sec/step)\n",
            "I0707 16:50:14.314146 140263261390720 learning.py:507] global step 694: loss = 5.3472 (0.322 sec/step)\n",
            "I0707 16:50:14.832217 140263261390720 learning.py:507] global step 695: loss = 6.2647 (0.516 sec/step)\n",
            "I0707 16:50:15.150293 140263261390720 learning.py:507] global step 696: loss = 5.5518 (0.316 sec/step)\n",
            "I0707 16:50:15.461308 140263261390720 learning.py:507] global step 697: loss = 5.6853 (0.309 sec/step)\n",
            "I0707 16:50:15.791377 140263261390720 learning.py:507] global step 698: loss = 5.6596 (0.328 sec/step)\n",
            "I0707 16:50:16.178408 140263261390720 learning.py:507] global step 699: loss = 5.9537 (0.385 sec/step)\n",
            "I0707 16:50:16.606993 140263261390720 learning.py:507] global step 700: loss = 4.4597 (0.427 sec/step)\n",
            "I0707 16:50:16.943132 140263261390720 learning.py:507] global step 701: loss = 5.5888 (0.335 sec/step)\n",
            "I0707 16:50:17.285421 140263261390720 learning.py:507] global step 702: loss = 5.1565 (0.340 sec/step)\n",
            "I0707 16:50:17.640070 140263261390720 learning.py:507] global step 703: loss = 4.1280 (0.353 sec/step)\n",
            "I0707 16:50:17.981890 140263261390720 learning.py:507] global step 704: loss = 5.7885 (0.340 sec/step)\n",
            "I0707 16:50:18.307957 140263261390720 learning.py:507] global step 705: loss = 4.7633 (0.325 sec/step)\n",
            "I0707 16:50:18.631398 140263261390720 learning.py:507] global step 706: loss = 5.1296 (0.322 sec/step)\n",
            "I0707 16:50:18.938699 140263261390720 learning.py:507] global step 707: loss = 5.5943 (0.306 sec/step)\n",
            "I0707 16:50:19.263430 140263261390720 learning.py:507] global step 708: loss = 5.7152 (0.323 sec/step)\n",
            "I0707 16:50:19.670988 140263261390720 learning.py:507] global step 709: loss = 6.0216 (0.406 sec/step)\n",
            "I0707 16:50:19.992118 140263261390720 learning.py:507] global step 710: loss = 4.4905 (0.320 sec/step)\n",
            "I0707 16:50:20.339473 140263261390720 learning.py:507] global step 711: loss = 4.8979 (0.346 sec/step)\n",
            "I0707 16:50:20.652702 140263261390720 learning.py:507] global step 712: loss = 4.3049 (0.312 sec/step)\n",
            "I0707 16:50:21.005706 140263261390720 learning.py:507] global step 713: loss = 4.0170 (0.351 sec/step)\n",
            "I0707 16:50:21.330947 140263261390720 learning.py:507] global step 714: loss = 5.6536 (0.323 sec/step)\n",
            "I0707 16:50:21.638995 140263261390720 learning.py:507] global step 715: loss = 6.4388 (0.306 sec/step)\n",
            "I0707 16:50:21.966857 140263261390720 learning.py:507] global step 716: loss = 6.0099 (0.326 sec/step)\n",
            "I0707 16:50:22.277762 140263261390720 learning.py:507] global step 717: loss = 5.4236 (0.309 sec/step)\n",
            "I0707 16:50:22.635770 140263261390720 learning.py:507] global step 718: loss = 4.5017 (0.356 sec/step)\n",
            "I0707 16:50:23.080590 140263261390720 learning.py:507] global step 719: loss = 4.9446 (0.443 sec/step)\n",
            "I0707 16:50:23.380382 140263261390720 learning.py:507] global step 720: loss = 5.6655 (0.298 sec/step)\n",
            "I0707 16:50:23.713105 140263261390720 learning.py:507] global step 721: loss = 4.7483 (0.331 sec/step)\n",
            "I0707 16:50:24.086716 140263261390720 learning.py:507] global step 722: loss = 5.3680 (0.372 sec/step)\n",
            "I0707 16:50:24.491328 140263261390720 learning.py:507] global step 723: loss = 5.3623 (0.403 sec/step)\n",
            "I0707 16:50:24.860195 140263261390720 learning.py:507] global step 724: loss = 5.5322 (0.367 sec/step)\n",
            "I0707 16:50:25.303909 140263261390720 learning.py:507] global step 725: loss = 5.0407 (0.442 sec/step)\n",
            "I0707 16:50:25.610275 140263261390720 learning.py:507] global step 726: loss = 5.5555 (0.305 sec/step)\n",
            "I0707 16:50:25.985139 140263261390720 learning.py:507] global step 727: loss = 5.6385 (0.373 sec/step)\n",
            "I0707 16:50:26.345445 140263261390720 learning.py:507] global step 728: loss = 6.5833 (0.359 sec/step)\n",
            "I0707 16:50:26.702387 140263261390720 learning.py:507] global step 729: loss = 6.1143 (0.355 sec/step)\n",
            "I0707 16:50:27.011060 140263261390720 learning.py:507] global step 730: loss = 5.1334 (0.307 sec/step)\n",
            "I0707 16:50:27.336681 140263261390720 learning.py:507] global step 731: loss = 5.8967 (0.324 sec/step)\n",
            "I0707 16:50:27.795270 140263261390720 learning.py:507] global step 732: loss = 4.6185 (0.457 sec/step)\n",
            "I0707 16:50:28.185232 140263261390720 learning.py:507] global step 733: loss = 5.6333 (0.388 sec/step)\n",
            "I0707 16:50:28.481884 140263261390720 learning.py:507] global step 734: loss = 4.7966 (0.295 sec/step)\n",
            "I0707 16:50:28.877785 140263261390720 learning.py:507] global step 735: loss = 5.3821 (0.394 sec/step)\n",
            "I0707 16:50:29.205601 140263261390720 learning.py:507] global step 736: loss = 5.4498 (0.326 sec/step)\n",
            "I0707 16:50:29.593540 140263261390720 learning.py:507] global step 737: loss = 4.4635 (0.386 sec/step)\n",
            "I0707 16:50:29.914497 140263261390720 learning.py:507] global step 738: loss = 6.0813 (0.319 sec/step)\n",
            "I0707 16:50:30.225158 140263261390720 learning.py:507] global step 739: loss = 5.3360 (0.309 sec/step)\n",
            "I0707 16:50:30.541319 140263261390720 learning.py:507] global step 740: loss = 5.6937 (0.315 sec/step)\n",
            "I0707 16:50:30.876215 140263261390720 learning.py:507] global step 741: loss = 6.1176 (0.333 sec/step)\n",
            "I0707 16:50:31.238556 140263261390720 learning.py:507] global step 742: loss = 5.9666 (0.361 sec/step)\n",
            "I0707 16:50:31.637186 140263261390720 learning.py:507] global step 743: loss = 5.2561 (0.397 sec/step)\n",
            "I0707 16:50:31.950757 140263261390720 learning.py:507] global step 744: loss = 7.7383 (0.312 sec/step)\n",
            "I0707 16:50:32.251393 140263261390720 learning.py:507] global step 745: loss = 4.5698 (0.299 sec/step)\n",
            "I0707 16:50:32.558568 140263261390720 learning.py:507] global step 746: loss = 4.8578 (0.305 sec/step)\n",
            "I0707 16:50:32.919420 140263261390720 learning.py:507] global step 747: loss = 5.5747 (0.359 sec/step)\n",
            "I0707 16:50:33.232069 140263261390720 learning.py:507] global step 748: loss = 4.2737 (0.311 sec/step)\n",
            "I0707 16:50:33.558802 140263261390720 learning.py:507] global step 749: loss = 5.6424 (0.325 sec/step)\n",
            "I0707 16:50:33.869419 140263261390720 learning.py:507] global step 750: loss = 5.0430 (0.309 sec/step)\n",
            "I0707 16:50:34.228548 140263261390720 learning.py:507] global step 751: loss = 4.1936 (0.357 sec/step)\n",
            "I0707 16:50:34.565533 140263261390720 learning.py:507] global step 752: loss = 5.8633 (0.335 sec/step)\n",
            "I0707 16:50:34.914218 140263261390720 learning.py:507] global step 753: loss = 5.5571 (0.347 sec/step)\n",
            "I0707 16:50:35.212845 140263261390720 learning.py:507] global step 754: loss = 4.5880 (0.297 sec/step)\n",
            "I0707 16:50:35.517310 140263261390720 learning.py:507] global step 755: loss = 4.9511 (0.303 sec/step)\n",
            "I0707 16:50:35.819015 140263261390720 learning.py:507] global step 756: loss = 4.0264 (0.300 sec/step)\n",
            "I0707 16:50:36.296725 140263261390720 learning.py:507] global step 757: loss = 5.6574 (0.476 sec/step)\n",
            "I0707 16:50:36.609709 140263261390720 learning.py:507] global step 758: loss = 5.7167 (0.311 sec/step)\n",
            "I0707 16:50:36.943822 140263261390720 learning.py:507] global step 759: loss = 5.7675 (0.332 sec/step)\n",
            "I0707 16:50:37.274617 140263261390720 learning.py:507] global step 760: loss = 5.6757 (0.329 sec/step)\n",
            "I0707 16:50:37.579801 140263261390720 learning.py:507] global step 761: loss = 4.5331 (0.303 sec/step)\n",
            "I0707 16:50:37.883033 140263261390720 learning.py:507] global step 762: loss = 4.1349 (0.302 sec/step)\n",
            "I0707 16:50:38.174627 140263261390720 learning.py:507] global step 763: loss = 6.2550 (0.290 sec/step)\n",
            "I0707 16:50:38.483977 140263261390720 learning.py:507] global step 764: loss = 5.6558 (0.308 sec/step)\n",
            "I0707 16:50:38.797951 140263261390720 learning.py:507] global step 765: loss = 4.9439 (0.312 sec/step)\n",
            "I0707 16:50:39.161107 140263261390720 learning.py:507] global step 766: loss = 5.3058 (0.362 sec/step)\n",
            "I0707 16:50:39.505817 140263261390720 learning.py:507] global step 767: loss = 4.7449 (0.343 sec/step)\n",
            "I0707 16:50:39.823532 140263261390720 learning.py:507] global step 768: loss = 5.9595 (0.316 sec/step)\n",
            "I0707 16:50:40.110675 140263261390720 learning.py:507] global step 769: loss = 5.7629 (0.286 sec/step)\n",
            "I0707 16:50:40.419966 140263261390720 learning.py:507] global step 770: loss = 5.7734 (0.308 sec/step)\n",
            "I0707 16:50:40.731721 140263261390720 learning.py:507] global step 771: loss = 5.8377 (0.310 sec/step)\n",
            "I0707 16:50:41.044557 140263261390720 learning.py:507] global step 772: loss = 4.2208 (0.311 sec/step)\n",
            "I0707 16:50:41.377536 140263261390720 learning.py:507] global step 773: loss = 4.7693 (0.331 sec/step)\n",
            "I0707 16:50:41.705861 140263261390720 learning.py:507] global step 774: loss = 5.6540 (0.327 sec/step)\n",
            "I0707 16:50:42.016852 140263261390720 learning.py:507] global step 775: loss = 5.5166 (0.309 sec/step)\n",
            "I0707 16:50:42.316198 140263261390720 learning.py:507] global step 776: loss = 5.0911 (0.298 sec/step)\n",
            "I0707 16:50:42.682650 140263261390720 learning.py:507] global step 777: loss = 5.9167 (0.365 sec/step)\n",
            "I0707 16:50:42.977436 140263261390720 learning.py:507] global step 778: loss = 5.7401 (0.293 sec/step)\n",
            "I0707 16:50:43.278346 140263261390720 learning.py:507] global step 779: loss = 4.6450 (0.299 sec/step)\n",
            "I0707 16:50:43.614541 140263261390720 learning.py:507] global step 780: loss = 4.9083 (0.335 sec/step)\n",
            "I0707 16:50:44.020679 140263261390720 learning.py:507] global step 781: loss = 4.6692 (0.404 sec/step)\n",
            "I0707 16:50:44.340678 140263261390720 learning.py:507] global step 782: loss = 4.3723 (0.318 sec/step)\n",
            "I0707 16:50:44.661092 140263261390720 learning.py:507] global step 783: loss = 4.8902 (0.319 sec/step)\n",
            "I0707 16:50:44.988924 140263261390720 learning.py:507] global step 784: loss = 4.5877 (0.326 sec/step)\n",
            "I0707 16:50:45.334300 140263261390720 learning.py:507] global step 785: loss = 6.6153 (0.344 sec/step)\n",
            "I0707 16:50:45.712244 140263261390720 learning.py:507] global step 786: loss = 5.7979 (0.376 sec/step)\n",
            "I0707 16:50:46.015799 140263261390720 learning.py:507] global step 787: loss = 5.6834 (0.302 sec/step)\n",
            "I0707 16:50:46.321953 140263261390720 learning.py:507] global step 788: loss = 5.2115 (0.305 sec/step)\n",
            "I0707 16:50:46.690922 140263261390720 learning.py:507] global step 789: loss = 6.0815 (0.367 sec/step)\n",
            "I0707 16:50:47.030456 140263261390720 learning.py:507] global step 790: loss = 6.5575 (0.338 sec/step)\n",
            "I0707 16:50:47.437001 140263261390720 learning.py:507] global step 791: loss = 4.5395 (0.405 sec/step)\n",
            "I0707 16:50:47.806415 140263261390720 learning.py:507] global step 792: loss = 5.9281 (0.368 sec/step)\n",
            "I0707 16:50:48.141956 140263261390720 learning.py:507] global step 793: loss = 4.7332 (0.334 sec/step)\n",
            "I0707 16:50:48.488150 140263261390720 learning.py:507] global step 794: loss = 5.6220 (0.344 sec/step)\n",
            "I0707 16:50:48.785706 140263261390720 learning.py:507] global step 795: loss = 5.8810 (0.296 sec/step)\n",
            "I0707 16:50:49.122478 140263261390720 learning.py:507] global step 796: loss = 5.7301 (0.335 sec/step)\n",
            "I0707 16:50:49.528563 140263261390720 learning.py:507] global step 797: loss = 4.4249 (0.404 sec/step)\n",
            "I0707 16:50:49.856478 140263261390720 learning.py:507] global step 798: loss = 4.8574 (0.326 sec/step)\n",
            "I0707 16:50:50.265223 140263261390720 learning.py:507] global step 799: loss = 4.8417 (0.407 sec/step)\n",
            "I0707 16:50:50.569087 140263261390720 learning.py:507] global step 800: loss = 4.4705 (0.302 sec/step)\n",
            "I0707 16:50:50.881062 140263261390720 learning.py:507] global step 801: loss = 4.8347 (0.310 sec/step)\n",
            "I0707 16:50:51.169625 140263261390720 learning.py:507] global step 802: loss = 6.0974 (0.287 sec/step)\n",
            "I0707 16:50:51.476037 140263261390720 learning.py:507] global step 803: loss = 4.6074 (0.305 sec/step)\n",
            "I0707 16:50:51.787038 140263261390720 learning.py:507] global step 804: loss = 4.7113 (0.309 sec/step)\n",
            "I0707 16:50:52.236586 140263261390720 learning.py:507] global step 805: loss = 6.3877 (0.448 sec/step)\n",
            "I0707 16:50:52.552484 140263261390720 learning.py:507] global step 806: loss = 5.4592 (0.314 sec/step)\n",
            "I0707 16:50:52.909081 140263261390720 learning.py:507] global step 807: loss = 4.1632 (0.355 sec/step)\n",
            "I0707 16:50:53.206080 140263261390720 learning.py:507] global step 808: loss = 6.1828 (0.295 sec/step)\n",
            "I0707 16:50:53.606801 140263261390720 learning.py:507] global step 809: loss = 4.4981 (0.399 sec/step)\n",
            "I0707 16:50:53.958714 140263261390720 learning.py:507] global step 810: loss = 5.7994 (0.350 sec/step)\n",
            "I0707 16:50:54.292601 140263261390720 learning.py:507] global step 811: loss = 6.1874 (0.332 sec/step)\n",
            "I0707 16:50:54.614815 140263261390720 learning.py:507] global step 812: loss = 4.7812 (0.321 sec/step)\n",
            "I0707 16:50:54.934869 140263261390720 learning.py:507] global step 813: loss = 5.5662 (0.318 sec/step)\n",
            "I0707 16:50:55.294947 140263261390720 learning.py:507] global step 814: loss = 5.2883 (0.358 sec/step)\n",
            "I0707 16:50:55.773308 140263261390720 learning.py:507] global step 815: loss = 5.6680 (0.477 sec/step)\n",
            "I0707 16:50:56.106563 140263261390720 learning.py:507] global step 816: loss = 5.1128 (0.332 sec/step)\n",
            "I0707 16:50:56.410676 140263261390720 learning.py:507] global step 817: loss = 4.8524 (0.302 sec/step)\n",
            "I0707 16:50:56.719563 140263261390720 learning.py:507] global step 818: loss = 5.3542 (0.307 sec/step)\n",
            "I0707 16:50:57.124319 140263261390720 learning.py:507] global step 819: loss = 5.6129 (0.403 sec/step)\n",
            "I0707 16:50:57.482668 140263261390720 learning.py:507] global step 820: loss = 4.2952 (0.357 sec/step)\n",
            "I0707 16:50:58.008459 140263261390720 learning.py:507] global step 821: loss = 4.7304 (0.524 sec/step)\n",
            "I0707 16:50:58.324075 140263261390720 learning.py:507] global step 822: loss = 6.0642 (0.314 sec/step)\n",
            "I0707 16:50:58.620854 140263261390720 learning.py:507] global step 823: loss = 5.0381 (0.295 sec/step)\n",
            "I0707 16:50:58.938631 140263261390720 learning.py:507] global step 824: loss = 4.7459 (0.316 sec/step)\n",
            "I0707 16:50:59.392929 140263261390720 learning.py:507] global step 825: loss = 5.0496 (0.452 sec/step)\n",
            "I0707 16:50:59.699156 140263261390720 learning.py:507] global step 826: loss = 5.9630 (0.304 sec/step)\n",
            "I0707 16:51:00.113877 140263261390720 learning.py:507] global step 827: loss = 5.2784 (0.413 sec/step)\n",
            "I0707 16:51:00.421166 140263261390720 learning.py:507] global step 828: loss = 4.9938 (0.305 sec/step)\n",
            "I0707 16:51:00.856338 140263261390720 learning.py:507] global step 829: loss = 3.7657 (0.433 sec/step)\n",
            "I0707 16:51:01.268222 140263261390720 learning.py:507] global step 830: loss = 6.4431 (0.410 sec/step)\n",
            "I0707 16:51:01.667946 140263261390720 learning.py:507] global step 831: loss = 4.1659 (0.398 sec/step)\n",
            "I0707 16:51:01.989459 140263261390720 learning.py:507] global step 832: loss = 4.4353 (0.320 sec/step)\n",
            "I0707 16:51:02.297691 140263261390720 learning.py:507] global step 833: loss = 4.0129 (0.307 sec/step)\n",
            "I0707 16:51:02.661277 140263261390720 learning.py:507] global step 834: loss = 4.9961 (0.362 sec/step)\n",
            "I0707 16:51:03.071067 140263261390720 learning.py:507] global step 835: loss = 6.8711 (0.408 sec/step)\n",
            "I0707 16:51:03.379688 140263261390720 learning.py:507] global step 836: loss = 6.3157 (0.307 sec/step)\n",
            "I0707 16:51:03.711844 140263261390720 learning.py:507] global step 837: loss = 4.0134 (0.330 sec/step)\n",
            "I0707 16:51:04.008631 140263261390720 learning.py:507] global step 838: loss = 4.7323 (0.295 sec/step)\n",
            "I0707 16:51:04.364161 140263261390720 learning.py:507] global step 839: loss = 4.9510 (0.354 sec/step)\n",
            "I0707 16:51:04.715181 140263261390720 learning.py:507] global step 840: loss = 5.2366 (0.349 sec/step)\n",
            "I0707 16:51:05.050796 140263261390720 learning.py:507] global step 841: loss = 4.7495 (0.334 sec/step)\n",
            "I0707 16:51:05.369845 140263261390720 learning.py:507] global step 842: loss = 4.4663 (0.317 sec/step)\n",
            "I0707 16:51:05.669450 140263261390720 learning.py:507] global step 843: loss = 5.1350 (0.298 sec/step)\n",
            "I0707 16:51:05.979194 140263261390720 learning.py:507] global step 844: loss = 5.5291 (0.308 sec/step)\n",
            "I0707 16:51:06.289950 140263261390720 learning.py:507] global step 845: loss = 4.4245 (0.309 sec/step)\n",
            "I0707 16:51:06.601607 140263261390720 learning.py:507] global step 846: loss = 4.5016 (0.310 sec/step)\n",
            "I0707 16:51:06.918774 140263261390720 learning.py:507] global step 847: loss = 5.0014 (0.316 sec/step)\n",
            "I0707 16:51:07.218727 140263261390720 learning.py:507] global step 848: loss = 5.9905 (0.298 sec/step)\n",
            "I0707 16:51:07.624012 140263261390720 learning.py:507] global step 849: loss = 4.7334 (0.403 sec/step)\n",
            "I0707 16:51:08.185531 140263261390720 learning.py:507] global step 850: loss = 6.2853 (0.560 sec/step)\n",
            "I0707 16:51:08.780030 140263261390720 learning.py:507] global step 851: loss = 4.3417 (0.592 sec/step)\n",
            "I0707 16:51:09.102876 140263261390720 learning.py:507] global step 852: loss = 5.0308 (0.321 sec/step)\n",
            "I0707 16:51:09.433379 140263261390720 learning.py:507] global step 853: loss = 5.2702 (0.329 sec/step)\n",
            "I0707 16:51:09.918782 140263261390720 learning.py:507] global step 854: loss = 6.3979 (0.484 sec/step)\n",
            "I0707 16:51:10.373111 140263261390720 learning.py:507] global step 855: loss = 7.7088 (0.453 sec/step)\n",
            "I0707 16:51:11.083446 140263261390720 learning.py:507] global step 856: loss = 4.2219 (0.709 sec/step)\n",
            "I0707 16:51:11.550310 140263261390720 learning.py:507] global step 857: loss = 5.0341 (0.465 sec/step)\n",
            "I0707 16:51:11.880017 140263261390720 learning.py:507] global step 858: loss = 4.5225 (0.328 sec/step)\n",
            "I0707 16:51:12.311499 140263261390720 learning.py:507] global step 859: loss = 4.8382 (0.430 sec/step)\n",
            "I0707 16:51:12.780722 140263261390720 learning.py:507] global step 860: loss = 5.8790 (0.467 sec/step)\n",
            "I0707 16:51:13.076234 140263261390720 learning.py:507] global step 861: loss = 5.0616 (0.294 sec/step)\n",
            "I0707 16:51:13.385770 140263261390720 learning.py:507] global step 862: loss = 4.9767 (0.308 sec/step)\n",
            "I0707 16:51:13.752258 140263261390720 learning.py:507] global step 863: loss = 5.7459 (0.365 sec/step)\n",
            "I0707 16:51:14.091946 140263261390720 learning.py:507] global step 864: loss = 5.3675 (0.338 sec/step)\n",
            "I0707 16:51:14.563248 140263261390720 learning.py:507] global step 865: loss = 4.3506 (0.470 sec/step)\n",
            "I0707 16:51:14.875238 140263261390720 learning.py:507] global step 866: loss = 4.9691 (0.310 sec/step)\n",
            "I0707 16:51:15.302202 140263261390720 learning.py:507] global step 867: loss = 5.9480 (0.425 sec/step)\n",
            "I0707 16:51:15.638948 140263261390720 learning.py:507] global step 868: loss = 3.6359 (0.335 sec/step)\n",
            "I0707 16:51:15.964169 140263261390720 learning.py:507] global step 869: loss = 5.4732 (0.324 sec/step)\n",
            "I0707 16:51:16.333315 140263261390720 learning.py:507] global step 870: loss = 4.6220 (0.368 sec/step)\n",
            "I0707 16:51:16.675247 140263261390720 learning.py:507] global step 871: loss = 4.5850 (0.340 sec/step)\n",
            "I0707 16:51:17.008152 140263261390720 learning.py:507] global step 872: loss = 5.9899 (0.331 sec/step)\n",
            "I0707 16:51:17.380094 140263261390720 learning.py:507] global step 873: loss = 4.4158 (0.370 sec/step)\n",
            "I0707 16:51:17.720778 140263261390720 learning.py:507] global step 874: loss = 4.1822 (0.339 sec/step)\n",
            "I0707 16:51:18.046231 140263261390720 learning.py:507] global step 875: loss = 4.3265 (0.324 sec/step)\n",
            "I0707 16:51:18.386282 140263261390720 learning.py:507] global step 876: loss = 5.5260 (0.338 sec/step)\n",
            "I0707 16:51:18.704432 140263261390720 learning.py:507] global step 877: loss = 4.7422 (0.317 sec/step)\n",
            "I0707 16:51:19.058189 140263261390720 learning.py:507] global step 878: loss = 4.9723 (0.352 sec/step)\n",
            "I0707 16:51:19.428255 140263261390720 learning.py:507] global step 879: loss = 4.4423 (0.369 sec/step)\n",
            "I0707 16:51:19.797796 140263261390720 learning.py:507] global step 880: loss = 6.0618 (0.368 sec/step)\n",
            "I0707 16:51:20.136642 140263261390720 learning.py:507] global step 881: loss = 5.3773 (0.337 sec/step)\n",
            "I0707 16:51:20.443316 140263261390720 learning.py:507] global step 882: loss = 5.2898 (0.305 sec/step)\n",
            "I0707 16:51:20.759510 140263261390720 learning.py:507] global step 883: loss = 6.4692 (0.315 sec/step)\n",
            "I0707 16:51:21.057123 140263261390720 learning.py:507] global step 884: loss = 6.3791 (0.296 sec/step)\n",
            "I0707 16:51:21.390635 140263261390720 learning.py:507] global step 885: loss = 4.8973 (0.332 sec/step)\n",
            "I0707 16:51:21.709375 140263261390720 learning.py:507] global step 886: loss = 4.7850 (0.317 sec/step)\n",
            "I0707 16:51:22.064345 140263261390720 learning.py:507] global step 887: loss = 5.2023 (0.353 sec/step)\n",
            "I0707 16:51:22.388619 140263261390720 learning.py:507] global step 888: loss = 4.6590 (0.323 sec/step)\n",
            "I0707 16:51:22.814841 140263261390720 learning.py:507] global step 889: loss = 5.5855 (0.424 sec/step)\n",
            "I0707 16:51:23.156360 140263261390720 learning.py:507] global step 890: loss = 5.0649 (0.340 sec/step)\n",
            "I0707 16:51:23.481154 140263261390720 learning.py:507] global step 891: loss = 4.8143 (0.323 sec/step)\n",
            "I0707 16:51:23.820994 140263261390720 learning.py:507] global step 892: loss = 5.2057 (0.338 sec/step)\n",
            "I0707 16:51:24.238938 140263261390720 learning.py:507] global step 893: loss = 5.3097 (0.416 sec/step)\n",
            "I0707 16:51:24.537668 140263261390720 learning.py:507] global step 894: loss = 5.0299 (0.297 sec/step)\n",
            "I0707 16:51:24.864268 140263261390720 learning.py:507] global step 895: loss = 5.2399 (0.325 sec/step)\n",
            "I0707 16:51:25.252953 140263261390720 learning.py:507] global step 896: loss = 6.2490 (0.387 sec/step)\n",
            "I0707 16:51:25.650643 140263261390720 learning.py:507] global step 897: loss = 5.8977 (0.396 sec/step)\n",
            "I0707 16:51:26.009021 140263261390720 learning.py:507] global step 898: loss = 5.6835 (0.357 sec/step)\n",
            "I0707 16:51:26.426998 140263261390720 learning.py:507] global step 899: loss = 5.1049 (0.416 sec/step)\n",
            "I0707 16:51:26.748358 140263261390720 learning.py:507] global step 900: loss = 5.2753 (0.320 sec/step)\n",
            "I0707 16:51:27.068326 140263261390720 learning.py:507] global step 901: loss = 4.2057 (0.318 sec/step)\n",
            "I0707 16:51:27.469306 140263261390720 learning.py:507] global step 902: loss = 4.3115 (0.399 sec/step)\n",
            "I0707 16:51:27.836439 140263261390720 learning.py:507] global step 903: loss = 5.1910 (0.364 sec/step)\n",
            "I0707 16:51:28.170932 140263261390720 learning.py:507] global step 904: loss = 6.7014 (0.333 sec/step)\n",
            "I0707 16:51:28.500391 140263261390720 learning.py:507] global step 905: loss = 5.1171 (0.328 sec/step)\n",
            "I0707 16:51:28.841271 140263261390720 learning.py:507] global step 906: loss = 4.6768 (0.339 sec/step)\n",
            "I0707 16:51:29.168565 140263261390720 learning.py:507] global step 907: loss = 4.8503 (0.326 sec/step)\n",
            "I0707 16:51:29.474000 140263261390720 learning.py:507] global step 908: loss = 5.7243 (0.304 sec/step)\n",
            "I0707 16:51:29.776629 140263261390720 learning.py:507] global step 909: loss = 4.6121 (0.301 sec/step)\n",
            "I0707 16:51:30.095227 140263261390720 learning.py:507] global step 910: loss = 5.1674 (0.317 sec/step)\n",
            "I0707 16:51:30.403701 140263261390720 learning.py:507] global step 911: loss = 4.7277 (0.307 sec/step)\n",
            "I0707 16:51:30.732713 140263261390720 learning.py:507] global step 912: loss = 5.2222 (0.327 sec/step)\n",
            "I0707 16:51:31.045522 140263261390720 learning.py:507] global step 913: loss = 6.1996 (0.311 sec/step)\n",
            "I0707 16:51:31.368398 140263261390720 learning.py:507] global step 914: loss = 5.6053 (0.321 sec/step)\n",
            "I0707 16:51:31.660383 140263261390720 learning.py:507] global step 915: loss = 4.6369 (0.290 sec/step)\n",
            "I0707 16:51:31.966112 140263261390720 learning.py:507] global step 916: loss = 4.7926 (0.304 sec/step)\n",
            "I0707 16:51:32.521519 140263261390720 learning.py:507] global step 917: loss = 5.7828 (0.553 sec/step)\n",
            "I0707 16:51:32.912541 140263261390720 learning.py:507] global step 918: loss = 5.7366 (0.389 sec/step)\n",
            "I0707 16:51:33.214692 140263261390720 learning.py:507] global step 919: loss = 4.1809 (0.300 sec/step)\n",
            "I0707 16:51:33.533819 140263261390720 learning.py:507] global step 920: loss = 6.0651 (0.317 sec/step)\n",
            "I0707 16:51:33.836157 140263261390720 learning.py:507] global step 921: loss = 5.0619 (0.301 sec/step)\n",
            "I0707 16:51:34.199987 140263261390720 learning.py:507] global step 922: loss = 4.4649 (0.362 sec/step)\n",
            "I0707 16:51:34.923953 140263261390720 learning.py:507] global step 923: loss = 5.2100 (0.722 sec/step)\n",
            "I0707 16:51:35.348120 140263261390720 learning.py:507] global step 924: loss = 4.3697 (0.423 sec/step)\n",
            "I0707 16:51:35.652472 140263261390720 learning.py:507] global step 925: loss = 7.1519 (0.303 sec/step)\n",
            "I0707 16:51:36.022236 140263261390720 learning.py:507] global step 926: loss = 6.1269 (0.368 sec/step)\n",
            "I0707 16:51:36.398626 140263261390720 learning.py:507] global step 927: loss = 5.9945 (0.374 sec/step)\n",
            "I0707 16:51:36.736526 140263261390720 learning.py:507] global step 928: loss = 5.1595 (0.336 sec/step)\n",
            "I0707 16:51:37.136123 140263261390720 learning.py:507] global step 929: loss = 5.1288 (0.396 sec/step)\n",
            "I0707 16:51:37.456259 140263261390720 learning.py:507] global step 930: loss = 5.4842 (0.318 sec/step)\n",
            "I0707 16:51:37.800338 140263261390720 learning.py:507] global step 931: loss = 5.1792 (0.342 sec/step)\n",
            "I0707 16:51:38.171099 140263261390720 learning.py:507] global step 932: loss = 4.8591 (0.369 sec/step)\n",
            "I0707 16:51:38.587213 140263261390720 learning.py:507] global step 933: loss = 5.4935 (0.414 sec/step)\n",
            "I0707 16:51:38.893190 140263261390720 learning.py:507] global step 934: loss = 6.0106 (0.304 sec/step)\n",
            "I0707 16:51:39.206720 140263261390720 learning.py:507] global step 935: loss = 4.3146 (0.312 sec/step)\n",
            "I0707 16:51:39.539277 140263261390720 learning.py:507] global step 936: loss = 4.4717 (0.331 sec/step)\n",
            "I0707 16:51:39.971167 140263261390720 learning.py:507] global step 937: loss = 5.0405 (0.430 sec/step)\n",
            "I0707 16:51:40.325061 140263261390720 learning.py:507] global step 938: loss = 4.3512 (0.352 sec/step)\n",
            "I0707 16:51:40.715490 140263261390720 learning.py:507] global step 939: loss = 5.7329 (0.389 sec/step)\n",
            "I0707 16:51:41.039091 140263261390720 learning.py:507] global step 940: loss = 5.8708 (0.321 sec/step)\n",
            "I0707 16:51:41.355574 140263261390720 learning.py:507] global step 941: loss = 6.0682 (0.315 sec/step)\n",
            "I0707 16:51:41.663573 140263261390720 learning.py:507] global step 942: loss = 4.1110 (0.306 sec/step)\n",
            "I0707 16:51:41.991302 140263261390720 learning.py:507] global step 943: loss = 5.6103 (0.326 sec/step)\n",
            "I0707 16:51:42.323616 140263261390720 learning.py:507] global step 944: loss = 5.1682 (0.331 sec/step)\n",
            "I0707 16:51:42.706428 140263261390720 learning.py:507] global step 945: loss = 4.6883 (0.381 sec/step)\n",
            "I0707 16:51:43.035391 140263261390720 learning.py:507] global step 946: loss = 4.9788 (0.327 sec/step)\n",
            "I0707 16:51:43.336354 140263261390720 learning.py:507] global step 947: loss = 4.1646 (0.299 sec/step)\n",
            "I0707 16:51:43.699775 140263261390720 learning.py:507] global step 948: loss = 3.7659 (0.362 sec/step)\n",
            "I0707 16:51:44.056450 140263261390720 learning.py:507] global step 949: loss = 5.0828 (0.355 sec/step)\n",
            "I0707 16:51:44.389581 140263261390720 learning.py:507] global step 950: loss = 5.5563 (0.331 sec/step)\n",
            "I0707 16:51:44.693438 140263261390720 learning.py:507] global step 951: loss = 4.6118 (0.302 sec/step)\n",
            "I0707 16:51:45.052448 140263261390720 learning.py:507] global step 952: loss = 4.1054 (0.357 sec/step)\n",
            "I0707 16:51:45.359240 140263261390720 learning.py:507] global step 953: loss = 6.0682 (0.305 sec/step)\n",
            "I0707 16:51:45.697234 140263261390720 learning.py:507] global step 954: loss = 4.6938 (0.335 sec/step)\n",
            "I0707 16:51:46.125459 140263261390720 learning.py:507] global step 955: loss = 5.6962 (0.426 sec/step)\n",
            "I0707 16:51:46.446695 140263261390720 learning.py:507] global step 956: loss = 4.5229 (0.319 sec/step)\n",
            "I0707 16:51:46.782849 140263261390720 learning.py:507] global step 957: loss = 5.7785 (0.334 sec/step)\n",
            "I0707 16:51:47.100323 140263261390720 learning.py:507] global step 958: loss = 4.6547 (0.316 sec/step)\n",
            "I0707 16:51:47.404789 140263261390720 learning.py:507] global step 959: loss = 4.9117 (0.303 sec/step)\n",
            "I0707 16:51:47.738369 140263261390720 learning.py:507] global step 960: loss = 5.2164 (0.332 sec/step)\n",
            "I0707 16:51:48.036132 140263261390720 learning.py:507] global step 961: loss = 4.8236 (0.296 sec/step)\n",
            "I0707 16:51:48.339105 140263261390720 learning.py:507] global step 962: loss = 4.0446 (0.301 sec/step)\n",
            "I0707 16:51:48.661956 140263261390720 learning.py:507] global step 963: loss = 4.0774 (0.321 sec/step)\n",
            "I0707 16:51:48.988750 140263261390720 learning.py:507] global step 964: loss = 4.8820 (0.325 sec/step)\n",
            "I0707 16:51:49.433569 140263261390720 learning.py:507] global step 965: loss = 4.7604 (0.443 sec/step)\n",
            "I0707 16:51:49.756076 140263261390720 learning.py:507] global step 966: loss = 5.4210 (0.321 sec/step)\n",
            "I0707 16:51:50.072275 140263261390720 learning.py:507] global step 967: loss = 3.9462 (0.315 sec/step)\n",
            "I0707 16:51:50.386916 140263261390720 learning.py:507] global step 968: loss = 4.4158 (0.313 sec/step)\n",
            "I0707 16:51:50.705964 140263261390720 learning.py:507] global step 969: loss = 7.1820 (0.317 sec/step)\n",
            "I0707 16:51:51.016895 140263261390720 learning.py:507] global step 970: loss = 5.5267 (0.309 sec/step)\n",
            "I0707 16:51:51.539253 140263261390720 learning.py:507] global step 971: loss = 5.8416 (0.521 sec/step)\n",
            "I0707 16:51:51.849141 140263261390720 learning.py:507] global step 972: loss = 5.1183 (0.308 sec/step)\n",
            "I0707 16:51:52.166223 140263261390720 learning.py:507] global step 973: loss = 4.3030 (0.315 sec/step)\n",
            "I0707 16:51:52.481836 140263261390720 learning.py:507] global step 974: loss = 4.6508 (0.314 sec/step)\n",
            "I0707 16:51:52.795981 140263261390720 learning.py:507] global step 975: loss = 5.3320 (0.312 sec/step)\n",
            "I0707 16:51:53.382293 140263261390720 learning.py:507] global step 976: loss = 4.5788 (0.409 sec/step)\n",
            "I0707 16:51:53.989422 140263261390720 learning.py:507] global step 977: loss = 4.6592 (0.603 sec/step)\n",
            "I0707 16:51:53.990186 140260287772416 supervisor.py:1050] Recording summary at step 977.\n",
            "I0707 16:51:54.306132 140263261390720 learning.py:507] global step 978: loss = 4.7100 (0.308 sec/step)\n",
            "I0707 16:51:54.745658 140263261390720 learning.py:507] global step 979: loss = 5.9518 (0.438 sec/step)\n",
            "I0707 16:51:54.826504 140260296165120 supervisor.py:1099] global_step/sec: 2.82527\n",
            "I0707 16:51:55.059659 140263261390720 learning.py:507] global step 980: loss = 7.4700 (0.312 sec/step)\n",
            "I0707 16:51:55.401632 140263261390720 learning.py:507] global step 981: loss = 4.6279 (0.340 sec/step)\n",
            "I0707 16:51:55.731400 140263261390720 learning.py:507] global step 982: loss = 6.5901 (0.328 sec/step)\n",
            "I0707 16:51:56.041885 140263261390720 learning.py:507] global step 983: loss = 5.0629 (0.309 sec/step)\n",
            "I0707 16:51:56.422360 140263261390720 learning.py:507] global step 984: loss = 4.8825 (0.379 sec/step)\n",
            "I0707 16:51:56.859721 140263261390720 learning.py:507] global step 985: loss = 4.6363 (0.436 sec/step)\n",
            "I0707 16:51:57.195577 140263261390720 learning.py:507] global step 986: loss = 4.1309 (0.334 sec/step)\n",
            "I0707 16:51:57.615711 140263261390720 learning.py:507] global step 987: loss = 6.1262 (0.419 sec/step)\n",
            "I0707 16:51:57.940798 140263261390720 learning.py:507] global step 988: loss = 6.2880 (0.323 sec/step)\n",
            "I0707 16:51:58.263428 140263261390720 learning.py:507] global step 989: loss = 5.0397 (0.321 sec/step)\n",
            "I0707 16:51:58.639840 140263261390720 learning.py:507] global step 990: loss = 4.6188 (0.375 sec/step)\n",
            "I0707 16:51:58.951290 140263261390720 learning.py:507] global step 991: loss = 5.1982 (0.310 sec/step)\n",
            "I0707 16:51:59.248761 140263261390720 learning.py:507] global step 992: loss = 4.5289 (0.296 sec/step)\n",
            "I0707 16:51:59.589120 140263261390720 learning.py:507] global step 993: loss = 5.9631 (0.339 sec/step)\n",
            "I0707 16:51:59.995508 140263261390720 learning.py:507] global step 994: loss = 3.8915 (0.405 sec/step)\n",
            "I0707 16:52:00.328396 140263261390720 learning.py:507] global step 995: loss = 4.5932 (0.331 sec/step)\n",
            "I0707 16:52:00.644855 140263261390720 learning.py:507] global step 996: loss = 4.1715 (0.315 sec/step)\n",
            "I0707 16:52:01.041486 140263261390720 learning.py:507] global step 997: loss = 6.9796 (0.395 sec/step)\n",
            "I0707 16:52:01.341225 140263261390720 learning.py:507] global step 998: loss = 4.7446 (0.298 sec/step)\n",
            "I0707 16:52:01.643686 140263261390720 learning.py:507] global step 999: loss = 4.6948 (0.301 sec/step)\n",
            "I0707 16:52:01.950153 140263261390720 learning.py:507] global step 1000: loss = 5.0103 (0.305 sec/step)\n",
            "I0707 16:52:02.341300 140263261390720 learning.py:507] global step 1001: loss = 4.6759 (0.389 sec/step)\n",
            "I0707 16:52:02.661462 140263261390720 learning.py:507] global step 1002: loss = 4.7452 (0.318 sec/step)\n",
            "I0707 16:52:03.135441 140263261390720 learning.py:507] global step 1003: loss = 6.0926 (0.472 sec/step)\n",
            "I0707 16:52:03.454795 140263261390720 learning.py:507] global step 1004: loss = 4.3473 (0.318 sec/step)\n",
            "I0707 16:52:03.779807 140263261390720 learning.py:507] global step 1005: loss = 4.1878 (0.323 sec/step)\n",
            "I0707 16:52:04.096360 140263261390720 learning.py:507] global step 1006: loss = 5.1375 (0.315 sec/step)\n",
            "I0707 16:52:04.560139 140263261390720 learning.py:507] global step 1007: loss = 5.7660 (0.462 sec/step)\n",
            "I0707 16:52:04.871317 140263261390720 learning.py:507] global step 1008: loss = 6.5988 (0.310 sec/step)\n",
            "I0707 16:52:05.192953 140263261390720 learning.py:507] global step 1009: loss = 5.1840 (0.320 sec/step)\n",
            "I0707 16:52:05.538217 140263261390720 learning.py:507] global step 1010: loss = 4.3930 (0.344 sec/step)\n",
            "I0707 16:52:05.906213 140263261390720 learning.py:507] global step 1011: loss = 4.3565 (0.366 sec/step)\n",
            "I0707 16:52:06.354776 140263261390720 learning.py:507] global step 1012: loss = 4.7651 (0.447 sec/step)\n",
            "I0707 16:52:06.934939 140263261390720 learning.py:507] global step 1013: loss = 5.6299 (0.579 sec/step)\n",
            "I0707 16:52:07.239768 140263261390720 learning.py:507] global step 1014: loss = 4.0541 (0.303 sec/step)\n",
            "I0707 16:52:07.553587 140263261390720 learning.py:507] global step 1015: loss = 4.8900 (0.312 sec/step)\n",
            "I0707 16:52:07.887275 140263261390720 learning.py:507] global step 1016: loss = 4.1385 (0.332 sec/step)\n",
            "I0707 16:52:08.281395 140263261390720 learning.py:507] global step 1017: loss = 5.2566 (0.392 sec/step)\n",
            "I0707 16:52:08.594807 140263261390720 learning.py:507] global step 1018: loss = 4.1205 (0.312 sec/step)\n",
            "I0707 16:52:09.088870 140263261390720 learning.py:507] global step 1019: loss = 4.8689 (0.492 sec/step)\n",
            "I0707 16:52:09.409684 140263261390720 learning.py:507] global step 1020: loss = 5.1996 (0.319 sec/step)\n",
            "I0707 16:52:09.722084 140263261390720 learning.py:507] global step 1021: loss = 4.3027 (0.311 sec/step)\n",
            "I0707 16:52:10.030212 140263261390720 learning.py:507] global step 1022: loss = 4.2785 (0.306 sec/step)\n",
            "I0707 16:52:10.414209 140263261390720 learning.py:507] global step 1023: loss = 4.7558 (0.382 sec/step)\n",
            "I0707 16:52:10.724582 140263261390720 learning.py:507] global step 1024: loss = 4.5657 (0.309 sec/step)\n",
            "I0707 16:52:11.097911 140263261390720 learning.py:507] global step 1025: loss = 4.7586 (0.372 sec/step)\n",
            "I0707 16:52:11.414951 140263261390720 learning.py:507] global step 1026: loss = 5.4762 (0.315 sec/step)\n",
            "I0707 16:52:11.725440 140263261390720 learning.py:507] global step 1027: loss = 5.6866 (0.309 sec/step)\n",
            "I0707 16:52:12.168638 140263261390720 learning.py:507] global step 1028: loss = 6.4385 (0.442 sec/step)\n",
            "I0707 16:52:12.606310 140263261390720 learning.py:507] global step 1029: loss = 4.9428 (0.436 sec/step)\n",
            "I0707 16:52:12.945286 140263261390720 learning.py:507] global step 1030: loss = 5.7872 (0.337 sec/step)\n",
            "I0707 16:52:13.234479 140263261390720 learning.py:507] global step 1031: loss = 5.2806 (0.287 sec/step)\n",
            "I0707 16:52:13.688083 140263261390720 learning.py:507] global step 1032: loss = 5.8030 (0.452 sec/step)\n",
            "I0707 16:52:14.018398 140263261390720 learning.py:507] global step 1033: loss = 4.9075 (0.329 sec/step)\n",
            "I0707 16:52:14.309685 140263261390720 learning.py:507] global step 1034: loss = 5.3435 (0.290 sec/step)\n",
            "I0707 16:52:14.724416 140263261390720 learning.py:507] global step 1035: loss = 4.3146 (0.413 sec/step)\n",
            "I0707 16:52:15.023344 140263261390720 learning.py:507] global step 1036: loss = 5.4561 (0.297 sec/step)\n",
            "I0707 16:52:15.334745 140263261390720 learning.py:507] global step 1037: loss = 6.0163 (0.310 sec/step)\n",
            "I0707 16:52:15.889548 140263261390720 learning.py:507] global step 1038: loss = 5.2767 (0.553 sec/step)\n",
            "I0707 16:52:16.197254 140263261390720 learning.py:507] global step 1039: loss = 5.1887 (0.306 sec/step)\n",
            "I0707 16:52:16.503685 140263261390720 learning.py:507] global step 1040: loss = 5.5966 (0.305 sec/step)\n",
            "I0707 16:52:16.883850 140263261390720 learning.py:507] global step 1041: loss = 5.5194 (0.378 sec/step)\n",
            "I0707 16:52:17.213048 140263261390720 learning.py:507] global step 1042: loss = 5.6951 (0.327 sec/step)\n",
            "I0707 16:52:17.528525 140263261390720 learning.py:507] global step 1043: loss = 4.5805 (0.314 sec/step)\n",
            "I0707 16:52:17.820924 140263261390720 learning.py:507] global step 1044: loss = 4.3705 (0.291 sec/step)\n",
            "I0707 16:52:18.124614 140263261390720 learning.py:507] global step 1045: loss = 5.0052 (0.302 sec/step)\n",
            "I0707 16:52:18.441087 140263261390720 learning.py:507] global step 1046: loss = 5.3574 (0.315 sec/step)\n",
            "I0707 16:52:18.925200 140263261390720 learning.py:507] global step 1047: loss = 4.3139 (0.482 sec/step)\n",
            "I0707 16:52:19.277173 140263261390720 learning.py:507] global step 1048: loss = 4.2775 (0.350 sec/step)\n",
            "I0707 16:52:19.598467 140263261390720 learning.py:507] global step 1049: loss = 4.9700 (0.320 sec/step)\n",
            "I0707 16:52:19.967097 140263261390720 learning.py:507] global step 1050: loss = 6.4857 (0.367 sec/step)\n",
            "I0707 16:52:20.308435 140263261390720 learning.py:507] global step 1051: loss = 5.4068 (0.340 sec/step)\n",
            "I0707 16:52:20.629436 140263261390720 learning.py:507] global step 1052: loss = 4.6576 (0.319 sec/step)\n",
            "I0707 16:52:21.225208 140263261390720 learning.py:507] global step 1053: loss = 3.9316 (0.594 sec/step)\n",
            "I0707 16:52:21.531581 140263261390720 learning.py:507] global step 1054: loss = 5.1896 (0.305 sec/step)\n",
            "I0707 16:52:21.842895 140263261390720 learning.py:507] global step 1055: loss = 6.2489 (0.310 sec/step)\n",
            "I0707 16:52:22.241556 140263261390720 learning.py:507] global step 1056: loss = 4.7227 (0.397 sec/step)\n",
            "I0707 16:52:22.579671 140263261390720 learning.py:507] global step 1057: loss = 5.5741 (0.336 sec/step)\n",
            "I0707 16:52:22.919313 140263261390720 learning.py:507] global step 1058: loss = 4.4256 (0.338 sec/step)\n",
            "I0707 16:52:23.236497 140263261390720 learning.py:507] global step 1059: loss = 4.3781 (0.316 sec/step)\n",
            "I0707 16:52:23.528095 140263261390720 learning.py:507] global step 1060: loss = 4.5567 (0.290 sec/step)\n",
            "I0707 16:52:23.904593 140263261390720 learning.py:507] global step 1061: loss = 5.2916 (0.375 sec/step)\n",
            "I0707 16:52:24.227669 140263261390720 learning.py:507] global step 1062: loss = 5.7398 (0.322 sec/step)\n",
            "I0707 16:52:24.528373 140263261390720 learning.py:507] global step 1063: loss = 6.5189 (0.299 sec/step)\n",
            "I0707 16:52:24.885168 140263261390720 learning.py:507] global step 1064: loss = 6.1489 (0.355 sec/step)\n",
            "I0707 16:52:25.178460 140263261390720 learning.py:507] global step 1065: loss = 5.5031 (0.292 sec/step)\n",
            "I0707 16:52:25.504127 140263261390720 learning.py:507] global step 1066: loss = 3.6738 (0.324 sec/step)\n",
            "I0707 16:52:25.921878 140263261390720 learning.py:507] global step 1067: loss = 4.4527 (0.416 sec/step)\n",
            "I0707 16:52:26.227098 140263261390720 learning.py:507] global step 1068: loss = 5.1315 (0.304 sec/step)\n",
            "I0707 16:52:26.532955 140263261390720 learning.py:507] global step 1069: loss = 5.3870 (0.304 sec/step)\n",
            "I0707 16:52:26.902371 140263261390720 learning.py:507] global step 1070: loss = 4.5747 (0.368 sec/step)\n",
            "I0707 16:52:27.205059 140263261390720 learning.py:507] global step 1071: loss = 5.5136 (0.301 sec/step)\n",
            "I0707 16:52:27.550999 140263261390720 learning.py:507] global step 1072: loss = 4.5360 (0.344 sec/step)\n",
            "I0707 16:52:27.892881 140263261390720 learning.py:507] global step 1073: loss = 4.3807 (0.340 sec/step)\n",
            "I0707 16:52:28.221957 140263261390720 learning.py:507] global step 1074: loss = 6.1202 (0.327 sec/step)\n",
            "I0707 16:52:28.536034 140263261390720 learning.py:507] global step 1075: loss = 4.3787 (0.312 sec/step)\n",
            "I0707 16:52:29.011519 140263261390720 learning.py:507] global step 1076: loss = 6.4440 (0.474 sec/step)\n",
            "I0707 16:52:29.433737 140263261390720 learning.py:507] global step 1077: loss = 5.6696 (0.421 sec/step)\n",
            "I0707 16:52:29.726151 140263261390720 learning.py:507] global step 1078: loss = 5.8676 (0.291 sec/step)\n",
            "I0707 16:52:30.030292 140263261390720 learning.py:507] global step 1079: loss = 5.3105 (0.303 sec/step)\n",
            "I0707 16:52:30.383477 140263261390720 learning.py:507] global step 1080: loss = 4.6523 (0.351 sec/step)\n",
            "I0707 16:52:30.706252 140263261390720 learning.py:507] global step 1081: loss = 4.3541 (0.321 sec/step)\n",
            "I0707 16:52:31.113625 140263261390720 learning.py:507] global step 1082: loss = 4.1736 (0.405 sec/step)\n",
            "I0707 16:52:31.417273 140263261390720 learning.py:507] global step 1083: loss = 5.5979 (0.302 sec/step)\n",
            "I0707 16:52:31.717840 140263261390720 learning.py:507] global step 1084: loss = 5.0427 (0.299 sec/step)\n",
            "I0707 16:52:32.101629 140263261390720 learning.py:507] global step 1085: loss = 4.6813 (0.382 sec/step)\n",
            "I0707 16:52:32.400199 140263261390720 learning.py:507] global step 1086: loss = 4.2085 (0.297 sec/step)\n",
            "I0707 16:52:32.709848 140263261390720 learning.py:507] global step 1087: loss = 5.1528 (0.308 sec/step)\n",
            "I0707 16:52:33.018515 140263261390720 learning.py:507] global step 1088: loss = 4.1716 (0.306 sec/step)\n",
            "I0707 16:52:33.375756 140263261390720 learning.py:507] global step 1089: loss = 4.5106 (0.355 sec/step)\n",
            "I0707 16:52:33.709264 140263261390720 learning.py:507] global step 1090: loss = 5.3344 (0.332 sec/step)\n",
            "I0707 16:52:34.030174 140263261390720 learning.py:507] global step 1091: loss = 7.0017 (0.319 sec/step)\n",
            "I0707 16:52:34.344902 140263261390720 learning.py:507] global step 1092: loss = 4.9062 (0.313 sec/step)\n",
            "I0707 16:52:34.644871 140263261390720 learning.py:507] global step 1093: loss = 4.9451 (0.298 sec/step)\n",
            "I0707 16:52:34.936835 140263261390720 learning.py:507] global step 1094: loss = 5.8105 (0.289 sec/step)\n",
            "I0707 16:52:35.246356 140263261390720 learning.py:507] global step 1095: loss = 4.1373 (0.308 sec/step)\n",
            "I0707 16:52:35.555872 140263261390720 learning.py:507] global step 1096: loss = 5.9432 (0.307 sec/step)\n",
            "I0707 16:52:35.908128 140263261390720 learning.py:507] global step 1097: loss = 3.9346 (0.350 sec/step)\n",
            "I0707 16:52:36.258989 140263261390720 learning.py:507] global step 1098: loss = 5.2460 (0.349 sec/step)\n",
            "I0707 16:52:36.562393 140263261390720 learning.py:507] global step 1099: loss = 4.9501 (0.302 sec/step)\n",
            "I0707 16:52:36.873606 140263261390720 learning.py:507] global step 1100: loss = 5.1090 (0.310 sec/step)\n",
            "I0707 16:52:37.217036 140263261390720 learning.py:507] global step 1101: loss = 3.9699 (0.342 sec/step)\n",
            "I0707 16:52:37.535112 140263261390720 learning.py:507] global step 1102: loss = 5.3975 (0.317 sec/step)\n",
            "I0707 16:52:37.908301 140263261390720 learning.py:507] global step 1103: loss = 5.7724 (0.371 sec/step)\n",
            "I0707 16:52:38.239003 140263261390720 learning.py:507] global step 1104: loss = 3.7654 (0.329 sec/step)\n",
            "I0707 16:52:38.823287 140263261390720 learning.py:507] global step 1105: loss = 5.7458 (0.583 sec/step)\n",
            "I0707 16:52:39.152422 140263261390720 learning.py:507] global step 1106: loss = 5.2003 (0.327 sec/step)\n",
            "I0707 16:52:39.469872 140263261390720 learning.py:507] global step 1107: loss = 4.9109 (0.316 sec/step)\n",
            "I0707 16:52:39.793501 140263261390720 learning.py:507] global step 1108: loss = 4.7134 (0.322 sec/step)\n",
            "I0707 16:52:40.239689 140263261390720 learning.py:507] global step 1109: loss = 4.8248 (0.444 sec/step)\n",
            "I0707 16:52:40.719777 140263261390720 learning.py:507] global step 1110: loss = 6.7093 (0.478 sec/step)\n",
            "I0707 16:52:41.413688 140263261390720 learning.py:507] global step 1111: loss = 5.1283 (0.692 sec/step)\n",
            "I0707 16:52:41.722046 140263261390720 learning.py:507] global step 1112: loss = 5.1466 (0.307 sec/step)\n",
            "I0707 16:52:42.034785 140263261390720 learning.py:507] global step 1113: loss = 5.4551 (0.311 sec/step)\n",
            "I0707 16:52:42.363240 140263261390720 learning.py:507] global step 1114: loss = 5.8786 (0.327 sec/step)\n",
            "I0707 16:52:42.714260 140263261390720 learning.py:507] global step 1115: loss = 5.7745 (0.349 sec/step)\n",
            "I0707 16:52:43.039478 140263261390720 learning.py:507] global step 1116: loss = 3.8389 (0.324 sec/step)\n",
            "I0707 16:52:43.358983 140263261390720 learning.py:507] global step 1117: loss = 3.7277 (0.318 sec/step)\n",
            "I0707 16:52:43.955061 140263261390720 learning.py:507] global step 1118: loss = 5.1665 (0.594 sec/step)\n",
            "I0707 16:52:44.269669 140263261390720 learning.py:507] global step 1119: loss = 4.0122 (0.313 sec/step)\n",
            "I0707 16:52:44.564104 140263261390720 learning.py:507] global step 1120: loss = 4.8827 (0.293 sec/step)\n",
            "I0707 16:52:44.993777 140263261390720 learning.py:507] global step 1121: loss = 5.6124 (0.428 sec/step)\n",
            "I0707 16:52:45.282782 140263261390720 learning.py:507] global step 1122: loss = 5.8543 (0.287 sec/step)\n",
            "I0707 16:52:45.648089 140263261390720 learning.py:507] global step 1123: loss = 4.5262 (0.364 sec/step)\n",
            "I0707 16:52:45.979458 140263261390720 learning.py:507] global step 1124: loss = 3.7106 (0.329 sec/step)\n",
            "I0707 16:52:46.295730 140263261390720 learning.py:507] global step 1125: loss = 5.0136 (0.315 sec/step)\n",
            "I0707 16:52:46.622305 140263261390720 learning.py:507] global step 1126: loss = 4.6427 (0.325 sec/step)\n",
            "I0707 16:52:47.044348 140263261390720 learning.py:507] global step 1127: loss = 5.3396 (0.420 sec/step)\n",
            "I0707 16:52:47.340755 140263261390720 learning.py:507] global step 1128: loss = 5.1037 (0.294 sec/step)\n",
            "I0707 16:52:47.755170 140263261390720 learning.py:507] global step 1129: loss = 4.7028 (0.413 sec/step)\n",
            "I0707 16:52:48.206182 140263261390720 learning.py:507] global step 1130: loss = 4.9179 (0.449 sec/step)\n",
            "I0707 16:52:48.508059 140263261390720 learning.py:507] global step 1131: loss = 5.0089 (0.300 sec/step)\n",
            "I0707 16:52:48.852257 140263261390720 learning.py:507] global step 1132: loss = 5.9946 (0.342 sec/step)\n",
            "I0707 16:52:49.170952 140263261390720 learning.py:507] global step 1133: loss = 6.9384 (0.317 sec/step)\n",
            "I0707 16:52:49.497412 140263261390720 learning.py:507] global step 1134: loss = 4.3798 (0.325 sec/step)\n",
            "I0707 16:52:49.929666 140263261390720 learning.py:507] global step 1135: loss = 5.9664 (0.431 sec/step)\n",
            "I0707 16:52:50.254632 140263261390720 learning.py:507] global step 1136: loss = 4.1253 (0.323 sec/step)\n",
            "I0707 16:52:50.561354 140263261390720 learning.py:507] global step 1137: loss = 4.9763 (0.305 sec/step)\n",
            "I0707 16:52:51.131286 140263261390720 learning.py:507] global step 1138: loss = 5.2121 (0.568 sec/step)\n",
            "I0707 16:52:51.456125 140263261390720 learning.py:507] global step 1139: loss = 3.2161 (0.323 sec/step)\n",
            "I0707 16:52:51.759941 140263261390720 learning.py:507] global step 1140: loss = 3.7744 (0.302 sec/step)\n",
            "I0707 16:52:52.069955 140263261390720 learning.py:507] global step 1141: loss = 5.3948 (0.309 sec/step)\n",
            "I0707 16:52:52.492525 140263261390720 learning.py:507] global step 1142: loss = 4.2691 (0.421 sec/step)\n",
            "I0707 16:52:52.787932 140263261390720 learning.py:507] global step 1143: loss = 4.6258 (0.293 sec/step)\n",
            "I0707 16:52:53.430384 140263261390720 learning.py:507] global step 1144: loss = 4.7003 (0.641 sec/step)\n",
            "I0707 16:52:53.740143 140263261390720 learning.py:507] global step 1145: loss = 5.4217 (0.308 sec/step)\n",
            "I0707 16:52:54.045310 140263261390720 learning.py:507] global step 1146: loss = 4.1232 (0.304 sec/step)\n",
            "I0707 16:52:54.357707 140263261390720 learning.py:507] global step 1147: loss = 4.8778 (0.311 sec/step)\n",
            "I0707 16:52:54.673558 140263261390720 learning.py:507] global step 1148: loss = 4.3970 (0.314 sec/step)\n",
            "I0707 16:52:54.989907 140263261390720 learning.py:507] global step 1149: loss = 4.8113 (0.315 sec/step)\n",
            "I0707 16:52:55.631396 140263261390720 learning.py:507] global step 1150: loss = 5.2146 (0.640 sec/step)\n",
            "I0707 16:52:55.923455 140263261390720 learning.py:507] global step 1151: loss = 5.0347 (0.290 sec/step)\n",
            "I0707 16:52:56.229238 140263261390720 learning.py:507] global step 1152: loss = 5.7423 (0.304 sec/step)\n",
            "I0707 16:52:56.563426 140263261390720 learning.py:507] global step 1153: loss = 4.1171 (0.333 sec/step)\n",
            "I0707 16:52:56.874697 140263261390720 learning.py:507] global step 1154: loss = 5.7892 (0.310 sec/step)\n",
            "I0707 16:52:57.254510 140263261390720 learning.py:507] global step 1155: loss = 5.8699 (0.378 sec/step)\n",
            "I0707 16:52:57.935071 140263261390720 learning.py:507] global step 1156: loss = 4.2015 (0.679 sec/step)\n",
            "I0707 16:52:58.246391 140263261390720 learning.py:507] global step 1157: loss = 5.2922 (0.310 sec/step)\n",
            "I0707 16:52:58.556130 140263261390720 learning.py:507] global step 1158: loss = 5.5030 (0.308 sec/step)\n",
            "I0707 16:52:58.864340 140263261390720 learning.py:507] global step 1159: loss = 3.8296 (0.307 sec/step)\n",
            "I0707 16:52:59.174199 140263261390720 learning.py:507] global step 1160: loss = 4.6881 (0.308 sec/step)\n",
            "I0707 16:52:59.502366 140263261390720 learning.py:507] global step 1161: loss = 4.8371 (0.326 sec/step)\n",
            "I0707 16:52:59.868223 140263261390720 learning.py:507] global step 1162: loss = 5.2052 (0.364 sec/step)\n",
            "I0707 16:53:00.211358 140263261390720 learning.py:507] global step 1163: loss = 5.1193 (0.341 sec/step)\n",
            "I0707 16:53:00.514142 140263261390720 learning.py:507] global step 1164: loss = 4.1996 (0.301 sec/step)\n",
            "I0707 16:53:00.864806 140263261390720 learning.py:507] global step 1165: loss = 4.1759 (0.349 sec/step)\n",
            "I0707 16:53:01.173801 140263261390720 learning.py:507] global step 1166: loss = 5.2712 (0.307 sec/step)\n",
            "I0707 16:53:01.490313 140263261390720 learning.py:507] global step 1167: loss = 6.9611 (0.315 sec/step)\n",
            "I0707 16:53:01.828378 140263261390720 learning.py:507] global step 1168: loss = 5.5963 (0.336 sec/step)\n",
            "I0707 16:53:02.124724 140263261390720 learning.py:507] global step 1169: loss = 4.3275 (0.295 sec/step)\n",
            "I0707 16:53:02.608427 140263261390720 learning.py:507] global step 1170: loss = 4.4531 (0.482 sec/step)\n",
            "I0707 16:53:03.046540 140263261390720 learning.py:507] global step 1171: loss = 5.3564 (0.436 sec/step)\n",
            "I0707 16:53:03.346356 140263261390720 learning.py:507] global step 1172: loss = 5.9211 (0.298 sec/step)\n",
            "I0707 16:53:03.677258 140263261390720 learning.py:507] global step 1173: loss = 4.4405 (0.329 sec/step)\n",
            "I0707 16:53:04.124334 140263261390720 learning.py:507] global step 1174: loss = 6.0579 (0.445 sec/step)\n",
            "I0707 16:53:04.444253 140263261390720 learning.py:507] global step 1175: loss = 4.3706 (0.318 sec/step)\n",
            "I0707 16:53:05.035033 140263261390720 learning.py:507] global step 1176: loss = 6.3935 (0.589 sec/step)\n",
            "I0707 16:53:05.348308 140263261390720 learning.py:507] global step 1177: loss = 5.4630 (0.311 sec/step)\n",
            "I0707 16:53:05.730950 140263261390720 learning.py:507] global step 1178: loss = 4.5692 (0.381 sec/step)\n",
            "I0707 16:53:06.094250 140263261390720 learning.py:507] global step 1179: loss = 4.4942 (0.362 sec/step)\n",
            "I0707 16:53:06.530341 140263261390720 learning.py:507] global step 1180: loss = 5.3555 (0.434 sec/step)\n",
            "I0707 16:53:06.873280 140263261390720 learning.py:507] global step 1181: loss = 5.4745 (0.341 sec/step)\n",
            "I0707 16:53:07.331169 140263261390720 learning.py:507] global step 1182: loss = 5.4262 (0.456 sec/step)\n",
            "I0707 16:53:07.642161 140263261390720 learning.py:507] global step 1183: loss = 4.4921 (0.309 sec/step)\n",
            "I0707 16:53:07.965187 140263261390720 learning.py:507] global step 1184: loss = 4.2579 (0.322 sec/step)\n",
            "I0707 16:53:08.296754 140263261390720 learning.py:507] global step 1185: loss = 5.0539 (0.330 sec/step)\n",
            "I0707 16:53:08.629805 140263261390720 learning.py:507] global step 1186: loss = 6.1596 (0.331 sec/step)\n",
            "I0707 16:53:08.983486 140263261390720 learning.py:507] global step 1187: loss = 4.0275 (0.351 sec/step)\n",
            "I0707 16:53:09.353395 140263261390720 learning.py:507] global step 1188: loss = 5.2141 (0.368 sec/step)\n",
            "I0707 16:53:09.730504 140263261390720 learning.py:507] global step 1189: loss = 5.4896 (0.375 sec/step)\n",
            "I0707 16:53:10.057640 140263261390720 learning.py:507] global step 1190: loss = 5.9376 (0.325 sec/step)\n",
            "I0707 16:53:10.339846 140263261390720 learning.py:507] global step 1191: loss = 5.5887 (0.280 sec/step)\n",
            "I0707 16:53:10.644182 140263261390720 learning.py:507] global step 1192: loss = 4.1710 (0.303 sec/step)\n",
            "I0707 16:53:10.954433 140263261390720 learning.py:507] global step 1193: loss = 4.5169 (0.309 sec/step)\n",
            "I0707 16:53:11.318444 140263261390720 learning.py:507] global step 1194: loss = 4.3233 (0.362 sec/step)\n",
            "I0707 16:53:11.631176 140263261390720 learning.py:507] global step 1195: loss = 5.6359 (0.311 sec/step)\n",
            "I0707 16:53:11.963263 140263261390720 learning.py:507] global step 1196: loss = 4.3698 (0.331 sec/step)\n",
            "I0707 16:53:12.308028 140263261390720 learning.py:507] global step 1197: loss = 4.2130 (0.343 sec/step)\n",
            "I0707 16:53:12.663354 140263261390720 learning.py:507] global step 1198: loss = 5.4332 (0.354 sec/step)\n",
            "I0707 16:53:12.981909 140263261390720 learning.py:507] global step 1199: loss = 3.9637 (0.317 sec/step)\n",
            "I0707 16:53:13.514964 140263261390720 learning.py:507] global step 1200: loss = 5.0213 (0.531 sec/step)\n",
            "I0707 16:53:13.822603 140263261390720 learning.py:507] global step 1201: loss = 5.6183 (0.306 sec/step)\n",
            "I0707 16:53:14.401404 140263261390720 learning.py:507] global step 1202: loss = 4.3512 (0.577 sec/step)\n",
            "I0707 16:53:14.705851 140263261390720 learning.py:507] global step 1203: loss = 5.1657 (0.303 sec/step)\n",
            "I0707 16:53:15.085202 140263261390720 learning.py:507] global step 1204: loss = 3.7898 (0.378 sec/step)\n",
            "I0707 16:53:15.401126 140263261390720 learning.py:507] global step 1205: loss = 4.6130 (0.314 sec/step)\n",
            "I0707 16:53:15.941800 140263261390720 learning.py:507] global step 1206: loss = 5.4585 (0.539 sec/step)\n",
            "I0707 16:53:16.301493 140263261390720 learning.py:507] global step 1207: loss = 3.8555 (0.358 sec/step)\n",
            "I0707 16:53:16.994091 140263261390720 learning.py:507] global step 1208: loss = 5.2004 (0.691 sec/step)\n",
            "I0707 16:53:17.312847 140263261390720 learning.py:507] global step 1209: loss = 4.8876 (0.317 sec/step)\n",
            "I0707 16:53:17.674317 140263261390720 learning.py:507] global step 1210: loss = 4.3642 (0.360 sec/step)\n",
            "I0707 16:53:17.987845 140263261390720 learning.py:507] global step 1211: loss = 5.9928 (0.312 sec/step)\n",
            "I0707 16:53:18.360129 140263261390720 learning.py:507] global step 1212: loss = 5.1281 (0.370 sec/step)\n",
            "I0707 16:53:18.703972 140263261390720 learning.py:507] global step 1213: loss = 4.9992 (0.342 sec/step)\n",
            "I0707 16:53:19.001022 140263261390720 learning.py:507] global step 1214: loss = 4.8396 (0.295 sec/step)\n",
            "I0707 16:53:19.327242 140263261390720 learning.py:507] global step 1215: loss = 6.0664 (0.323 sec/step)\n",
            "I0707 16:53:19.859686 140263261390720 learning.py:507] global step 1216: loss = 3.7663 (0.531 sec/step)\n",
            "I0707 16:53:20.210973 140263261390720 learning.py:507] global step 1217: loss = 4.9956 (0.349 sec/step)\n",
            "I0707 16:53:20.551411 140263261390720 learning.py:507] global step 1218: loss = 5.2400 (0.339 sec/step)\n",
            "I0707 16:53:20.884421 140263261390720 learning.py:507] global step 1219: loss = 4.7906 (0.331 sec/step)\n",
            "I0707 16:53:21.220362 140263261390720 learning.py:507] global step 1220: loss = 4.3820 (0.334 sec/step)\n",
            "I0707 16:53:21.598859 140263261390720 learning.py:507] global step 1221: loss = 5.4796 (0.377 sec/step)\n",
            "I0707 16:53:21.898642 140263261390720 learning.py:507] global step 1222: loss = 4.6789 (0.298 sec/step)\n",
            "I0707 16:53:22.276895 140263261390720 learning.py:507] global step 1223: loss = 5.0839 (0.377 sec/step)\n",
            "I0707 16:53:22.705405 140263261390720 learning.py:507] global step 1224: loss = 4.7345 (0.427 sec/step)\n",
            "I0707 16:53:23.040058 140263261390720 learning.py:507] global step 1225: loss = 4.9846 (0.333 sec/step)\n",
            "I0707 16:53:23.410450 140263261390720 learning.py:507] global step 1226: loss = 3.3579 (0.368 sec/step)\n",
            "I0707 16:53:23.753519 140263261390720 learning.py:507] global step 1227: loss = 5.9534 (0.341 sec/step)\n",
            "I0707 16:53:24.072156 140263261390720 learning.py:507] global step 1228: loss = 4.9594 (0.317 sec/step)\n",
            "I0707 16:53:24.523971 140263261390720 learning.py:507] global step 1229: loss = 4.0644 (0.450 sec/step)\n",
            "I0707 16:53:24.818228 140263261390720 learning.py:507] global step 1230: loss = 6.2213 (0.292 sec/step)\n",
            "I0707 16:53:25.157736 140263261390720 learning.py:507] global step 1231: loss = 4.3840 (0.338 sec/step)\n",
            "I0707 16:53:25.471155 140263261390720 learning.py:507] global step 1232: loss = 4.0251 (0.312 sec/step)\n",
            "I0707 16:53:25.777291 140263261390720 learning.py:507] global step 1233: loss = 4.7394 (0.304 sec/step)\n",
            "I0707 16:53:26.082577 140263261390720 learning.py:507] global step 1234: loss = 3.9666 (0.304 sec/step)\n",
            "I0707 16:53:26.400063 140263261390720 learning.py:507] global step 1235: loss = 5.4896 (0.316 sec/step)\n",
            "I0707 16:53:26.713742 140263261390720 learning.py:507] global step 1236: loss = 5.0654 (0.312 sec/step)\n",
            "I0707 16:53:27.045932 140263261390720 learning.py:507] global step 1237: loss = 4.5437 (0.331 sec/step)\n",
            "I0707 16:53:27.509449 140263261390720 learning.py:507] global step 1238: loss = 3.9319 (0.462 sec/step)\n",
            "I0707 16:53:27.936511 140263261390720 learning.py:507] global step 1239: loss = 5.8677 (0.425 sec/step)\n",
            "I0707 16:53:28.285284 140263261390720 learning.py:507] global step 1240: loss = 5.6565 (0.347 sec/step)\n",
            "I0707 16:53:28.637430 140263261390720 learning.py:507] global step 1241: loss = 4.9337 (0.348 sec/step)\n",
            "I0707 16:53:28.952140 140263261390720 learning.py:507] global step 1242: loss = 5.6357 (0.312 sec/step)\n",
            "I0707 16:53:29.329992 140263261390720 learning.py:507] global step 1243: loss = 4.4738 (0.376 sec/step)\n",
            "I0707 16:53:29.783087 140263261390720 learning.py:507] global step 1244: loss = 5.3445 (0.451 sec/step)\n",
            "I0707 16:53:30.144259 140263261390720 learning.py:507] global step 1245: loss = 4.6408 (0.359 sec/step)\n",
            "I0707 16:53:30.483983 140263261390720 learning.py:507] global step 1246: loss = 4.6853 (0.338 sec/step)\n",
            "I0707 16:53:30.806679 140263261390720 learning.py:507] global step 1247: loss = 4.2763 (0.321 sec/step)\n",
            "I0707 16:53:31.167011 140263261390720 learning.py:507] global step 1248: loss = 5.0882 (0.358 sec/step)\n",
            "I0707 16:53:31.632384 140263261390720 learning.py:507] global step 1249: loss = 4.5934 (0.463 sec/step)\n",
            "I0707 16:53:32.107536 140263261390720 learning.py:507] global step 1250: loss = 4.9629 (0.473 sec/step)\n",
            "I0707 16:53:32.524108 140263261390720 learning.py:507] global step 1251: loss = 4.1995 (0.415 sec/step)\n",
            "I0707 16:53:32.857014 140263261390720 learning.py:507] global step 1252: loss = 4.2357 (0.331 sec/step)\n",
            "I0707 16:53:33.216916 140263261390720 learning.py:507] global step 1253: loss = 4.8312 (0.358 sec/step)\n",
            "I0707 16:53:33.523641 140263261390720 learning.py:507] global step 1254: loss = 4.9401 (0.305 sec/step)\n",
            "I0707 16:53:33.879615 140263261390720 learning.py:507] global step 1255: loss = 6.3829 (0.354 sec/step)\n",
            "I0707 16:53:34.173185 140263261390720 learning.py:507] global step 1256: loss = 6.7072 (0.292 sec/step)\n",
            "I0707 16:53:34.522224 140263261390720 learning.py:507] global step 1257: loss = 5.3449 (0.347 sec/step)\n",
            "I0707 16:53:34.845153 140263261390720 learning.py:507] global step 1258: loss = 4.8880 (0.321 sec/step)\n",
            "I0707 16:53:35.141888 140263261390720 learning.py:507] global step 1259: loss = 4.5103 (0.295 sec/step)\n",
            "I0707 16:53:35.493889 140263261390720 learning.py:507] global step 1260: loss = 5.5676 (0.350 sec/step)\n",
            "I0707 16:53:35.832329 140263261390720 learning.py:507] global step 1261: loss = 5.6559 (0.336 sec/step)\n",
            "I0707 16:53:36.127352 140263261390720 learning.py:507] global step 1262: loss = 5.8471 (0.293 sec/step)\n",
            "I0707 16:53:36.512708 140263261390720 learning.py:507] global step 1263: loss = 4.7159 (0.383 sec/step)\n",
            "I0707 16:53:36.821523 140263261390720 learning.py:507] global step 1264: loss = 4.6533 (0.307 sec/step)\n",
            "I0707 16:53:37.179074 140263261390720 learning.py:507] global step 1265: loss = 4.0792 (0.356 sec/step)\n",
            "I0707 16:53:37.538405 140263261390720 learning.py:507] global step 1266: loss = 5.8757 (0.358 sec/step)\n",
            "I0707 16:53:38.093100 140263261390720 learning.py:507] global step 1267: loss = 4.7832 (0.553 sec/step)\n",
            "I0707 16:53:38.401166 140263261390720 learning.py:507] global step 1268: loss = 4.7605 (0.306 sec/step)\n",
            "I0707 16:53:38.758912 140263261390720 learning.py:507] global step 1269: loss = 4.3561 (0.356 sec/step)\n",
            "I0707 16:53:39.099296 140263261390720 learning.py:507] global step 1270: loss = 4.3097 (0.339 sec/step)\n",
            "I0707 16:53:39.432867 140263261390720 learning.py:507] global step 1271: loss = 4.6633 (0.332 sec/step)\n",
            "I0707 16:53:39.822305 140263261390720 learning.py:507] global step 1272: loss = 4.3624 (0.387 sec/step)\n",
            "I0707 16:53:40.534243 140263261390720 learning.py:507] global step 1273: loss = 3.9934 (0.708 sec/step)\n",
            "I0707 16:53:40.873075 140263261390720 learning.py:507] global step 1274: loss = 5.1337 (0.337 sec/step)\n",
            "I0707 16:53:41.228160 140263261390720 learning.py:507] global step 1275: loss = 5.4433 (0.353 sec/step)\n",
            "I0707 16:53:41.560115 140263261390720 learning.py:507] global step 1276: loss = 4.7296 (0.330 sec/step)\n",
            "I0707 16:53:41.949489 140263261390720 learning.py:507] global step 1277: loss = 5.6071 (0.388 sec/step)\n",
            "I0707 16:53:42.355443 140263261390720 learning.py:507] global step 1278: loss = 3.9142 (0.404 sec/step)\n",
            "I0707 16:53:42.662472 140263261390720 learning.py:507] global step 1279: loss = 4.1511 (0.305 sec/step)\n",
            "I0707 16:53:43.103655 140263261390720 learning.py:507] global step 1280: loss = 4.3755 (0.440 sec/step)\n",
            "I0707 16:53:43.416007 140263261390720 learning.py:507] global step 1281: loss = 4.4618 (0.311 sec/step)\n",
            "I0707 16:53:43.723150 140263261390720 learning.py:507] global step 1282: loss = 4.8275 (0.305 sec/step)\n",
            "I0707 16:53:44.061168 140263261390720 learning.py:507] global step 1283: loss = 4.5364 (0.336 sec/step)\n",
            "I0707 16:53:44.362917 140263261390720 learning.py:507] global step 1284: loss = 4.4290 (0.300 sec/step)\n",
            "I0707 16:53:44.710512 140263261390720 learning.py:507] global step 1285: loss = 5.1721 (0.345 sec/step)\n",
            "I0707 16:53:45.049164 140263261390720 learning.py:507] global step 1286: loss = 5.2274 (0.337 sec/step)\n",
            "I0707 16:53:45.365497 140263261390720 learning.py:507] global step 1287: loss = 3.6993 (0.315 sec/step)\n",
            "I0707 16:53:45.678232 140263261390720 learning.py:507] global step 1288: loss = 4.0572 (0.311 sec/step)\n",
            "I0707 16:53:45.990583 140263261390720 learning.py:507] global step 1289: loss = 4.2350 (0.311 sec/step)\n",
            "I0707 16:53:46.313335 140263261390720 learning.py:507] global step 1290: loss = 5.5805 (0.321 sec/step)\n",
            "I0707 16:53:46.690758 140263261390720 learning.py:507] global step 1291: loss = 5.4665 (0.376 sec/step)\n",
            "I0707 16:53:46.999771 140263261390720 learning.py:507] global step 1292: loss = 3.2992 (0.307 sec/step)\n",
            "I0707 16:53:47.319870 140263261390720 learning.py:507] global step 1293: loss = 5.4742 (0.318 sec/step)\n",
            "I0707 16:53:47.624764 140263261390720 learning.py:507] global step 1294: loss = 3.4701 (0.303 sec/step)\n",
            "I0707 16:53:48.003259 140263261390720 learning.py:507] global step 1295: loss = 5.4332 (0.377 sec/step)\n",
            "I0707 16:53:48.330144 140263261390720 learning.py:507] global step 1296: loss = 4.6279 (0.325 sec/step)\n",
            "I0707 16:53:48.638359 140263261390720 learning.py:507] global step 1297: loss = 4.7735 (0.306 sec/step)\n",
            "I0707 16:53:49.002038 140263261390720 learning.py:507] global step 1298: loss = 6.3549 (0.362 sec/step)\n",
            "I0707 16:53:49.324064 140263261390720 learning.py:507] global step 1299: loss = 5.7200 (0.320 sec/step)\n",
            "I0707 16:53:49.699271 140263261390720 learning.py:507] global step 1300: loss = 6.5339 (0.373 sec/step)\n",
            "I0707 16:53:50.017871 140263261390720 learning.py:507] global step 1301: loss = 5.1555 (0.317 sec/step)\n",
            "I0707 16:53:50.361244 140263261390720 learning.py:507] global step 1302: loss = 5.9337 (0.342 sec/step)\n",
            "I0707 16:53:50.671979 140263261390720 learning.py:507] global step 1303: loss = 6.5403 (0.309 sec/step)\n",
            "I0707 16:53:51.040157 140263261390720 learning.py:507] global step 1304: loss = 5.3676 (0.366 sec/step)\n",
            "I0707 16:53:51.364757 140263261390720 learning.py:507] global step 1305: loss = 3.8990 (0.323 sec/step)\n",
            "I0707 16:53:51.824884 140263261390720 learning.py:507] global step 1306: loss = 5.0801 (0.458 sec/step)\n",
            "I0707 16:53:52.120677 140263261390720 learning.py:507] global step 1307: loss = 6.1725 (0.294 sec/step)\n",
            "I0707 16:53:52.580355 140263261390720 learning.py:507] global step 1308: loss = 5.7167 (0.458 sec/step)\n",
            "I0707 16:53:52.914737 140263261390720 learning.py:507] global step 1309: loss = 3.8496 (0.333 sec/step)\n",
            "I0707 16:53:53.570342 140263261390720 learning.py:507] global step 1310: loss = 5.2324 (0.544 sec/step)\n",
            "I0707 16:53:53.676954 140260287772416 supervisor.py:1050] Recording summary at step 1310.\n",
            "I0707 16:53:53.928185 140263261390720 learning.py:507] global step 1311: loss = 4.8716 (0.356 sec/step)\n",
            "I0707 16:53:54.245263 140263261390720 learning.py:507] global step 1312: loss = 4.6403 (0.316 sec/step)\n",
            "I0707 16:53:54.638909 140263261390720 learning.py:507] global step 1313: loss = 4.3464 (0.392 sec/step)\n",
            "I0707 16:53:54.825429 140260296165120 supervisor.py:1099] global_step/sec: 2.78336\n",
            "I0707 16:53:54.952002 140263261390720 learning.py:507] global step 1314: loss = 5.0180 (0.311 sec/step)\n",
            "I0707 16:53:55.304991 140263261390720 learning.py:507] global step 1315: loss = 4.9837 (0.351 sec/step)\n",
            "I0707 16:53:55.616974 140263261390720 learning.py:507] global step 1316: loss = 3.9105 (0.310 sec/step)\n",
            "I0707 16:53:55.928781 140263261390720 learning.py:507] global step 1317: loss = 4.6847 (0.310 sec/step)\n",
            "I0707 16:53:56.228470 140263261390720 learning.py:507] global step 1318: loss = 3.7500 (0.298 sec/step)\n",
            "I0707 16:53:56.586161 140263261390720 learning.py:507] global step 1319: loss = 5.0766 (0.356 sec/step)\n",
            "I0707 16:53:56.879459 140263261390720 learning.py:507] global step 1320: loss = 5.5352 (0.292 sec/step)\n",
            "I0707 16:53:57.192110 140263261390720 learning.py:507] global step 1321: loss = 5.1873 (0.311 sec/step)\n",
            "I0707 16:53:57.525981 140263261390720 learning.py:507] global step 1322: loss = 5.0049 (0.332 sec/step)\n",
            "I0707 16:53:57.863059 140263261390720 learning.py:507] global step 1323: loss = 5.2169 (0.335 sec/step)\n",
            "I0707 16:53:58.168509 140263261390720 learning.py:507] global step 1324: loss = 3.5763 (0.304 sec/step)\n",
            "I0707 16:53:58.585803 140263261390720 learning.py:507] global step 1325: loss = 3.6329 (0.415 sec/step)\n",
            "I0707 16:53:58.905791 140263261390720 learning.py:507] global step 1326: loss = 4.6508 (0.318 sec/step)\n",
            "I0707 16:53:59.253572 140263261390720 learning.py:507] global step 1327: loss = 6.1284 (0.346 sec/step)\n",
            "I0707 16:53:59.585240 140263261390720 learning.py:507] global step 1328: loss = 4.5758 (0.329 sec/step)\n",
            "I0707 16:53:59.922681 140263261390720 learning.py:507] global step 1329: loss = 4.4524 (0.336 sec/step)\n",
            "I0707 16:54:00.291957 140263261390720 learning.py:507] global step 1330: loss = 6.0339 (0.368 sec/step)\n",
            "I0707 16:54:00.630600 140263261390720 learning.py:507] global step 1331: loss = 4.5457 (0.337 sec/step)\n",
            "I0707 16:54:00.938968 140263261390720 learning.py:507] global step 1332: loss = 4.4821 (0.306 sec/step)\n",
            "I0707 16:54:01.351771 140263261390720 learning.py:507] global step 1333: loss = 4.3083 (0.411 sec/step)\n",
            "I0707 16:54:01.703572 140263261390720 learning.py:507] global step 1334: loss = 5.3021 (0.350 sec/step)\n",
            "I0707 16:54:02.248944 140263261390720 learning.py:507] global step 1335: loss = 4.9395 (0.544 sec/step)\n",
            "I0707 16:54:02.575475 140263261390720 learning.py:507] global step 1336: loss = 5.1397 (0.325 sec/step)\n",
            "I0707 16:54:02.922798 140263261390720 learning.py:507] global step 1337: loss = 4.3094 (0.345 sec/step)\n",
            "I0707 16:54:03.247250 140263261390720 learning.py:507] global step 1338: loss = 5.5425 (0.323 sec/step)\n",
            "I0707 16:54:03.551013 140263261390720 learning.py:507] global step 1339: loss = 5.4738 (0.302 sec/step)\n",
            "I0707 16:54:03.871333 140263261390720 learning.py:507] global step 1340: loss = 4.1134 (0.319 sec/step)\n",
            "I0707 16:54:04.564933 140263261390720 learning.py:507] global step 1341: loss = 4.7034 (0.692 sec/step)\n",
            "I0707 16:54:04.895397 140263261390720 learning.py:507] global step 1342: loss = 4.1282 (0.329 sec/step)\n",
            "I0707 16:54:05.204036 140263261390720 learning.py:507] global step 1343: loss = 5.3865 (0.307 sec/step)\n",
            "I0707 16:54:05.498543 140263261390720 learning.py:507] global step 1344: loss = 6.7202 (0.293 sec/step)\n",
            "I0707 16:54:05.807321 140263261390720 learning.py:507] global step 1345: loss = 3.9175 (0.307 sec/step)\n",
            "I0707 16:54:06.105970 140263261390720 learning.py:507] global step 1346: loss = 3.8060 (0.297 sec/step)\n",
            "I0707 16:54:06.431439 140263261390720 learning.py:507] global step 1347: loss = 5.1375 (0.323 sec/step)\n",
            "I0707 16:54:06.771792 140263261390720 learning.py:507] global step 1348: loss = 4.9844 (0.338 sec/step)\n",
            "I0707 16:54:07.103672 140263261390720 learning.py:507] global step 1349: loss = 4.1365 (0.330 sec/step)\n",
            "I0707 16:54:07.419292 140263261390720 learning.py:507] global step 1350: loss = 6.4009 (0.314 sec/step)\n",
            "I0707 16:54:07.797095 140263261390720 learning.py:507] global step 1351: loss = 6.2118 (0.376 sec/step)\n",
            "I0707 16:54:08.114288 140263261390720 learning.py:507] global step 1352: loss = 5.4334 (0.315 sec/step)\n",
            "I0707 16:54:08.712782 140263261390720 learning.py:507] global step 1353: loss = 4.1437 (0.597 sec/step)\n",
            "I0707 16:54:09.058923 140263261390720 learning.py:507] global step 1354: loss = 4.8942 (0.345 sec/step)\n",
            "I0707 16:54:09.348451 140263261390720 learning.py:507] global step 1355: loss = 4.1223 (0.287 sec/step)\n",
            "I0707 16:54:09.696578 140263261390720 learning.py:507] global step 1356: loss = 4.8616 (0.346 sec/step)\n",
            "I0707 16:54:10.099363 140263261390720 learning.py:507] global step 1357: loss = 4.9367 (0.401 sec/step)\n",
            "I0707 16:54:10.423174 140263261390720 learning.py:507] global step 1358: loss = 5.8334 (0.322 sec/step)\n",
            "I0707 16:54:11.138668 140263261390720 learning.py:507] global step 1359: loss = 4.4353 (0.714 sec/step)\n",
            "I0707 16:54:11.450928 140263261390720 learning.py:507] global step 1360: loss = 4.6096 (0.311 sec/step)\n",
            "I0707 16:54:11.868161 140263261390720 learning.py:507] global step 1361: loss = 5.6831 (0.415 sec/step)\n",
            "I0707 16:54:12.209162 140263261390720 learning.py:507] global step 1362: loss = 4.5844 (0.339 sec/step)\n",
            "I0707 16:54:12.559706 140263261390720 learning.py:507] global step 1363: loss = 5.2347 (0.349 sec/step)\n",
            "I0707 16:54:12.887055 140263261390720 learning.py:507] global step 1364: loss = 7.1789 (0.326 sec/step)\n",
            "I0707 16:54:13.204575 140263261390720 learning.py:507] global step 1365: loss = 5.9935 (0.316 sec/step)\n",
            "I0707 16:54:13.517906 140263261390720 learning.py:507] global step 1366: loss = 4.2113 (0.312 sec/step)\n",
            "I0707 16:54:13.864928 140263261390720 learning.py:507] global step 1367: loss = 5.2783 (0.345 sec/step)\n",
            "I0707 16:54:14.193681 140263261390720 learning.py:507] global step 1368: loss = 5.1407 (0.327 sec/step)\n",
            "I0707 16:54:14.606677 140263261390720 learning.py:507] global step 1369: loss = 5.0562 (0.411 sec/step)\n",
            "I0707 16:54:14.927040 140263261390720 learning.py:507] global step 1370: loss = 4.5387 (0.319 sec/step)\n",
            "I0707 16:54:15.235781 140263261390720 learning.py:507] global step 1371: loss = 6.4555 (0.307 sec/step)\n",
            "I0707 16:54:15.545503 140263261390720 learning.py:507] global step 1372: loss = 4.9008 (0.308 sec/step)\n",
            "I0707 16:54:15.853780 140263261390720 learning.py:507] global step 1373: loss = 6.9589 (0.307 sec/step)\n",
            "I0707 16:54:16.174901 140263261390720 learning.py:507] global step 1374: loss = 5.1942 (0.319 sec/step)\n",
            "I0707 16:54:16.501597 140263261390720 learning.py:507] global step 1375: loss = 3.5772 (0.325 sec/step)\n",
            "I0707 16:54:16.835154 140263261390720 learning.py:507] global step 1376: loss = 3.9254 (0.332 sec/step)\n",
            "I0707 16:54:17.153202 140263261390720 learning.py:507] global step 1377: loss = 4.1900 (0.316 sec/step)\n",
            "I0707 16:54:17.444620 140263261390720 learning.py:507] global step 1378: loss = 4.7983 (0.290 sec/step)\n",
            "I0707 16:54:17.757864 140263261390720 learning.py:507] global step 1379: loss = 6.8457 (0.312 sec/step)\n",
            "I0707 16:54:18.053377 140263261390720 learning.py:507] global step 1380: loss = 3.7995 (0.294 sec/step)\n",
            "I0707 16:54:18.372288 140263261390720 learning.py:507] global step 1381: loss = 3.7628 (0.317 sec/step)\n",
            "I0707 16:54:18.787497 140263261390720 learning.py:507] global step 1382: loss = 3.4775 (0.414 sec/step)\n",
            "I0707 16:54:19.103721 140263261390720 learning.py:507] global step 1383: loss = 5.2576 (0.314 sec/step)\n",
            "I0707 16:54:19.423340 140263261390720 learning.py:507] global step 1384: loss = 5.0076 (0.318 sec/step)\n",
            "I0707 16:54:19.761694 140263261390720 learning.py:507] global step 1385: loss = 5.5266 (0.336 sec/step)\n",
            "I0707 16:54:20.070226 140263261390720 learning.py:507] global step 1386: loss = 3.8720 (0.307 sec/step)\n",
            "I0707 16:54:20.421562 140263261390720 learning.py:507] global step 1387: loss = 4.3383 (0.350 sec/step)\n",
            "I0707 16:54:20.847420 140263261390720 learning.py:507] global step 1388: loss = 4.5225 (0.424 sec/step)\n",
            "I0707 16:54:21.414783 140263261390720 learning.py:507] global step 1389: loss = 4.6028 (0.566 sec/step)\n",
            "I0707 16:54:21.801539 140263261390720 learning.py:507] global step 1390: loss = 5.1431 (0.385 sec/step)\n",
            "I0707 16:54:22.112460 140263261390720 learning.py:507] global step 1391: loss = 3.8923 (0.309 sec/step)\n",
            "I0707 16:54:22.419692 140263261390720 learning.py:507] global step 1392: loss = 3.8938 (0.305 sec/step)\n",
            "I0707 16:54:22.805052 140263261390720 learning.py:507] global step 1393: loss = 4.4637 (0.384 sec/step)\n",
            "I0707 16:54:23.115977 140263261390720 learning.py:507] global step 1394: loss = 6.9508 (0.309 sec/step)\n",
            "I0707 16:54:23.809284 140263261390720 learning.py:507] global step 1395: loss = 6.4539 (0.692 sec/step)\n",
            "I0707 16:54:24.234399 140263261390720 learning.py:507] global step 1396: loss = 5.5344 (0.423 sec/step)\n",
            "I0707 16:54:24.576601 140263261390720 learning.py:507] global step 1397: loss = 6.3676 (0.340 sec/step)\n",
            "I0707 16:54:24.879760 140263261390720 learning.py:507] global step 1398: loss = 5.0465 (0.301 sec/step)\n",
            "I0707 16:54:25.207347 140263261390720 learning.py:507] global step 1399: loss = 4.2712 (0.326 sec/step)\n",
            "I0707 16:54:25.525577 140263261390720 learning.py:507] global step 1400: loss = 5.2768 (0.316 sec/step)\n",
            "I0707 16:54:25.841643 140263261390720 learning.py:507] global step 1401: loss = 3.6474 (0.314 sec/step)\n",
            "I0707 16:54:26.199908 140263261390720 learning.py:507] global step 1402: loss = 4.1222 (0.357 sec/step)\n",
            "I0707 16:54:26.545540 140263261390720 learning.py:507] global step 1403: loss = 3.5696 (0.344 sec/step)\n",
            "I0707 16:54:26.836856 140263261390720 learning.py:507] global step 1404: loss = 5.1347 (0.290 sec/step)\n",
            "I0707 16:54:27.161531 140263261390720 learning.py:507] global step 1405: loss = 5.3295 (0.323 sec/step)\n",
            "I0707 16:54:27.474426 140263261390720 learning.py:507] global step 1406: loss = 5.1688 (0.311 sec/step)\n",
            "I0707 16:54:27.783576 140263261390720 learning.py:507] global step 1407: loss = 3.9256 (0.308 sec/step)\n",
            "I0707 16:54:28.087671 140263261390720 learning.py:507] global step 1408: loss = 5.0003 (0.302 sec/step)\n",
            "I0707 16:54:28.426953 140263261390720 learning.py:507] global step 1409: loss = 4.1263 (0.337 sec/step)\n",
            "I0707 16:54:28.870365 140263261390720 learning.py:507] global step 1410: loss = 4.8755 (0.441 sec/step)\n",
            "I0707 16:54:29.221991 140263261390720 learning.py:507] global step 1411: loss = 5.1364 (0.350 sec/step)\n",
            "I0707 16:54:29.539736 140263261390720 learning.py:507] global step 1412: loss = 4.1137 (0.316 sec/step)\n",
            "I0707 16:54:29.855747 140263261390720 learning.py:507] global step 1413: loss = 4.0916 (0.314 sec/step)\n",
            "I0707 16:54:30.144215 140263261390720 learning.py:507] global step 1414: loss = 5.7842 (0.287 sec/step)\n",
            "I0707 16:54:30.441291 140263261390720 learning.py:507] global step 1415: loss = 5.5625 (0.295 sec/step)\n",
            "I0707 16:54:30.856446 140263261390720 learning.py:507] global step 1416: loss = 5.9617 (0.414 sec/step)\n",
            "I0707 16:54:31.193274 140263261390720 learning.py:507] global step 1417: loss = 6.0511 (0.335 sec/step)\n",
            "I0707 16:54:31.690505 140263261390720 learning.py:507] global step 1418: loss = 5.6621 (0.495 sec/step)\n",
            "I0707 16:54:31.994868 140263261390720 learning.py:507] global step 1419: loss = 4.5670 (0.303 sec/step)\n",
            "I0707 16:54:32.296687 140263261390720 learning.py:507] global step 1420: loss = 5.4574 (0.300 sec/step)\n",
            "I0707 16:54:32.609555 140263261390720 learning.py:507] global step 1421: loss = 6.2766 (0.311 sec/step)\n",
            "I0707 16:54:32.957106 140263261390720 learning.py:507] global step 1422: loss = 5.3365 (0.346 sec/step)\n",
            "I0707 16:54:33.299320 140263261390720 learning.py:507] global step 1423: loss = 4.4839 (0.340 sec/step)\n",
            "I0707 16:54:33.616218 140263261390720 learning.py:507] global step 1424: loss = 5.0673 (0.314 sec/step)\n",
            "I0707 16:54:33.953674 140263261390720 learning.py:507] global step 1425: loss = 4.5968 (0.335 sec/step)\n",
            "I0707 16:54:34.290672 140263261390720 learning.py:507] global step 1426: loss = 4.7268 (0.335 sec/step)\n",
            "I0707 16:54:34.623966 140263261390720 learning.py:507] global step 1427: loss = 5.3042 (0.332 sec/step)\n",
            "I0707 16:54:35.009002 140263261390720 learning.py:507] global step 1428: loss = 4.6065 (0.383 sec/step)\n",
            "I0707 16:54:35.319626 140263261390720 learning.py:507] global step 1429: loss = 4.9917 (0.309 sec/step)\n",
            "I0707 16:54:35.651927 140263261390720 learning.py:507] global step 1430: loss = 6.5575 (0.330 sec/step)\n",
            "I0707 16:54:35.973676 140263261390720 learning.py:507] global step 1431: loss = 4.4833 (0.320 sec/step)\n",
            "I0707 16:54:36.272819 140263261390720 learning.py:507] global step 1432: loss = 5.6594 (0.297 sec/step)\n",
            "I0707 16:54:36.576551 140263261390720 learning.py:507] global step 1433: loss = 6.2210 (0.302 sec/step)\n",
            "I0707 16:54:36.898918 140263261390720 learning.py:507] global step 1434: loss = 5.0224 (0.321 sec/step)\n",
            "I0707 16:54:37.202786 140263261390720 learning.py:507] global step 1435: loss = 4.5640 (0.302 sec/step)\n",
            "I0707 16:54:37.495280 140263261390720 learning.py:507] global step 1436: loss = 4.4777 (0.291 sec/step)\n",
            "I0707 16:54:37.847193 140263261390720 learning.py:507] global step 1437: loss = 4.6231 (0.350 sec/step)\n",
            "I0707 16:54:38.264452 140263261390720 learning.py:507] global step 1438: loss = 4.8249 (0.416 sec/step)\n",
            "I0707 16:54:38.577435 140263261390720 learning.py:507] global step 1439: loss = 5.2674 (0.311 sec/step)\n",
            "I0707 16:54:38.890658 140263261390720 learning.py:507] global step 1440: loss = 4.2467 (0.312 sec/step)\n",
            "I0707 16:54:39.193893 140263261390720 learning.py:507] global step 1441: loss = 5.7045 (0.301 sec/step)\n",
            "I0707 16:54:39.496572 140263261390720 learning.py:507] global step 1442: loss = 4.5384 (0.301 sec/step)\n",
            "I0707 16:54:39.812876 140263261390720 learning.py:507] global step 1443: loss = 5.5100 (0.315 sec/step)\n",
            "I0707 16:54:40.262262 140263261390720 learning.py:507] global step 1444: loss = 5.2054 (0.448 sec/step)\n",
            "I0707 16:54:40.582712 140263261390720 learning.py:507] global step 1445: loss = 5.2103 (0.319 sec/step)\n",
            "I0707 16:54:40.948729 140263261390720 learning.py:507] global step 1446: loss = 4.0104 (0.365 sec/step)\n",
            "I0707 16:54:41.254045 140263261390720 learning.py:507] global step 1447: loss = 5.8976 (0.304 sec/step)\n",
            "I0707 16:54:41.557590 140263261390720 learning.py:507] global step 1448: loss = 5.3552 (0.302 sec/step)\n",
            "I0707 16:54:41.848415 140263261390720 learning.py:507] global step 1449: loss = 3.5095 (0.289 sec/step)\n",
            "I0707 16:54:42.167338 140263261390720 learning.py:507] global step 1450: loss = 5.3383 (0.317 sec/step)\n",
            "I0707 16:54:42.471846 140263261390720 learning.py:507] global step 1451: loss = 6.1150 (0.303 sec/step)\n",
            "I0707 16:54:42.831278 140263261390720 learning.py:507] global step 1452: loss = 3.5483 (0.358 sec/step)\n",
            "I0707 16:54:43.136005 140263261390720 learning.py:507] global step 1453: loss = 4.6113 (0.303 sec/step)\n",
            "I0707 16:54:43.456053 140263261390720 learning.py:507] global step 1454: loss = 3.8875 (0.319 sec/step)\n",
            "I0707 16:54:43.787785 140263261390720 learning.py:507] global step 1455: loss = 4.7624 (0.330 sec/step)\n",
            "I0707 16:54:44.118558 140263261390720 learning.py:507] global step 1456: loss = 5.4804 (0.329 sec/step)\n",
            "I0707 16:54:44.455779 140263261390720 learning.py:507] global step 1457: loss = 3.7341 (0.336 sec/step)\n",
            "I0707 16:54:44.776657 140263261390720 learning.py:507] global step 1458: loss = 4.7413 (0.319 sec/step)\n",
            "I0707 16:54:45.097408 140263261390720 learning.py:507] global step 1459: loss = 4.8961 (0.319 sec/step)\n",
            "I0707 16:54:45.422896 140263261390720 learning.py:507] global step 1460: loss = 5.1778 (0.324 sec/step)\n",
            "I0707 16:54:45.736413 140263261390720 learning.py:507] global step 1461: loss = 4.2019 (0.312 sec/step)\n",
            "I0707 16:54:46.117986 140263261390720 learning.py:507] global step 1462: loss = 4.7449 (0.379 sec/step)\n",
            "I0707 16:54:46.467612 140263261390720 learning.py:507] global step 1463: loss = 6.1596 (0.348 sec/step)\n",
            "I0707 16:54:46.802614 140263261390720 learning.py:507] global step 1464: loss = 4.8989 (0.333 sec/step)\n",
            "I0707 16:54:47.123772 140263261390720 learning.py:507] global step 1465: loss = 4.1329 (0.320 sec/step)\n",
            "I0707 16:54:47.446776 140263261390720 learning.py:507] global step 1466: loss = 4.0219 (0.321 sec/step)\n",
            "I0707 16:54:47.755774 140263261390720 learning.py:507] global step 1467: loss = 4.7915 (0.307 sec/step)\n",
            "I0707 16:54:48.143900 140263261390720 learning.py:507] global step 1468: loss = 3.9507 (0.386 sec/step)\n",
            "I0707 16:54:48.469974 140263261390720 learning.py:507] global step 1469: loss = 4.6236 (0.324 sec/step)\n",
            "I0707 16:54:48.803965 140263261390720 learning.py:507] global step 1470: loss = 3.5590 (0.332 sec/step)\n",
            "I0707 16:54:49.124869 140263261390720 learning.py:507] global step 1471: loss = 4.3349 (0.319 sec/step)\n",
            "I0707 16:54:49.440733 140263261390720 learning.py:507] global step 1472: loss = 3.5846 (0.314 sec/step)\n",
            "I0707 16:54:49.731924 140263261390720 learning.py:507] global step 1473: loss = 5.4211 (0.290 sec/step)\n",
            "I0707 16:54:50.119861 140263261390720 learning.py:507] global step 1474: loss = 3.5090 (0.385 sec/step)\n",
            "I0707 16:54:50.450955 140263261390720 learning.py:507] global step 1475: loss = 5.1055 (0.329 sec/step)\n",
            "I0707 16:54:50.778854 140263261390720 learning.py:507] global step 1476: loss = 5.0762 (0.326 sec/step)\n",
            "I0707 16:54:51.114325 140263261390720 learning.py:507] global step 1477: loss = 4.4026 (0.334 sec/step)\n",
            "I0707 16:54:51.434335 140263261390720 learning.py:507] global step 1478: loss = 3.8839 (0.318 sec/step)\n",
            "I0707 16:54:51.729452 140263261390720 learning.py:507] global step 1479: loss = 5.2697 (0.293 sec/step)\n",
            "I0707 16:54:52.138681 140263261390720 learning.py:507] global step 1480: loss = 4.4281 (0.407 sec/step)\n",
            "I0707 16:54:52.538682 140263261390720 learning.py:507] global step 1481: loss = 4.8666 (0.398 sec/step)\n",
            "I0707 16:54:52.938191 140263261390720 learning.py:507] global step 1482: loss = 4.2018 (0.398 sec/step)\n",
            "I0707 16:54:53.250072 140263261390720 learning.py:507] global step 1483: loss = 3.8566 (0.310 sec/step)\n",
            "I0707 16:54:53.612140 140263261390720 learning.py:507] global step 1484: loss = 3.9013 (0.360 sec/step)\n",
            "I0707 16:54:53.928093 140263261390720 learning.py:507] global step 1485: loss = 4.2258 (0.314 sec/step)\n",
            "I0707 16:54:54.326751 140263261390720 learning.py:507] global step 1486: loss = 4.3800 (0.397 sec/step)\n",
            "I0707 16:54:54.635770 140263261390720 learning.py:507] global step 1487: loss = 5.5121 (0.307 sec/step)\n",
            "I0707 16:54:54.997224 140263261390720 learning.py:507] global step 1488: loss = 4.8361 (0.360 sec/step)\n",
            "I0707 16:54:55.291815 140263261390720 learning.py:507] global step 1489: loss = 5.2554 (0.292 sec/step)\n",
            "I0707 16:54:55.652422 140263261390720 learning.py:507] global step 1490: loss = 5.3660 (0.358 sec/step)\n",
            "I0707 16:54:55.962955 140263261390720 learning.py:507] global step 1491: loss = 5.6436 (0.309 sec/step)\n",
            "I0707 16:54:56.317998 140263261390720 learning.py:507] global step 1492: loss = 3.9142 (0.353 sec/step)\n",
            "I0707 16:54:56.784731 140263261390720 learning.py:507] global step 1493: loss = 4.3457 (0.463 sec/step)\n",
            "I0707 16:54:57.169354 140263261390720 learning.py:507] global step 1494: loss = 5.0933 (0.382 sec/step)\n",
            "I0707 16:54:57.503500 140263261390720 learning.py:507] global step 1495: loss = 4.3822 (0.332 sec/step)\n",
            "I0707 16:54:57.854525 140263261390720 learning.py:507] global step 1496: loss = 3.6480 (0.349 sec/step)\n",
            "I0707 16:54:58.232732 140263261390720 learning.py:507] global step 1497: loss = 3.6808 (0.377 sec/step)\n",
            "I0707 16:54:58.604481 140263261390720 learning.py:507] global step 1498: loss = 4.6668 (0.370 sec/step)\n",
            "I0707 16:54:58.903600 140263261390720 learning.py:507] global step 1499: loss = 5.1002 (0.297 sec/step)\n",
            "I0707 16:54:59.273431 140263261390720 learning.py:507] global step 1500: loss = 4.5068 (0.368 sec/step)\n",
            "I0707 16:54:59.674466 140263261390720 learning.py:507] global step 1501: loss = 5.3267 (0.399 sec/step)\n",
            "I0707 16:54:59.977275 140263261390720 learning.py:507] global step 1502: loss = 4.7472 (0.301 sec/step)\n",
            "I0707 16:55:00.400732 140263261390720 learning.py:507] global step 1503: loss = 5.8734 (0.422 sec/step)\n",
            "I0707 16:55:00.716667 140263261390720 learning.py:507] global step 1504: loss = 4.4868 (0.314 sec/step)\n",
            "I0707 16:55:01.007994 140263261390720 learning.py:507] global step 1505: loss = 4.7184 (0.289 sec/step)\n",
            "I0707 16:55:01.332459 140263261390720 learning.py:507] global step 1506: loss = 5.2661 (0.323 sec/step)\n",
            "I0707 16:55:01.660514 140263261390720 learning.py:507] global step 1507: loss = 5.0492 (0.326 sec/step)\n",
            "I0707 16:55:01.994959 140263261390720 learning.py:507] global step 1508: loss = 4.0340 (0.333 sec/step)\n",
            "I0707 16:55:02.311647 140263261390720 learning.py:507] global step 1509: loss = 5.2202 (0.315 sec/step)\n",
            "I0707 16:55:02.621924 140263261390720 learning.py:507] global step 1510: loss = 4.5133 (0.309 sec/step)\n",
            "I0707 16:55:02.929619 140263261390720 learning.py:507] global step 1511: loss = 4.2222 (0.306 sec/step)\n",
            "I0707 16:55:03.241553 140263261390720 learning.py:507] global step 1512: loss = 4.5103 (0.310 sec/step)\n",
            "I0707 16:55:03.596470 140263261390720 learning.py:507] global step 1513: loss = 4.3734 (0.353 sec/step)\n",
            "I0707 16:55:03.913947 140263261390720 learning.py:507] global step 1514: loss = 6.3149 (0.316 sec/step)\n",
            "I0707 16:55:04.218789 140263261390720 learning.py:507] global step 1515: loss = 5.9418 (0.303 sec/step)\n",
            "I0707 16:55:04.544875 140263261390720 learning.py:507] global step 1516: loss = 4.5301 (0.325 sec/step)\n",
            "I0707 16:55:04.868709 140263261390720 learning.py:507] global step 1517: loss = 4.9038 (0.322 sec/step)\n",
            "I0707 16:55:05.163512 140263261390720 learning.py:507] global step 1518: loss = 4.7772 (0.293 sec/step)\n",
            "I0707 16:55:05.587756 140263261390720 learning.py:507] global step 1519: loss = 6.0690 (0.422 sec/step)\n",
            "I0707 16:55:05.947448 140263261390720 learning.py:507] global step 1520: loss = 4.5252 (0.358 sec/step)\n",
            "I0707 16:55:06.493709 140263261390720 learning.py:507] global step 1521: loss = 5.8958 (0.545 sec/step)\n",
            "I0707 16:55:06.803017 140263261390720 learning.py:507] global step 1522: loss = 5.0930 (0.308 sec/step)\n",
            "I0707 16:55:07.136640 140263261390720 learning.py:507] global step 1523: loss = 3.7792 (0.332 sec/step)\n",
            "I0707 16:55:07.446392 140263261390720 learning.py:507] global step 1524: loss = 5.4049 (0.308 sec/step)\n",
            "I0707 16:55:07.785377 140263261390720 learning.py:507] global step 1525: loss = 3.8921 (0.337 sec/step)\n",
            "I0707 16:55:08.123257 140263261390720 learning.py:507] global step 1526: loss = 4.8318 (0.336 sec/step)\n",
            "I0707 16:55:08.886653 140263261390720 learning.py:507] global step 1527: loss = 3.7256 (0.762 sec/step)\n",
            "I0707 16:55:09.230610 140263261390720 learning.py:507] global step 1528: loss = 5.3567 (0.342 sec/step)\n",
            "I0707 16:55:09.536170 140263261390720 learning.py:507] global step 1529: loss = 5.0637 (0.304 sec/step)\n",
            "I0707 16:55:09.894182 140263261390720 learning.py:507] global step 1530: loss = 5.2868 (0.356 sec/step)\n",
            "I0707 16:55:10.221252 140263261390720 learning.py:507] global step 1531: loss = 4.8557 (0.325 sec/step)\n",
            "I0707 16:55:10.561894 140263261390720 learning.py:507] global step 1532: loss = 4.7619 (0.339 sec/step)\n",
            "I0707 16:55:10.897858 140263261390720 learning.py:507] global step 1533: loss = 4.4385 (0.334 sec/step)\n",
            "I0707 16:55:11.241520 140263261390720 learning.py:507] global step 1534: loss = 4.3315 (0.342 sec/step)\n",
            "I0707 16:55:11.539508 140263261390720 learning.py:507] global step 1535: loss = 3.9817 (0.296 sec/step)\n",
            "I0707 16:55:11.900901 140263261390720 learning.py:507] global step 1536: loss = 5.5251 (0.360 sec/step)\n",
            "I0707 16:55:12.218654 140263261390720 learning.py:507] global step 1537: loss = 4.0684 (0.316 sec/step)\n",
            "I0707 16:55:12.519074 140263261390720 learning.py:507] global step 1538: loss = 5.3547 (0.298 sec/step)\n",
            "I0707 16:55:12.818879 140263261390720 learning.py:507] global step 1539: loss = 5.3327 (0.298 sec/step)\n",
            "I0707 16:55:13.128259 140263261390720 learning.py:507] global step 1540: loss = 4.9121 (0.307 sec/step)\n",
            "I0707 16:55:13.414196 140263261390720 learning.py:507] global step 1541: loss = 5.7898 (0.284 sec/step)\n",
            "I0707 16:55:13.733307 140263261390720 learning.py:507] global step 1542: loss = 5.0244 (0.317 sec/step)\n",
            "I0707 16:55:14.062889 140263261390720 learning.py:507] global step 1543: loss = 5.4561 (0.328 sec/step)\n",
            "I0707 16:55:14.362545 140263261390720 learning.py:507] global step 1544: loss = 6.0944 (0.298 sec/step)\n",
            "I0707 16:55:14.767808 140263261390720 learning.py:507] global step 1545: loss = 6.7091 (0.404 sec/step)\n",
            "I0707 16:55:15.100356 140263261390720 learning.py:507] global step 1546: loss = 3.7866 (0.331 sec/step)\n",
            "I0707 16:55:15.429274 140263261390720 learning.py:507] global step 1547: loss = 4.6845 (0.327 sec/step)\n",
            "I0707 16:55:15.728950 140263261390720 learning.py:507] global step 1548: loss = 4.5241 (0.298 sec/step)\n",
            "I0707 16:55:16.042494 140263261390720 learning.py:507] global step 1549: loss = 4.0661 (0.312 sec/step)\n",
            "I0707 16:55:16.366255 140263261390720 learning.py:507] global step 1550: loss = 5.3983 (0.322 sec/step)\n",
            "I0707 16:55:16.738095 140263261390720 learning.py:507] global step 1551: loss = 3.9252 (0.370 sec/step)\n",
            "I0707 16:55:17.051954 140263261390720 learning.py:507] global step 1552: loss = 3.4476 (0.312 sec/step)\n",
            "I0707 16:55:17.473366 140263261390720 learning.py:507] global step 1553: loss = 5.5248 (0.420 sec/step)\n",
            "I0707 16:55:17.786417 140263261390720 learning.py:507] global step 1554: loss = 5.1315 (0.311 sec/step)\n",
            "I0707 16:55:18.145773 140263261390720 learning.py:507] global step 1555: loss = 4.4881 (0.357 sec/step)\n",
            "I0707 16:55:18.504173 140263261390720 learning.py:507] global step 1556: loss = 3.4270 (0.357 sec/step)\n",
            "I0707 16:55:18.906384 140263261390720 learning.py:507] global step 1557: loss = 4.6738 (0.401 sec/step)\n",
            "I0707 16:55:19.248248 140263261390720 learning.py:507] global step 1558: loss = 3.5261 (0.340 sec/step)\n",
            "I0707 16:55:19.579071 140263261390720 learning.py:507] global step 1559: loss = 5.6235 (0.329 sec/step)\n",
            "I0707 16:55:19.881463 140263261390720 learning.py:507] global step 1560: loss = 4.5373 (0.301 sec/step)\n",
            "I0707 16:55:20.195880 140263261390720 learning.py:507] global step 1561: loss = 4.9943 (0.313 sec/step)\n",
            "I0707 16:55:20.599084 140263261390720 learning.py:507] global step 1562: loss = 4.2571 (0.402 sec/step)\n",
            "I0707 16:55:20.941495 140263261390720 learning.py:507] global step 1563: loss = 3.9301 (0.341 sec/step)\n",
            "I0707 16:55:21.260421 140263261390720 learning.py:507] global step 1564: loss = 5.3843 (0.317 sec/step)\n",
            "I0707 16:55:21.568810 140263261390720 learning.py:507] global step 1565: loss = 4.3380 (0.307 sec/step)\n",
            "I0707 16:55:21.892656 140263261390720 learning.py:507] global step 1566: loss = 5.7237 (0.322 sec/step)\n",
            "I0707 16:55:22.237888 140263261390720 learning.py:507] global step 1567: loss = 4.3658 (0.343 sec/step)\n",
            "I0707 16:55:22.551651 140263261390720 learning.py:507] global step 1568: loss = 4.8220 (0.312 sec/step)\n",
            "I0707 16:55:22.882222 140263261390720 learning.py:507] global step 1569: loss = 4.7005 (0.329 sec/step)\n",
            "I0707 16:55:23.203690 140263261390720 learning.py:507] global step 1570: loss = 4.3119 (0.320 sec/step)\n",
            "I0707 16:55:23.526638 140263261390720 learning.py:507] global step 1571: loss = 3.9838 (0.321 sec/step)\n",
            "I0707 16:55:23.810606 140263261390720 learning.py:507] global step 1572: loss = 4.1741 (0.282 sec/step)\n",
            "I0707 16:55:24.433638 140263261390720 learning.py:507] global step 1573: loss = 6.2254 (0.618 sec/step)\n",
            "I0707 16:55:24.763048 140263261390720 learning.py:507] global step 1574: loss = 4.6854 (0.327 sec/step)\n",
            "I0707 16:55:25.075760 140263261390720 learning.py:507] global step 1575: loss = 5.1104 (0.311 sec/step)\n",
            "I0707 16:55:25.405072 140263261390720 learning.py:507] global step 1576: loss = 4.5262 (0.328 sec/step)\n",
            "I0707 16:55:25.728664 140263261390720 learning.py:507] global step 1577: loss = 4.8802 (0.322 sec/step)\n",
            "I0707 16:55:26.026223 140263261390720 learning.py:507] global step 1578: loss = 4.1766 (0.296 sec/step)\n",
            "I0707 16:55:26.741407 140263261390720 learning.py:507] global step 1579: loss = 3.8562 (0.713 sec/step)\n",
            "I0707 16:55:27.031731 140263261390720 learning.py:507] global step 1580: loss = 4.5613 (0.288 sec/step)\n",
            "I0707 16:55:27.455519 140263261390720 learning.py:507] global step 1581: loss = 4.7660 (0.422 sec/step)\n",
            "I0707 16:55:27.734701 140263261390720 learning.py:507] global step 1582: loss = 5.5480 (0.277 sec/step)\n",
            "I0707 16:55:28.048328 140263261390720 learning.py:507] global step 1583: loss = 4.1230 (0.312 sec/step)\n",
            "I0707 16:55:28.375752 140263261390720 learning.py:507] global step 1584: loss = 6.3803 (0.326 sec/step)\n",
            "I0707 16:55:28.671839 140263261390720 learning.py:507] global step 1585: loss = 5.4848 (0.294 sec/step)\n",
            "I0707 16:55:28.993128 140263261390720 learning.py:507] global step 1586: loss = 4.5032 (0.320 sec/step)\n",
            "I0707 16:55:29.290817 140263261390720 learning.py:507] global step 1587: loss = 7.2151 (0.296 sec/step)\n",
            "I0707 16:55:29.615606 140263261390720 learning.py:507] global step 1588: loss = 4.6325 (0.323 sec/step)\n",
            "I0707 16:55:29.947232 140263261390720 learning.py:507] global step 1589: loss = 3.0798 (0.330 sec/step)\n",
            "I0707 16:55:30.285280 140263261390720 learning.py:507] global step 1590: loss = 3.5399 (0.336 sec/step)\n",
            "I0707 16:55:30.610952 140263261390720 learning.py:507] global step 1591: loss = 4.1981 (0.323 sec/step)\n",
            "I0707 16:55:30.955702 140263261390720 learning.py:507] global step 1592: loss = 4.6015 (0.343 sec/step)\n",
            "I0707 16:55:31.255653 140263261390720 learning.py:507] global step 1593: loss = 4.7650 (0.298 sec/step)\n",
            "I0707 16:55:31.556843 140263261390720 learning.py:507] global step 1594: loss = 4.9572 (0.300 sec/step)\n",
            "I0707 16:55:31.861899 140263261390720 learning.py:507] global step 1595: loss = 5.3023 (0.304 sec/step)\n",
            "I0707 16:55:32.230131 140263261390720 learning.py:507] global step 1596: loss = 5.3683 (0.366 sec/step)\n",
            "I0707 16:55:32.586651 140263261390720 learning.py:507] global step 1597: loss = 3.3208 (0.355 sec/step)\n",
            "I0707 16:55:32.895758 140263261390720 learning.py:507] global step 1598: loss = 4.4898 (0.307 sec/step)\n",
            "I0707 16:55:33.213627 140263261390720 learning.py:507] global step 1599: loss = 4.8429 (0.316 sec/step)\n",
            "I0707 16:55:33.514602 140263261390720 learning.py:507] global step 1600: loss = 4.2278 (0.299 sec/step)\n",
            "I0707 16:55:33.925372 140263261390720 learning.py:507] global step 1601: loss = 5.1257 (0.408 sec/step)\n",
            "I0707 16:55:34.236924 140263261390720 learning.py:507] global step 1602: loss = 3.9940 (0.310 sec/step)\n",
            "I0707 16:55:34.557814 140263261390720 learning.py:507] global step 1603: loss = 4.7578 (0.318 sec/step)\n",
            "I0707 16:55:34.887428 140263261390720 learning.py:507] global step 1604: loss = 4.8980 (0.325 sec/step)\n",
            "I0707 16:55:35.206265 140263261390720 learning.py:507] global step 1605: loss = 3.6145 (0.317 sec/step)\n",
            "I0707 16:55:35.503103 140263261390720 learning.py:507] global step 1606: loss = 4.1354 (0.295 sec/step)\n",
            "I0707 16:55:35.988731 140263261390720 learning.py:507] global step 1607: loss = 6.0987 (0.484 sec/step)\n",
            "I0707 16:55:36.275115 140263261390720 learning.py:507] global step 1608: loss = 5.4175 (0.285 sec/step)\n",
            "I0707 16:55:36.601039 140263261390720 learning.py:507] global step 1609: loss = 5.6480 (0.323 sec/step)\n",
            "I0707 16:55:36.900226 140263261390720 learning.py:507] global step 1610: loss = 3.7278 (0.296 sec/step)\n",
            "I0707 16:55:37.196376 140263261390720 learning.py:507] global step 1611: loss = 6.1791 (0.294 sec/step)\n",
            "I0707 16:55:37.521587 140263261390720 learning.py:507] global step 1612: loss = 4.1139 (0.323 sec/step)\n",
            "I0707 16:55:37.836361 140263261390720 learning.py:507] global step 1613: loss = 5.7923 (0.313 sec/step)\n",
            "I0707 16:55:38.139497 140263261390720 learning.py:507] global step 1614: loss = 4.9110 (0.301 sec/step)\n",
            "I0707 16:55:38.433742 140263261390720 learning.py:507] global step 1615: loss = 6.6649 (0.293 sec/step)\n",
            "I0707 16:55:38.796686 140263261390720 learning.py:507] global step 1616: loss = 3.9748 (0.361 sec/step)\n",
            "I0707 16:55:39.095878 140263261390720 learning.py:507] global step 1617: loss = 4.9855 (0.298 sec/step)\n",
            "I0707 16:55:39.419897 140263261390720 learning.py:507] global step 1618: loss = 4.8847 (0.322 sec/step)\n",
            "I0707 16:55:39.742080 140263261390720 learning.py:507] global step 1619: loss = 4.0429 (0.320 sec/step)\n",
            "I0707 16:55:40.038038 140263261390720 learning.py:507] global step 1620: loss = 4.6572 (0.294 sec/step)\n",
            "I0707 16:55:40.335694 140263261390720 learning.py:507] global step 1621: loss = 4.0804 (0.296 sec/step)\n",
            "I0707 16:55:40.760479 140263261390720 learning.py:507] global step 1622: loss = 4.1356 (0.423 sec/step)\n",
            "I0707 16:55:41.055311 140263261390720 learning.py:507] global step 1623: loss = 5.3917 (0.293 sec/step)\n",
            "I0707 16:55:41.364379 140263261390720 learning.py:507] global step 1624: loss = 4.4701 (0.307 sec/step)\n",
            "I0707 16:55:41.706672 140263261390720 learning.py:507] global step 1625: loss = 3.5688 (0.341 sec/step)\n",
            "I0707 16:55:42.066071 140263261390720 learning.py:507] global step 1626: loss = 6.1693 (0.358 sec/step)\n",
            "I0707 16:55:42.375859 140263261390720 learning.py:507] global step 1627: loss = 5.5000 (0.307 sec/step)\n",
            "I0707 16:55:42.787651 140263261390720 learning.py:507] global step 1628: loss = 3.3796 (0.409 sec/step)\n",
            "I0707 16:55:43.128192 140263261390720 learning.py:507] global step 1629: loss = 4.4304 (0.339 sec/step)\n",
            "I0707 16:55:43.470001 140263261390720 learning.py:507] global step 1630: loss = 4.8633 (0.340 sec/step)\n",
            "I0707 16:55:43.767486 140263261390720 learning.py:507] global step 1631: loss = 3.3276 (0.296 sec/step)\n",
            "I0707 16:55:44.137019 140263261390720 learning.py:507] global step 1632: loss = 5.5258 (0.368 sec/step)\n",
            "I0707 16:55:44.454144 140263261390720 learning.py:507] global step 1633: loss = 3.3838 (0.315 sec/step)\n",
            "I0707 16:55:44.748364 140263261390720 learning.py:507] global step 1634: loss = 4.3671 (0.292 sec/step)\n",
            "I0707 16:55:45.137475 140263261390720 learning.py:507] global step 1635: loss = 5.7933 (0.387 sec/step)\n",
            "I0707 16:55:45.452409 140263261390720 learning.py:507] global step 1636: loss = 3.0512 (0.313 sec/step)\n",
            "I0707 16:55:45.886809 140263261390720 learning.py:507] global step 1637: loss = 3.9936 (0.433 sec/step)\n",
            "I0707 16:55:46.219941 140263261390720 learning.py:507] global step 1638: loss = 4.6910 (0.331 sec/step)\n",
            "I0707 16:55:46.516465 140263261390720 learning.py:507] global step 1639: loss = 3.6130 (0.294 sec/step)\n",
            "I0707 16:55:46.840592 140263261390720 learning.py:507] global step 1640: loss = 3.2113 (0.322 sec/step)\n",
            "I0707 16:55:47.145773 140263261390720 learning.py:507] global step 1641: loss = 3.5342 (0.303 sec/step)\n",
            "I0707 16:55:47.437145 140263261390720 learning.py:507] global step 1642: loss = 3.5573 (0.290 sec/step)\n",
            "I0707 16:55:47.780788 140263261390720 learning.py:507] global step 1643: loss = 5.7062 (0.342 sec/step)\n",
            "I0707 16:55:48.100943 140263261390720 learning.py:507] global step 1644: loss = 5.1553 (0.318 sec/step)\n",
            "I0707 16:55:48.429637 140263261390720 learning.py:507] global step 1645: loss = 4.0086 (0.327 sec/step)\n",
            "I0707 16:55:48.744611 140263261390720 learning.py:507] global step 1646: loss = 3.6618 (0.313 sec/step)\n",
            "I0707 16:55:49.052441 140263261390720 learning.py:507] global step 1647: loss = 4.6674 (0.306 sec/step)\n",
            "I0707 16:55:49.346521 140263261390720 learning.py:507] global step 1648: loss = 4.0603 (0.292 sec/step)\n",
            "I0707 16:55:49.650925 140263261390720 learning.py:507] global step 1649: loss = 4.0236 (0.303 sec/step)\n",
            "I0707 16:55:49.974149 140263261390720 learning.py:507] global step 1650: loss = 4.8642 (0.322 sec/step)\n",
            "I0707 16:55:50.324145 140263261390720 learning.py:507] global step 1651: loss = 3.4137 (0.348 sec/step)\n",
            "I0707 16:55:50.678338 140263261390720 learning.py:507] global step 1652: loss = 3.5350 (0.353 sec/step)\n",
            "I0707 16:55:51.009530 140263261390720 learning.py:507] global step 1653: loss = 4.0987 (0.330 sec/step)\n",
            "I0707 16:55:51.350819 140263261390720 learning.py:507] global step 1654: loss = 5.0650 (0.340 sec/step)\n",
            "I0707 16:55:51.748131 140263261390720 learning.py:507] global step 1655: loss = 4.2393 (0.396 sec/step)\n",
            "I0707 16:55:52.105463 140263261390720 learning.py:507] global step 1656: loss = 5.3273 (0.356 sec/step)\n",
            "I0707 16:55:52.457924 140263261390720 learning.py:507] global step 1657: loss = 3.7337 (0.351 sec/step)\n",
            "I0707 16:55:52.763079 140263261390720 learning.py:507] global step 1658: loss = 5.3344 (0.304 sec/step)\n",
            "I0707 16:55:52.977923 140260304557824 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0707 16:55:54.051921 140263261390720 learning.py:507] global step 1659: loss = 4.7074 (1.282 sec/step)\n",
            "I0707 16:55:54.057359 140260287772416 supervisor.py:1050] Recording summary at step 1659.\n",
            "I0707 16:55:54.525151 140263261390720 learning.py:507] global step 1660: loss = 5.0032 (0.437 sec/step)\n",
            "I0707 16:55:55.045704 140260296165120 supervisor.py:1099] global_step/sec: 2.88637\n",
            "I0707 16:55:55.220558 140263261390720 learning.py:507] global step 1661: loss = 4.4257 (0.694 sec/step)\n",
            "I0707 16:55:55.641423 140263261390720 learning.py:507] global step 1662: loss = 5.5671 (0.419 sec/step)\n",
            "I0707 16:55:55.967893 140263261390720 learning.py:507] global step 1663: loss = 3.8680 (0.324 sec/step)\n",
            "I0707 16:55:56.379734 140263261390720 learning.py:507] global step 1664: loss = 4.7939 (0.410 sec/step)\n",
            "I0707 16:55:56.766166 140263261390720 learning.py:507] global step 1665: loss = 4.4217 (0.385 sec/step)\n",
            "I0707 16:55:57.339797 140263261390720 learning.py:507] global step 1666: loss = 4.7641 (0.572 sec/step)\n",
            "I0707 16:55:57.691073 140263261390720 learning.py:507] global step 1667: loss = 4.6404 (0.350 sec/step)\n",
            "I0707 16:55:58.033087 140263261390720 learning.py:507] global step 1668: loss = 5.7543 (0.340 sec/step)\n",
            "I0707 16:55:58.337157 140263261390720 learning.py:507] global step 1669: loss = 5.2158 (0.302 sec/step)\n",
            "I0707 16:55:58.682045 140263261390720 learning.py:507] global step 1670: loss = 4.3124 (0.343 sec/step)\n",
            "I0707 16:55:59.083960 140263261390720 learning.py:507] global step 1671: loss = 4.6007 (0.400 sec/step)\n",
            "I0707 16:55:59.531796 140263261390720 learning.py:507] global step 1672: loss = 4.0780 (0.446 sec/step)\n",
            "I0707 16:55:59.838921 140263261390720 learning.py:507] global step 1673: loss = 3.5142 (0.305 sec/step)\n",
            "I0707 16:56:00.131319 140263261390720 learning.py:507] global step 1674: loss = 4.0983 (0.291 sec/step)\n",
            "I0707 16:56:00.434684 140263261390720 learning.py:507] global step 1675: loss = 5.4646 (0.302 sec/step)\n",
            "I0707 16:56:00.767182 140263261390720 learning.py:507] global step 1676: loss = 4.1505 (0.331 sec/step)\n",
            "I0707 16:56:01.073235 140263261390720 learning.py:507] global step 1677: loss = 4.4828 (0.304 sec/step)\n",
            "I0707 16:56:01.449105 140263261390720 learning.py:507] global step 1678: loss = 4.5439 (0.367 sec/step)\n",
            "I0707 16:56:01.814803 140263261390720 learning.py:507] global step 1679: loss = 4.4115 (0.364 sec/step)\n",
            "I0707 16:56:02.120942 140263261390720 learning.py:507] global step 1680: loss = 4.1750 (0.304 sec/step)\n",
            "I0707 16:56:02.484776 140263261390720 learning.py:507] global step 1681: loss = 4.9274 (0.362 sec/step)\n",
            "I0707 16:56:02.820881 140263261390720 learning.py:507] global step 1682: loss = 5.6586 (0.335 sec/step)\n",
            "I0707 16:56:03.153635 140263261390720 learning.py:507] global step 1683: loss = 4.2752 (0.331 sec/step)\n",
            "I0707 16:56:03.503316 140263261390720 learning.py:507] global step 1684: loss = 4.3650 (0.348 sec/step)\n",
            "I0707 16:56:03.841777 140263261390720 learning.py:507] global step 1685: loss = 3.4804 (0.337 sec/step)\n",
            "I0707 16:56:04.264579 140263261390720 learning.py:507] global step 1686: loss = 3.4204 (0.421 sec/step)\n",
            "I0707 16:56:04.593617 140263261390720 learning.py:507] global step 1687: loss = 5.1172 (0.328 sec/step)\n",
            "I0707 16:56:04.915022 140263261390720 learning.py:507] global step 1688: loss = 5.1643 (0.320 sec/step)\n",
            "I0707 16:56:05.254644 140263261390720 learning.py:507] global step 1689: loss = 5.2265 (0.338 sec/step)\n",
            "I0707 16:56:05.569314 140263261390720 learning.py:507] global step 1690: loss = 4.4879 (0.313 sec/step)\n",
            "I0707 16:56:05.864001 140263261390720 learning.py:507] global step 1691: loss = 5.0068 (0.293 sec/step)\n",
            "I0707 16:56:06.378949 140263261390720 learning.py:507] global step 1692: loss = 4.6633 (0.513 sec/step)\n",
            "I0707 16:56:06.700573 140263261390720 learning.py:507] global step 1693: loss = 5.2210 (0.320 sec/step)\n",
            "I0707 16:56:07.009299 140263261390720 learning.py:507] global step 1694: loss = 3.7677 (0.307 sec/step)\n",
            "I0707 16:56:07.339978 140263261390720 learning.py:507] global step 1695: loss = 4.0041 (0.329 sec/step)\n",
            "I0707 16:56:07.670244 140263261390720 learning.py:507] global step 1696: loss = 5.3247 (0.328 sec/step)\n",
            "I0707 16:56:07.998153 140263261390720 learning.py:507] global step 1697: loss = 4.6251 (0.326 sec/step)\n",
            "I0707 16:56:08.470164 140263261390720 learning.py:507] global step 1698: loss = 4.7355 (0.470 sec/step)\n",
            "I0707 16:56:08.847575 140263261390720 learning.py:507] global step 1699: loss = 6.3538 (0.376 sec/step)\n",
            "I0707 16:56:09.141542 140263261390720 learning.py:507] global step 1700: loss = 4.8099 (0.292 sec/step)\n",
            "I0707 16:56:09.442784 140263261390720 learning.py:507] global step 1701: loss = 3.6223 (0.299 sec/step)\n",
            "I0707 16:56:09.802272 140263261390720 learning.py:507] global step 1702: loss = 3.3204 (0.358 sec/step)\n",
            "I0707 16:56:10.159779 140263261390720 learning.py:507] global step 1703: loss = 3.4677 (0.356 sec/step)\n",
            "I0707 16:56:10.495291 140263261390720 learning.py:507] global step 1704: loss = 6.0958 (0.334 sec/step)\n",
            "I0707 16:56:10.915705 140263261390720 learning.py:507] global step 1705: loss = 4.8214 (0.419 sec/step)\n",
            "I0707 16:56:11.223663 140263261390720 learning.py:507] global step 1706: loss = 4.9300 (0.306 sec/step)\n",
            "I0707 16:56:11.560980 140263261390720 learning.py:507] global step 1707: loss = 4.7173 (0.335 sec/step)\n",
            "I0707 16:56:11.895798 140263261390720 learning.py:507] global step 1708: loss = 4.3925 (0.333 sec/step)\n",
            "I0707 16:56:12.227277 140263261390720 learning.py:507] global step 1709: loss = 5.1555 (0.330 sec/step)\n",
            "I0707 16:56:12.552332 140263261390720 learning.py:507] global step 1710: loss = 4.3338 (0.323 sec/step)\n",
            "I0707 16:56:12.894215 140263261390720 learning.py:507] global step 1711: loss = 4.1740 (0.340 sec/step)\n",
            "I0707 16:56:13.220754 140263261390720 learning.py:507] global step 1712: loss = 4.1027 (0.325 sec/step)\n",
            "I0707 16:56:13.511554 140263261390720 learning.py:507] global step 1713: loss = 4.9860 (0.289 sec/step)\n",
            "I0707 16:56:13.815432 140263261390720 learning.py:507] global step 1714: loss = 4.4966 (0.302 sec/step)\n",
            "I0707 16:56:14.146880 140263261390720 learning.py:507] global step 1715: loss = 4.3458 (0.330 sec/step)\n",
            "I0707 16:56:14.467574 140263261390720 learning.py:507] global step 1716: loss = 4.4958 (0.319 sec/step)\n",
            "I0707 16:56:14.798356 140263261390720 learning.py:507] global step 1717: loss = 4.5664 (0.329 sec/step)\n",
            "I0707 16:56:15.091585 140263261390720 learning.py:507] global step 1718: loss = 4.5128 (0.292 sec/step)\n",
            "I0707 16:56:15.373564 140263261390720 learning.py:507] global step 1719: loss = 4.9094 (0.280 sec/step)\n",
            "I0707 16:56:15.664913 140263261390720 learning.py:507] global step 1720: loss = 5.2941 (0.290 sec/step)\n",
            "I0707 16:56:15.982843 140263261390720 learning.py:507] global step 1721: loss = 3.4211 (0.316 sec/step)\n",
            "I0707 16:56:16.297572 140263261390720 learning.py:507] global step 1722: loss = 3.9371 (0.313 sec/step)\n",
            "I0707 16:56:16.659943 140263261390720 learning.py:507] global step 1723: loss = 4.2636 (0.361 sec/step)\n",
            "I0707 16:56:16.966281 140263261390720 learning.py:507] global step 1724: loss = 5.0818 (0.305 sec/step)\n",
            "I0707 16:56:17.277297 140263261390720 learning.py:507] global step 1725: loss = 4.3988 (0.310 sec/step)\n",
            "I0707 16:56:17.588071 140263261390720 learning.py:507] global step 1726: loss = 5.3008 (0.309 sec/step)\n",
            "I0707 16:56:17.921839 140263261390720 learning.py:507] global step 1727: loss = 3.8577 (0.332 sec/step)\n",
            "I0707 16:56:18.230766 140263261390720 learning.py:507] global step 1728: loss = 4.0620 (0.307 sec/step)\n",
            "I0707 16:56:18.532016 140263261390720 learning.py:507] global step 1729: loss = 4.2174 (0.299 sec/step)\n",
            "I0707 16:56:18.845498 140263261390720 learning.py:507] global step 1730: loss = 4.6286 (0.312 sec/step)\n",
            "I0707 16:56:19.203298 140263261390720 learning.py:507] global step 1731: loss = 3.7694 (0.356 sec/step)\n",
            "I0707 16:56:19.513046 140263261390720 learning.py:507] global step 1732: loss = 4.1485 (0.308 sec/step)\n",
            "I0707 16:56:19.829291 140263261390720 learning.py:507] global step 1733: loss = 3.9917 (0.315 sec/step)\n",
            "I0707 16:56:20.148942 140263261390720 learning.py:507] global step 1734: loss = 4.3841 (0.318 sec/step)\n",
            "I0707 16:56:20.459808 140263261390720 learning.py:507] global step 1735: loss = 5.9178 (0.309 sec/step)\n",
            "I0707 16:56:20.756163 140263261390720 learning.py:507] global step 1736: loss = 5.7169 (0.295 sec/step)\n",
            "I0707 16:56:21.126525 140263261390720 learning.py:507] global step 1737: loss = 4.0551 (0.369 sec/step)\n",
            "I0707 16:56:21.450567 140263261390720 learning.py:507] global step 1738: loss = 5.9987 (0.322 sec/step)\n",
            "I0707 16:56:21.811453 140263261390720 learning.py:507] global step 1739: loss = 5.2235 (0.359 sec/step)\n",
            "I0707 16:56:22.163547 140263261390720 learning.py:507] global step 1740: loss = 3.8480 (0.350 sec/step)\n",
            "I0707 16:56:22.492518 140263261390720 learning.py:507] global step 1741: loss = 3.8402 (0.327 sec/step)\n",
            "I0707 16:56:22.781495 140263261390720 learning.py:507] global step 1742: loss = 5.3168 (0.287 sec/step)\n",
            "I0707 16:56:23.075100 140263261390720 learning.py:507] global step 1743: loss = 4.6717 (0.291 sec/step)\n",
            "I0707 16:56:23.400345 140263261390720 learning.py:507] global step 1744: loss = 4.5491 (0.324 sec/step)\n",
            "I0707 16:56:23.693286 140263261390720 learning.py:507] global step 1745: loss = 4.7130 (0.291 sec/step)\n",
            "I0707 16:56:24.005567 140263261390720 learning.py:507] global step 1746: loss = 5.4184 (0.311 sec/step)\n",
            "I0707 16:56:24.332736 140263261390720 learning.py:507] global step 1747: loss = 3.6653 (0.325 sec/step)\n",
            "I0707 16:56:24.627924 140263261390720 learning.py:507] global step 1748: loss = 5.9129 (0.293 sec/step)\n",
            "I0707 16:56:24.910646 140263261390720 learning.py:507] global step 1749: loss = 5.0446 (0.281 sec/step)\n",
            "I0707 16:56:25.253915 140263261390720 learning.py:507] global step 1750: loss = 4.3413 (0.342 sec/step)\n",
            "I0707 16:56:25.537078 140263261390720 learning.py:507] global step 1751: loss = 4.8945 (0.281 sec/step)\n",
            "I0707 16:56:25.848189 140263261390720 learning.py:507] global step 1752: loss = 6.3013 (0.309 sec/step)\n",
            "I0707 16:56:26.160533 140263261390720 learning.py:507] global step 1753: loss = 4.0853 (0.310 sec/step)\n",
            "I0707 16:56:26.484250 140263261390720 learning.py:507] global step 1754: loss = 3.8622 (0.322 sec/step)\n",
            "I0707 16:56:26.793768 140263261390720 learning.py:507] global step 1755: loss = 3.7935 (0.308 sec/step)\n",
            "I0707 16:56:27.085199 140263261390720 learning.py:507] global step 1756: loss = 4.1487 (0.290 sec/step)\n",
            "I0707 16:56:27.405862 140263261390720 learning.py:507] global step 1757: loss = 5.6077 (0.319 sec/step)\n",
            "I0707 16:56:27.789285 140263261390720 learning.py:507] global step 1758: loss = 3.9884 (0.382 sec/step)\n",
            "I0707 16:56:28.124841 140263261390720 learning.py:507] global step 1759: loss = 3.9262 (0.334 sec/step)\n",
            "I0707 16:56:28.498552 140263261390720 learning.py:507] global step 1760: loss = 4.8277 (0.372 sec/step)\n",
            "I0707 16:56:28.801485 140263261390720 learning.py:507] global step 1761: loss = 4.1247 (0.301 sec/step)\n",
            "I0707 16:56:29.121355 140263261390720 learning.py:507] global step 1762: loss = 5.6072 (0.318 sec/step)\n",
            "I0707 16:56:29.451971 140263261390720 learning.py:507] global step 1763: loss = 6.4838 (0.329 sec/step)\n",
            "I0707 16:56:29.896343 140263261390720 learning.py:507] global step 1764: loss = 5.1815 (0.443 sec/step)\n",
            "I0707 16:56:30.249520 140263261390720 learning.py:507] global step 1765: loss = 3.6069 (0.351 sec/step)\n",
            "I0707 16:56:30.595081 140263261390720 learning.py:507] global step 1766: loss = 3.9740 (0.344 sec/step)\n",
            "I0707 16:56:30.898866 140263261390720 learning.py:507] global step 1767: loss = 5.7098 (0.302 sec/step)\n",
            "I0707 16:56:31.215984 140263261390720 learning.py:507] global step 1768: loss = 5.8632 (0.315 sec/step)\n",
            "I0707 16:56:31.516806 140263261390720 learning.py:507] global step 1769: loss = 4.2924 (0.299 sec/step)\n",
            "I0707 16:56:31.858460 140263261390720 learning.py:507] global step 1770: loss = 3.0293 (0.340 sec/step)\n",
            "I0707 16:56:32.172158 140263261390720 learning.py:507] global step 1771: loss = 4.5144 (0.312 sec/step)\n",
            "I0707 16:56:32.519553 140263261390720 learning.py:507] global step 1772: loss = 3.9565 (0.346 sec/step)\n",
            "I0707 16:56:32.949109 140263261390720 learning.py:507] global step 1773: loss = 4.9856 (0.428 sec/step)\n",
            "I0707 16:56:33.333528 140263261390720 learning.py:507] global step 1774: loss = 4.6755 (0.383 sec/step)\n",
            "I0707 16:56:33.662939 140263261390720 learning.py:507] global step 1775: loss = 3.0911 (0.328 sec/step)\n",
            "I0707 16:56:33.994334 140263261390720 learning.py:507] global step 1776: loss = 4.5638 (0.330 sec/step)\n",
            "I0707 16:56:34.317230 140263261390720 learning.py:507] global step 1777: loss = 3.0880 (0.321 sec/step)\n",
            "I0707 16:56:34.675138 140263261390720 learning.py:507] global step 1778: loss = 3.3521 (0.356 sec/step)\n",
            "I0707 16:56:35.008940 140263261390720 learning.py:507] global step 1779: loss = 5.4659 (0.332 sec/step)\n",
            "I0707 16:56:35.475201 140263261390720 learning.py:507] global step 1780: loss = 5.4911 (0.465 sec/step)\n",
            "I0707 16:56:35.853978 140263261390720 learning.py:507] global step 1781: loss = 4.2414 (0.377 sec/step)\n",
            "I0707 16:56:36.163617 140263261390720 learning.py:507] global step 1782: loss = 5.3504 (0.308 sec/step)\n",
            "I0707 16:56:36.477231 140263261390720 learning.py:507] global step 1783: loss = 3.9490 (0.312 sec/step)\n",
            "I0707 16:56:36.831689 140263261390720 learning.py:507] global step 1784: loss = 5.7541 (0.353 sec/step)\n",
            "I0707 16:56:37.180525 140263261390720 learning.py:507] global step 1785: loss = 4.0013 (0.347 sec/step)\n",
            "I0707 16:56:37.556621 140263261390720 learning.py:507] global step 1786: loss = 4.1014 (0.374 sec/step)\n",
            "I0707 16:56:37.867188 140263261390720 learning.py:507] global step 1787: loss = 4.2600 (0.309 sec/step)\n",
            "I0707 16:56:38.176110 140263261390720 learning.py:507] global step 1788: loss = 4.8992 (0.307 sec/step)\n",
            "I0707 16:56:38.493536 140263261390720 learning.py:507] global step 1789: loss = 4.0303 (0.316 sec/step)\n",
            "I0707 16:56:38.791134 140263261390720 learning.py:507] global step 1790: loss = 4.0448 (0.296 sec/step)\n",
            "I0707 16:56:39.151841 140263261390720 learning.py:507] global step 1791: loss = 4.7682 (0.359 sec/step)\n",
            "I0707 16:56:39.497427 140263261390720 learning.py:507] global step 1792: loss = 4.0508 (0.343 sec/step)\n",
            "I0707 16:56:39.903188 140263261390720 learning.py:507] global step 1793: loss = 4.5947 (0.403 sec/step)\n",
            "I0707 16:56:40.223709 140263261390720 learning.py:507] global step 1794: loss = 4.9081 (0.319 sec/step)\n",
            "I0707 16:56:40.674249 140263261390720 learning.py:507] global step 1795: loss = 3.2489 (0.449 sec/step)\n",
            "I0707 16:56:41.009355 140263261390720 learning.py:507] global step 1796: loss = 5.4053 (0.333 sec/step)\n",
            "I0707 16:56:41.303068 140263261390720 learning.py:507] global step 1797: loss = 5.0313 (0.292 sec/step)\n",
            "I0707 16:56:41.643838 140263261390720 learning.py:507] global step 1798: loss = 4.9680 (0.339 sec/step)\n",
            "I0707 16:56:42.069942 140263261390720 learning.py:507] global step 1799: loss = 5.1033 (0.424 sec/step)\n",
            "I0707 16:56:42.381800 140263261390720 learning.py:507] global step 1800: loss = 3.5673 (0.310 sec/step)\n",
            "I0707 16:56:42.851484 140263261390720 learning.py:507] global step 1801: loss = 3.3564 (0.468 sec/step)\n",
            "I0707 16:56:43.176984 140263261390720 learning.py:507] global step 1802: loss = 5.2849 (0.324 sec/step)\n",
            "I0707 16:56:43.541172 140263261390720 learning.py:507] global step 1803: loss = 4.1233 (0.362 sec/step)\n",
            "I0707 16:56:43.875198 140263261390720 learning.py:507] global step 1804: loss = 3.3671 (0.332 sec/step)\n",
            "I0707 16:56:44.199920 140263261390720 learning.py:507] global step 1805: loss = 3.9446 (0.323 sec/step)\n",
            "I0707 16:56:44.502998 140263261390720 learning.py:507] global step 1806: loss = 5.0944 (0.302 sec/step)\n",
            "I0707 16:56:45.021847 140263261390720 learning.py:507] global step 1807: loss = 5.5181 (0.517 sec/step)\n",
            "I0707 16:56:45.311973 140263261390720 learning.py:507] global step 1808: loss = 6.2348 (0.288 sec/step)\n",
            "I0707 16:56:45.641159 140263261390720 learning.py:507] global step 1809: loss = 4.6241 (0.328 sec/step)\n",
            "I0707 16:56:45.955391 140263261390720 learning.py:507] global step 1810: loss = 4.1338 (0.313 sec/step)\n",
            "I0707 16:56:46.271889 140263261390720 learning.py:507] global step 1811: loss = 4.3910 (0.315 sec/step)\n",
            "I0707 16:56:46.581981 140263261390720 learning.py:507] global step 1812: loss = 3.4095 (0.308 sec/step)\n",
            "I0707 16:56:46.945755 140263261390720 learning.py:507] global step 1813: loss = 6.0499 (0.362 sec/step)\n",
            "I0707 16:56:47.260087 140263261390720 learning.py:507] global step 1814: loss = 3.7421 (0.313 sec/step)\n",
            "I0707 16:56:47.813376 140263261390720 learning.py:507] global step 1815: loss = 4.1746 (0.551 sec/step)\n",
            "I0707 16:56:48.143162 140263261390720 learning.py:507] global step 1816: loss = 5.9252 (0.328 sec/step)\n",
            "I0707 16:56:48.439886 140263261390720 learning.py:507] global step 1817: loss = 5.1758 (0.295 sec/step)\n",
            "I0707 16:56:48.799374 140263261390720 learning.py:507] global step 1818: loss = 4.2147 (0.358 sec/step)\n",
            "I0707 16:56:49.135505 140263261390720 learning.py:507] global step 1819: loss = 4.3982 (0.334 sec/step)\n",
            "I0707 16:56:49.440063 140263261390720 learning.py:507] global step 1820: loss = 5.2153 (0.303 sec/step)\n",
            "I0707 16:56:49.765494 140263261390720 learning.py:507] global step 1821: loss = 6.5489 (0.324 sec/step)\n",
            "I0707 16:56:50.135068 140263261390720 learning.py:507] global step 1822: loss = 3.2231 (0.367 sec/step)\n",
            "I0707 16:56:50.600591 140263261390720 learning.py:507] global step 1823: loss = 4.6848 (0.464 sec/step)\n",
            "I0707 16:56:51.113644 140263261390720 learning.py:507] global step 1824: loss = 4.4989 (0.512 sec/step)\n",
            "I0707 16:56:51.416803 140263261390720 learning.py:507] global step 1825: loss = 3.1210 (0.300 sec/step)\n",
            "I0707 16:56:51.831356 140263261390720 learning.py:507] global step 1826: loss = 4.0835 (0.413 sec/step)\n",
            "I0707 16:56:52.140303 140263261390720 learning.py:507] global step 1827: loss = 3.7355 (0.307 sec/step)\n",
            "I0707 16:56:52.444694 140263261390720 learning.py:507] global step 1828: loss = 4.6056 (0.303 sec/step)\n",
            "I0707 16:56:52.964892 140263261390720 learning.py:507] global step 1829: loss = 6.9092 (0.518 sec/step)\n",
            "I0707 16:56:53.303943 140263261390720 learning.py:507] global step 1830: loss = 3.8126 (0.337 sec/step)\n",
            "I0707 16:56:53.632356 140263261390720 learning.py:507] global step 1831: loss = 4.0860 (0.326 sec/step)\n",
            "I0707 16:56:54.073333 140263261390720 learning.py:507] global step 1832: loss = 6.1866 (0.439 sec/step)\n",
            "I0707 16:56:54.381517 140263261390720 learning.py:507] global step 1833: loss = 4.0075 (0.306 sec/step)\n",
            "I0707 16:56:54.674612 140263261390720 learning.py:507] global step 1834: loss = 3.2664 (0.291 sec/step)\n",
            "I0707 16:56:54.988532 140263261390720 learning.py:507] global step 1835: loss = 5.2500 (0.312 sec/step)\n",
            "I0707 16:56:55.358695 140263261390720 learning.py:507] global step 1836: loss = 5.2054 (0.368 sec/step)\n",
            "I0707 16:56:55.667011 140263261390720 learning.py:507] global step 1837: loss = 4.6884 (0.307 sec/step)\n",
            "I0707 16:56:55.967319 140263261390720 learning.py:507] global step 1838: loss = 4.7279 (0.299 sec/step)\n",
            "I0707 16:56:56.377697 140263261390720 learning.py:507] global step 1839: loss = 4.6507 (0.409 sec/step)\n",
            "I0707 16:56:56.701915 140263261390720 learning.py:507] global step 1840: loss = 4.5753 (0.322 sec/step)\n",
            "I0707 16:56:57.003668 140263261390720 learning.py:507] global step 1841: loss = 5.6804 (0.300 sec/step)\n",
            "I0707 16:56:57.327001 140263261390720 learning.py:507] global step 1842: loss = 5.0748 (0.322 sec/step)\n",
            "I0707 16:56:57.687765 140263261390720 learning.py:507] global step 1843: loss = 3.1731 (0.359 sec/step)\n",
            "I0707 16:56:58.141962 140263261390720 learning.py:507] global step 1844: loss = 4.5917 (0.452 sec/step)\n",
            "I0707 16:56:58.481687 140263261390720 learning.py:507] global step 1845: loss = 3.5294 (0.338 sec/step)\n",
            "I0707 16:56:58.804500 140263261390720 learning.py:507] global step 1846: loss = 4.1434 (0.321 sec/step)\n",
            "I0707 16:56:59.188779 140263261390720 learning.py:507] global step 1847: loss = 4.7850 (0.383 sec/step)\n",
            "I0707 16:56:59.481283 140263261390720 learning.py:507] global step 1848: loss = 6.1297 (0.291 sec/step)\n",
            "I0707 16:56:59.923459 140263261390720 learning.py:507] global step 1849: loss = 4.3011 (0.441 sec/step)\n",
            "I0707 16:57:00.454810 140263261390720 learning.py:507] global step 1850: loss = 4.0772 (0.530 sec/step)\n",
            "I0707 16:57:00.770661 140263261390720 learning.py:507] global step 1851: loss = 3.4646 (0.314 sec/step)\n",
            "I0707 16:57:01.272731 140263261390720 learning.py:507] global step 1852: loss = 6.1233 (0.500 sec/step)\n",
            "I0707 16:57:01.569948 140263261390720 learning.py:507] global step 1853: loss = 3.4382 (0.296 sec/step)\n",
            "I0707 16:57:01.853576 140263261390720 learning.py:507] global step 1854: loss = 4.2331 (0.282 sec/step)\n",
            "I0707 16:57:02.246622 140263261390720 learning.py:507] global step 1855: loss = 8.9985 (0.391 sec/step)\n",
            "I0707 16:57:02.592207 140263261390720 learning.py:507] global step 1856: loss = 5.5591 (0.344 sec/step)\n",
            "I0707 16:57:02.892552 140263261390720 learning.py:507] global step 1857: loss = 4.6565 (0.299 sec/step)\n",
            "I0707 16:57:03.468608 140263261390720 learning.py:507] global step 1858: loss = 4.6565 (0.574 sec/step)\n",
            "I0707 16:57:03.812088 140263261390720 learning.py:507] global step 1859: loss = 4.2071 (0.342 sec/step)\n",
            "I0707 16:57:04.134295 140263261390720 learning.py:507] global step 1860: loss = 5.5180 (0.320 sec/step)\n",
            "I0707 16:57:04.443973 140263261390720 learning.py:507] global step 1861: loss = 4.0700 (0.308 sec/step)\n",
            "I0707 16:57:04.777608 140263261390720 learning.py:507] global step 1862: loss = 4.6879 (0.332 sec/step)\n",
            "I0707 16:57:05.072467 140263261390720 learning.py:507] global step 1863: loss = 3.8603 (0.293 sec/step)\n",
            "I0707 16:57:05.437166 140263261390720 learning.py:507] global step 1864: loss = 5.7169 (0.363 sec/step)\n",
            "I0707 16:57:05.798800 140263261390720 learning.py:507] global step 1865: loss = 5.9952 (0.360 sec/step)\n",
            "I0707 16:57:06.104454 140263261390720 learning.py:507] global step 1866: loss = 3.4845 (0.304 sec/step)\n",
            "I0707 16:57:06.604186 140263261390720 learning.py:507] global step 1867: loss = 4.2459 (0.498 sec/step)\n",
            "I0707 16:57:06.917627 140263261390720 learning.py:507] global step 1868: loss = 4.7534 (0.312 sec/step)\n",
            "I0707 16:57:07.258064 140263261390720 learning.py:507] global step 1869: loss = 4.1693 (0.339 sec/step)\n",
            "I0707 16:57:07.642219 140263261390720 learning.py:507] global step 1870: loss = 4.1383 (0.383 sec/step)\n",
            "I0707 16:57:07.934638 140263261390720 learning.py:507] global step 1871: loss = 4.2147 (0.291 sec/step)\n",
            "I0707 16:57:08.263923 140263261390720 learning.py:507] global step 1872: loss = 4.2754 (0.327 sec/step)\n",
            "I0707 16:57:08.866280 140263261390720 learning.py:507] global step 1873: loss = 4.7374 (0.601 sec/step)\n",
            "I0707 16:57:09.157934 140263261390720 learning.py:507] global step 1874: loss = 4.2481 (0.290 sec/step)\n",
            "I0707 16:57:09.499319 140263261390720 learning.py:507] global step 1875: loss = 4.1687 (0.340 sec/step)\n",
            "I0707 16:57:09.869684 140263261390720 learning.py:507] global step 1876: loss = 4.1475 (0.369 sec/step)\n",
            "I0707 16:57:10.181031 140263261390720 learning.py:507] global step 1877: loss = 3.7826 (0.310 sec/step)\n",
            "I0707 16:57:10.496285 140263261390720 learning.py:507] global step 1878: loss = 3.8312 (0.314 sec/step)\n",
            "I0707 16:57:10.806734 140263261390720 learning.py:507] global step 1879: loss = 5.1294 (0.309 sec/step)\n",
            "I0707 16:57:11.115255 140263261390720 learning.py:507] global step 1880: loss = 3.0438 (0.307 sec/step)\n",
            "I0707 16:57:11.427579 140263261390720 learning.py:507] global step 1881: loss = 4.4731 (0.311 sec/step)\n",
            "I0707 16:57:11.722755 140263261390720 learning.py:507] global step 1882: loss = 4.2109 (0.293 sec/step)\n",
            "I0707 16:57:12.010622 140263261390720 learning.py:507] global step 1883: loss = 2.9220 (0.286 sec/step)\n",
            "I0707 16:57:12.322129 140263261390720 learning.py:507] global step 1884: loss = 5.5981 (0.310 sec/step)\n",
            "I0707 16:57:12.650505 140263261390720 learning.py:507] global step 1885: loss = 4.5540 (0.327 sec/step)\n",
            "I0707 16:57:12.972509 140263261390720 learning.py:507] global step 1886: loss = 3.3760 (0.320 sec/step)\n",
            "I0707 16:57:13.274594 140263261390720 learning.py:507] global step 1887: loss = 3.8864 (0.300 sec/step)\n",
            "I0707 16:57:13.572549 140263261390720 learning.py:507] global step 1888: loss = 4.3254 (0.296 sec/step)\n",
            "I0707 16:57:13.886318 140263261390720 learning.py:507] global step 1889: loss = 5.4924 (0.312 sec/step)\n",
            "I0707 16:57:14.201032 140263261390720 learning.py:507] global step 1890: loss = 4.6386 (0.313 sec/step)\n",
            "I0707 16:57:14.520284 140263261390720 learning.py:507] global step 1891: loss = 5.0053 (0.318 sec/step)\n",
            "I0707 16:57:14.849991 140263261390720 learning.py:507] global step 1892: loss = 4.8094 (0.328 sec/step)\n",
            "I0707 16:57:15.180158 140263261390720 learning.py:507] global step 1893: loss = 4.7054 (0.328 sec/step)\n",
            "I0707 16:57:15.499013 140263261390720 learning.py:507] global step 1894: loss = 4.1828 (0.317 sec/step)\n",
            "I0707 16:57:15.870198 140263261390720 learning.py:507] global step 1895: loss = 4.7437 (0.369 sec/step)\n",
            "I0707 16:57:16.186497 140263261390720 learning.py:507] global step 1896: loss = 4.6106 (0.315 sec/step)\n",
            "I0707 16:57:16.496424 140263261390720 learning.py:507] global step 1897: loss = 4.8356 (0.308 sec/step)\n",
            "I0707 16:57:16.833878 140263261390720 learning.py:507] global step 1898: loss = 4.0003 (0.336 sec/step)\n",
            "I0707 16:57:17.344312 140263261390720 learning.py:507] global step 1899: loss = 4.5708 (0.509 sec/step)\n",
            "I0707 16:57:17.670628 140263261390720 learning.py:507] global step 1900: loss = 4.0327 (0.325 sec/step)\n",
            "I0707 16:57:18.018375 140263261390720 learning.py:507] global step 1901: loss = 4.2629 (0.346 sec/step)\n",
            "I0707 16:57:18.319599 140263261390720 learning.py:507] global step 1902: loss = 4.8834 (0.299 sec/step)\n",
            "I0707 16:57:18.621042 140263261390720 learning.py:507] global step 1903: loss = 4.7995 (0.300 sec/step)\n",
            "I0707 16:57:18.965801 140263261390720 learning.py:507] global step 1904: loss = 5.8028 (0.343 sec/step)\n",
            "I0707 16:57:19.293913 140263261390720 learning.py:507] global step 1905: loss = 4.1972 (0.327 sec/step)\n",
            "I0707 16:57:19.626849 140263261390720 learning.py:507] global step 1906: loss = 4.0956 (0.331 sec/step)\n",
            "I0707 16:57:20.053091 140263261390720 learning.py:507] global step 1907: loss = 4.6205 (0.425 sec/step)\n",
            "I0707 16:57:20.354589 140263261390720 learning.py:507] global step 1908: loss = 3.4970 (0.300 sec/step)\n",
            "I0707 16:57:20.680437 140263261390720 learning.py:507] global step 1909: loss = 5.7027 (0.324 sec/step)\n",
            "I0707 16:57:21.005078 140263261390720 learning.py:507] global step 1910: loss = 3.6639 (0.323 sec/step)\n",
            "I0707 16:57:21.297701 140263261390720 learning.py:507] global step 1911: loss = 3.8615 (0.291 sec/step)\n",
            "I0707 16:57:21.586494 140263261390720 learning.py:507] global step 1912: loss = 3.6800 (0.287 sec/step)\n",
            "I0707 16:57:21.924359 140263261390720 learning.py:507] global step 1913: loss = 3.9382 (0.336 sec/step)\n",
            "I0707 16:57:22.250712 140263261390720 learning.py:507] global step 1914: loss = 6.4250 (0.325 sec/step)\n",
            "I0707 16:57:22.564328 140263261390720 learning.py:507] global step 1915: loss = 4.0311 (0.312 sec/step)\n",
            "I0707 16:57:22.879193 140263261390720 learning.py:507] global step 1916: loss = 4.4374 (0.313 sec/step)\n",
            "I0707 16:57:23.226497 140263261390720 learning.py:507] global step 1917: loss = 4.5009 (0.345 sec/step)\n",
            "I0707 16:57:23.555304 140263261390720 learning.py:507] global step 1918: loss = 5.3351 (0.327 sec/step)\n",
            "I0707 16:57:23.854176 140263261390720 learning.py:507] global step 1919: loss = 4.7061 (0.297 sec/step)\n",
            "I0707 16:57:24.182003 140263261390720 learning.py:507] global step 1920: loss = 4.9419 (0.326 sec/step)\n",
            "I0707 16:57:24.509180 140263261390720 learning.py:507] global step 1921: loss = 4.2822 (0.326 sec/step)\n",
            "I0707 16:57:24.814262 140263261390720 learning.py:507] global step 1922: loss = 6.0503 (0.303 sec/step)\n",
            "I0707 16:57:25.158962 140263261390720 learning.py:507] global step 1923: loss = 3.7907 (0.343 sec/step)\n",
            "I0707 16:57:25.506231 140263261390720 learning.py:507] global step 1924: loss = 5.0490 (0.345 sec/step)\n",
            "I0707 16:57:25.833585 140263261390720 learning.py:507] global step 1925: loss = 5.3203 (0.326 sec/step)\n",
            "I0707 16:57:26.157836 140263261390720 learning.py:507] global step 1926: loss = 4.7846 (0.322 sec/step)\n",
            "I0707 16:57:26.717177 140263261390720 learning.py:507] global step 1927: loss = 5.2542 (0.558 sec/step)\n",
            "I0707 16:57:27.029625 140263261390720 learning.py:507] global step 1928: loss = 5.5183 (0.311 sec/step)\n",
            "I0707 16:57:27.435675 140263261390720 learning.py:507] global step 1929: loss = 3.9134 (0.404 sec/step)\n",
            "I0707 16:57:27.755676 140263261390720 learning.py:507] global step 1930: loss = 3.9950 (0.318 sec/step)\n",
            "I0707 16:57:28.104760 140263261390720 learning.py:507] global step 1931: loss = 4.7432 (0.347 sec/step)\n",
            "I0707 16:57:28.438096 140263261390720 learning.py:507] global step 1932: loss = 3.7619 (0.332 sec/step)\n",
            "I0707 16:57:29.195142 140263261390720 learning.py:507] global step 1933: loss = 3.8573 (0.756 sec/step)\n",
            "I0707 16:57:29.483036 140263261390720 learning.py:507] global step 1934: loss = 5.4585 (0.286 sec/step)\n",
            "I0707 16:57:29.786912 140263261390720 learning.py:507] global step 1935: loss = 3.5832 (0.302 sec/step)\n",
            "I0707 16:57:30.118727 140263261390720 learning.py:507] global step 1936: loss = 3.5685 (0.330 sec/step)\n",
            "I0707 16:57:30.439511 140263261390720 learning.py:507] global step 1937: loss = 4.7317 (0.319 sec/step)\n",
            "I0707 16:57:30.817326 140263261390720 learning.py:507] global step 1938: loss = 3.8061 (0.376 sec/step)\n",
            "I0707 16:57:31.188447 140263261390720 learning.py:507] global step 1939: loss = 4.8839 (0.369 sec/step)\n",
            "I0707 16:57:31.488148 140263261390720 learning.py:507] global step 1940: loss = 4.4166 (0.298 sec/step)\n",
            "I0707 16:57:31.799682 140263261390720 learning.py:507] global step 1941: loss = 5.6936 (0.310 sec/step)\n",
            "I0707 16:57:32.123011 140263261390720 learning.py:507] global step 1942: loss = 4.8946 (0.322 sec/step)\n",
            "I0707 16:57:32.469486 140263261390720 learning.py:507] global step 1943: loss = 3.8408 (0.345 sec/step)\n",
            "I0707 16:57:32.808360 140263261390720 learning.py:507] global step 1944: loss = 3.5119 (0.337 sec/step)\n",
            "I0707 16:57:33.107765 140263261390720 learning.py:507] global step 1945: loss = 3.6983 (0.298 sec/step)\n",
            "I0707 16:57:33.435755 140263261390720 learning.py:507] global step 1946: loss = 4.8305 (0.326 sec/step)\n",
            "I0707 16:57:33.732571 140263261390720 learning.py:507] global step 1947: loss = 4.3471 (0.295 sec/step)\n",
            "I0707 16:57:34.038162 140263261390720 learning.py:507] global step 1948: loss = 5.4991 (0.303 sec/step)\n",
            "I0707 16:57:34.391031 140263261390720 learning.py:507] global step 1949: loss = 5.5953 (0.351 sec/step)\n",
            "I0707 16:57:34.689726 140263261390720 learning.py:507] global step 1950: loss = 5.4017 (0.297 sec/step)\n",
            "I0707 16:57:34.989306 140263261390720 learning.py:507] global step 1951: loss = 4.6494 (0.298 sec/step)\n",
            "I0707 16:57:35.338553 140263261390720 learning.py:507] global step 1952: loss = 3.6461 (0.348 sec/step)\n",
            "I0707 16:57:35.653148 140263261390720 learning.py:507] global step 1953: loss = 4.5711 (0.313 sec/step)\n",
            "I0707 16:57:35.986567 140263261390720 learning.py:507] global step 1954: loss = 4.9495 (0.332 sec/step)\n",
            "I0707 16:57:36.324287 140263261390720 learning.py:507] global step 1955: loss = 4.3608 (0.336 sec/step)\n",
            "I0707 16:57:36.664307 140263261390720 learning.py:507] global step 1956: loss = 3.7297 (0.338 sec/step)\n",
            "I0707 16:57:37.264358 140263261390720 learning.py:507] global step 1957: loss = 3.5339 (0.598 sec/step)\n",
            "I0707 16:57:37.632203 140263261390720 learning.py:507] global step 1958: loss = 3.6834 (0.366 sec/step)\n",
            "I0707 16:57:38.074470 140263261390720 learning.py:507] global step 1959: loss = 3.0706 (0.441 sec/step)\n",
            "I0707 16:57:38.402856 140263261390720 learning.py:507] global step 1960: loss = 4.7471 (0.327 sec/step)\n",
            "I0707 16:57:38.731900 140263261390720 learning.py:507] global step 1961: loss = 3.7344 (0.327 sec/step)\n",
            "I0707 16:57:39.046924 140263261390720 learning.py:507] global step 1962: loss = 3.6561 (0.313 sec/step)\n",
            "I0707 16:57:39.869839 140263261390720 learning.py:507] global step 1963: loss = 4.1718 (0.821 sec/step)\n",
            "I0707 16:57:40.194909 140263261390720 learning.py:507] global step 1964: loss = 4.7545 (0.324 sec/step)\n",
            "I0707 16:57:40.617203 140263261390720 learning.py:507] global step 1965: loss = 4.2768 (0.421 sec/step)\n",
            "I0707 16:57:40.920279 140263261390720 learning.py:507] global step 1966: loss = 5.5531 (0.301 sec/step)\n",
            "I0707 16:57:41.221910 140263261390720 learning.py:507] global step 1967: loss = 4.6739 (0.300 sec/step)\n",
            "I0707 16:57:41.663235 140263261390720 learning.py:507] global step 1968: loss = 3.1639 (0.440 sec/step)\n",
            "I0707 16:57:42.006988 140263261390720 learning.py:507] global step 1969: loss = 3.0957 (0.342 sec/step)\n",
            "I0707 16:57:42.314458 140263261390720 learning.py:507] global step 1970: loss = 3.8391 (0.306 sec/step)\n",
            "I0707 16:57:42.740790 140263261390720 learning.py:507] global step 1971: loss = 4.5369 (0.425 sec/step)\n",
            "I0707 16:57:43.111695 140263261390720 learning.py:507] global step 1972: loss = 4.5966 (0.368 sec/step)\n",
            "I0707 16:57:43.421723 140263261390720 learning.py:507] global step 1973: loss = 3.1130 (0.308 sec/step)\n",
            "I0707 16:57:43.714211 140263261390720 learning.py:507] global step 1974: loss = 4.2986 (0.291 sec/step)\n",
            "I0707 16:57:44.024071 140263261390720 learning.py:507] global step 1975: loss = 3.7200 (0.308 sec/step)\n",
            "I0707 16:57:44.338255 140263261390720 learning.py:507] global step 1976: loss = 5.0400 (0.313 sec/step)\n",
            "I0707 16:57:44.713854 140263261390720 learning.py:507] global step 1977: loss = 5.2168 (0.374 sec/step)\n",
            "I0707 16:57:45.146866 140263261390720 learning.py:507] global step 1978: loss = 5.0900 (0.431 sec/step)\n",
            "I0707 16:57:45.504010 140263261390720 learning.py:507] global step 1979: loss = 2.4612 (0.356 sec/step)\n",
            "I0707 16:57:45.812798 140263261390720 learning.py:507] global step 1980: loss = 4.8965 (0.307 sec/step)\n",
            "I0707 16:57:46.140691 140263261390720 learning.py:507] global step 1981: loss = 4.9066 (0.326 sec/step)\n",
            "I0707 16:57:46.458138 140263261390720 learning.py:507] global step 1982: loss = 5.0623 (0.316 sec/step)\n",
            "I0707 16:57:46.776190 140263261390720 learning.py:507] global step 1983: loss = 4.1617 (0.316 sec/step)\n",
            "I0707 16:57:47.119530 140263261390720 learning.py:507] global step 1984: loss = 4.1145 (0.341 sec/step)\n",
            "I0707 16:57:47.496204 140263261390720 learning.py:507] global step 1985: loss = 4.3127 (0.375 sec/step)\n",
            "I0707 16:57:47.810698 140263261390720 learning.py:507] global step 1986: loss = 5.1535 (0.312 sec/step)\n",
            "I0707 16:57:48.186979 140263261390720 learning.py:507] global step 1987: loss = 4.0532 (0.375 sec/step)\n",
            "I0707 16:57:48.515921 140263261390720 learning.py:507] global step 1988: loss = 4.9944 (0.327 sec/step)\n",
            "I0707 16:57:48.863495 140263261390720 learning.py:507] global step 1989: loss = 6.7800 (0.346 sec/step)\n",
            "I0707 16:57:49.215211 140263261390720 learning.py:507] global step 1990: loss = 2.8987 (0.350 sec/step)\n",
            "I0707 16:57:49.697327 140263261390720 learning.py:507] global step 1991: loss = 2.4826 (0.479 sec/step)\n",
            "I0707 16:57:50.009875 140263261390720 learning.py:507] global step 1992: loss = 6.4114 (0.311 sec/step)\n",
            "I0707 16:57:50.393723 140263261390720 learning.py:507] global step 1993: loss = 4.0026 (0.382 sec/step)\n",
            "I0707 16:57:50.741181 140263261390720 learning.py:507] global step 1994: loss = 3.7801 (0.346 sec/step)\n",
            "I0707 16:57:51.091840 140263261390720 learning.py:507] global step 1995: loss = 4.3259 (0.349 sec/step)\n",
            "I0707 16:57:51.406157 140263261390720 learning.py:507] global step 1996: loss = 5.1894 (0.312 sec/step)\n",
            "I0707 16:57:51.815417 140263261390720 learning.py:507] global step 1997: loss = 7.3420 (0.408 sec/step)\n",
            "I0707 16:57:52.140034 140263261390720 learning.py:507] global step 1998: loss = 4.6750 (0.323 sec/step)\n",
            "I0707 16:57:52.542287 140263261390720 learning.py:507] global step 1999: loss = 4.5870 (0.401 sec/step)\n",
            "I0707 16:57:52.859163 140263261390720 learning.py:507] global step 2000: loss = 3.4052 (0.315 sec/step)\n",
            "I0707 16:57:53.489307 140263261390720 learning.py:507] global step 2001: loss = 4.0595 (0.559 sec/step)\n",
            "I0707 16:57:53.749919 140260287772416 supervisor.py:1050] Recording summary at step 2001.\n",
            "I0707 16:57:53.990614 140263261390720 learning.py:507] global step 2002: loss = 5.4980 (0.497 sec/step)\n",
            "I0707 16:57:54.330035 140263261390720 learning.py:507] global step 2003: loss = 4.5815 (0.338 sec/step)\n",
            "I0707 16:57:54.649375 140263261390720 learning.py:507] global step 2004: loss = 3.4213 (0.318 sec/step)\n",
            "I0707 16:57:54.862041 140260296165120 supervisor.py:1099] global_step/sec: 2.87106\n",
            "I0707 16:57:54.994708 140263261390720 learning.py:507] global step 2005: loss = 5.0599 (0.344 sec/step)\n",
            "I0707 16:57:55.310466 140263261390720 learning.py:507] global step 2006: loss = 4.1593 (0.314 sec/step)\n",
            "I0707 16:57:55.756851 140263261390720 learning.py:507] global step 2007: loss = 4.1725 (0.443 sec/step)\n",
            "I0707 16:57:56.180387 140263261390720 learning.py:507] global step 2008: loss = 3.8581 (0.421 sec/step)\n",
            "I0707 16:57:56.484978 140263261390720 learning.py:507] global step 2009: loss = 3.1475 (0.303 sec/step)\n",
            "I0707 16:57:56.807657 140263261390720 learning.py:507] global step 2010: loss = 3.7042 (0.319 sec/step)\n",
            "I0707 16:57:57.128192 140263261390720 learning.py:507] global step 2011: loss = 4.2890 (0.319 sec/step)\n",
            "I0707 16:57:57.490901 140263261390720 learning.py:507] global step 2012: loss = 5.6854 (0.361 sec/step)\n",
            "I0707 16:57:57.789180 140263261390720 learning.py:507] global step 2013: loss = 4.0245 (0.297 sec/step)\n",
            "I0707 16:57:58.102608 140263261390720 learning.py:507] global step 2014: loss = 5.0410 (0.312 sec/step)\n",
            "I0707 16:57:58.474653 140263261390720 learning.py:507] global step 2015: loss = 5.5329 (0.370 sec/step)\n",
            "I0707 16:57:58.782440 140263261390720 learning.py:507] global step 2016: loss = 2.9699 (0.306 sec/step)\n",
            "I0707 16:57:59.092146 140263261390720 learning.py:507] global step 2017: loss = 4.6578 (0.308 sec/step)\n",
            "I0707 16:57:59.432851 140263261390720 learning.py:507] global step 2018: loss = 3.8413 (0.339 sec/step)\n",
            "I0707 16:57:59.842544 140263261390720 learning.py:507] global step 2019: loss = 5.8119 (0.408 sec/step)\n",
            "I0707 16:58:00.154218 140263261390720 learning.py:507] global step 2020: loss = 4.1519 (0.309 sec/step)\n",
            "I0707 16:58:00.464901 140263261390720 learning.py:507] global step 2021: loss = 3.8814 (0.309 sec/step)\n",
            "I0707 16:58:00.779550 140263261390720 learning.py:507] global step 2022: loss = 3.6815 (0.313 sec/step)\n",
            "I0707 16:58:01.100765 140263261390720 learning.py:507] global step 2023: loss = 7.3117 (0.319 sec/step)\n",
            "I0707 16:58:01.439399 140263261390720 learning.py:507] global step 2024: loss = 4.7755 (0.337 sec/step)\n",
            "I0707 16:58:01.760321 140263261390720 learning.py:507] global step 2025: loss = 5.1176 (0.319 sec/step)\n",
            "I0707 16:58:02.071934 140263261390720 learning.py:507] global step 2026: loss = 4.5335 (0.310 sec/step)\n",
            "I0707 16:58:02.368198 140263261390720 learning.py:507] global step 2027: loss = 6.6304 (0.295 sec/step)\n",
            "I0707 16:58:02.897431 140263261390720 learning.py:507] global step 2028: loss = 4.1063 (0.528 sec/step)\n",
            "I0707 16:58:03.207203 140263261390720 learning.py:507] global step 2029: loss = 3.0501 (0.308 sec/step)\n",
            "I0707 16:58:03.637356 140263261390720 learning.py:507] global step 2030: loss = 4.1492 (0.428 sec/step)\n",
            "I0707 16:58:03.970342 140263261390720 learning.py:507] global step 2031: loss = 4.1262 (0.331 sec/step)\n",
            "I0707 16:58:04.302555 140263261390720 learning.py:507] global step 2032: loss = 3.9098 (0.331 sec/step)\n",
            "I0707 16:58:04.613125 140263261390720 learning.py:507] global step 2033: loss = 5.7950 (0.309 sec/step)\n",
            "I0707 16:58:05.148337 140263261390720 learning.py:507] global step 2034: loss = 4.3041 (0.534 sec/step)\n",
            "I0707 16:58:05.478257 140263261390720 learning.py:507] global step 2035: loss = 6.0129 (0.328 sec/step)\n",
            "I0707 16:58:05.803849 140263261390720 learning.py:507] global step 2036: loss = 5.3875 (0.324 sec/step)\n",
            "I0707 16:58:06.116873 140263261390720 learning.py:507] global step 2037: loss = 6.1268 (0.311 sec/step)\n",
            "I0707 16:58:06.421460 140263261390720 learning.py:507] global step 2038: loss = 6.5011 (0.301 sec/step)\n",
            "I0707 16:58:06.828307 140263261390720 learning.py:507] global step 2039: loss = 5.2599 (0.405 sec/step)\n",
            "I0707 16:58:07.237946 140263261390720 learning.py:507] global step 2040: loss = 4.0199 (0.408 sec/step)\n",
            "I0707 16:58:07.599280 140263261390720 learning.py:507] global step 2041: loss = 4.4935 (0.359 sec/step)\n",
            "I0707 16:58:07.967205 140263261390720 learning.py:507] global step 2042: loss = 4.0952 (0.366 sec/step)\n",
            "I0707 16:58:08.295214 140263261390720 learning.py:507] global step 2043: loss = 4.8926 (0.326 sec/step)\n",
            "I0707 16:58:08.664001 140263261390720 learning.py:507] global step 2044: loss = 4.5643 (0.367 sec/step)\n",
            "I0707 16:58:08.994192 140263261390720 learning.py:507] global step 2045: loss = 3.5164 (0.328 sec/step)\n",
            "I0707 16:58:09.309524 140263261390720 learning.py:507] global step 2046: loss = 5.8182 (0.314 sec/step)\n",
            "I0707 16:58:09.742309 140263261390720 learning.py:507] global step 2047: loss = 3.9101 (0.431 sec/step)\n",
            "I0707 16:58:10.252920 140263261390720 learning.py:507] global step 2048: loss = 3.1262 (0.508 sec/step)\n",
            "I0707 16:58:10.571440 140263261390720 learning.py:507] global step 2049: loss = 4.3265 (0.317 sec/step)\n",
            "I0707 16:58:10.893878 140263261390720 learning.py:507] global step 2050: loss = 4.8271 (0.321 sec/step)\n",
            "I0707 16:58:11.217066 140263261390720 learning.py:507] global step 2051: loss = 4.1250 (0.321 sec/step)\n",
            "I0707 16:58:11.565209 140263261390720 learning.py:507] global step 2052: loss = 5.1795 (0.346 sec/step)\n",
            "I0707 16:58:12.102089 140263261390720 learning.py:507] global step 2053: loss = 3.9372 (0.535 sec/step)\n",
            "I0707 16:58:12.421177 140263261390720 learning.py:507] global step 2054: loss = 5.4203 (0.317 sec/step)\n",
            "I0707 16:58:12.737676 140263261390720 learning.py:507] global step 2055: loss = 4.2832 (0.315 sec/step)\n",
            "I0707 16:58:13.028023 140263261390720 learning.py:507] global step 2056: loss = 4.5938 (0.289 sec/step)\n",
            "I0707 16:58:13.360653 140263261390720 learning.py:507] global step 2057: loss = 5.2190 (0.331 sec/step)\n",
            "I0707 16:58:13.820921 140263261390720 learning.py:507] global step 2058: loss = 3.6806 (0.459 sec/step)\n",
            "I0707 16:58:14.139919 140263261390720 learning.py:507] global step 2059: loss = 4.8897 (0.317 sec/step)\n",
            "I0707 16:58:14.456565 140263261390720 learning.py:507] global step 2060: loss = 3.7403 (0.315 sec/step)\n",
            "I0707 16:58:14.762024 140263261390720 learning.py:507] global step 2061: loss = 3.4727 (0.304 sec/step)\n",
            "I0707 16:58:15.072461 140263261390720 learning.py:507] global step 2062: loss = 3.3091 (0.309 sec/step)\n",
            "I0707 16:58:15.435317 140263261390720 learning.py:507] global step 2063: loss = 3.1552 (0.361 sec/step)\n",
            "I0707 16:58:15.927600 140263261390720 learning.py:507] global step 2064: loss = 4.5868 (0.491 sec/step)\n",
            "I0707 16:58:16.251265 140263261390720 learning.py:507] global step 2065: loss = 4.2978 (0.322 sec/step)\n",
            "I0707 16:58:16.552439 140263261390720 learning.py:507] global step 2066: loss = 5.4828 (0.299 sec/step)\n",
            "I0707 16:58:16.911294 140263261390720 learning.py:507] global step 2067: loss = 3.7038 (0.357 sec/step)\n",
            "I0707 16:58:17.251935 140263261390720 learning.py:507] global step 2068: loss = 5.1648 (0.339 sec/step)\n",
            "I0707 16:58:17.585512 140263261390720 learning.py:507] global step 2069: loss = 4.9816 (0.332 sec/step)\n",
            "I0707 16:58:17.887423 140263261390720 learning.py:507] global step 2070: loss = 4.1210 (0.300 sec/step)\n",
            "I0707 16:58:18.187722 140263261390720 learning.py:507] global step 2071: loss = 4.6334 (0.299 sec/step)\n",
            "I0707 16:58:18.518059 140263261390720 learning.py:507] global step 2072: loss = 5.5552 (0.329 sec/step)\n",
            "I0707 16:58:18.820400 140263261390720 learning.py:507] global step 2073: loss = 5.6412 (0.300 sec/step)\n",
            "I0707 16:58:19.165384 140263261390720 learning.py:507] global step 2074: loss = 4.5059 (0.343 sec/step)\n",
            "I0707 16:58:19.603498 140263261390720 learning.py:507] global step 2075: loss = 3.2506 (0.436 sec/step)\n",
            "I0707 16:58:19.919503 140263261390720 learning.py:507] global step 2076: loss = 4.3066 (0.314 sec/step)\n",
            "I0707 16:58:20.209323 140263261390720 learning.py:507] global step 2077: loss = 3.1362 (0.288 sec/step)\n",
            "I0707 16:58:20.593432 140263261390720 learning.py:507] global step 2078: loss = 4.0816 (0.382 sec/step)\n",
            "I0707 16:58:20.896706 140263261390720 learning.py:507] global step 2079: loss = 3.7008 (0.301 sec/step)\n",
            "I0707 16:58:21.206532 140263261390720 learning.py:507] global step 2080: loss = 3.5054 (0.308 sec/step)\n",
            "I0707 16:58:21.529387 140263261390720 learning.py:507] global step 2081: loss = 4.2940 (0.321 sec/step)\n",
            "I0707 16:58:21.882407 140263261390720 learning.py:507] global step 2082: loss = 4.0686 (0.352 sec/step)\n",
            "I0707 16:58:22.202516 140263261390720 learning.py:507] global step 2083: loss = 3.4411 (0.319 sec/step)\n",
            "I0707 16:58:22.495569 140263261390720 learning.py:507] global step 2084: loss = 4.9399 (0.291 sec/step)\n",
            "I0707 16:58:22.852866 140263261390720 learning.py:507] global step 2085: loss = 4.5370 (0.356 sec/step)\n",
            "I0707 16:58:23.199036 140263261390720 learning.py:507] global step 2086: loss = 4.7353 (0.344 sec/step)\n",
            "I0707 16:58:23.483887 140263261390720 learning.py:507] global step 2087: loss = 3.8833 (0.283 sec/step)\n",
            "I0707 16:58:23.813045 140263261390720 learning.py:507] global step 2088: loss = 4.0688 (0.327 sec/step)\n",
            "I0707 16:58:24.127375 140263261390720 learning.py:507] global step 2089: loss = 5.6234 (0.313 sec/step)\n",
            "I0707 16:58:24.455355 140263261390720 learning.py:507] global step 2090: loss = 4.5818 (0.326 sec/step)\n",
            "I0707 16:58:24.759374 140263261390720 learning.py:507] global step 2091: loss = 5.4877 (0.302 sec/step)\n",
            "I0707 16:58:25.079891 140263261390720 learning.py:507] global step 2092: loss = 6.5716 (0.319 sec/step)\n",
            "I0707 16:58:25.404702 140263261390720 learning.py:507] global step 2093: loss = 3.7727 (0.323 sec/step)\n",
            "I0707 16:58:25.716573 140263261390720 learning.py:507] global step 2094: loss = 4.4708 (0.310 sec/step)\n",
            "I0707 16:58:26.085704 140263261390720 learning.py:507] global step 2095: loss = 4.9148 (0.367 sec/step)\n",
            "I0707 16:58:26.405867 140263261390720 learning.py:507] global step 2096: loss = 3.9248 (0.318 sec/step)\n",
            "I0707 16:58:26.715598 140263261390720 learning.py:507] global step 2097: loss = 3.2078 (0.308 sec/step)\n",
            "I0707 16:58:27.031369 140263261390720 learning.py:507] global step 2098: loss = 4.4116 (0.314 sec/step)\n",
            "I0707 16:58:27.325971 140263261390720 learning.py:507] global step 2099: loss = 4.8962 (0.293 sec/step)\n",
            "I0707 16:58:27.675764 140263261390720 learning.py:507] global step 2100: loss = 5.0906 (0.346 sec/step)\n",
            "I0707 16:58:28.127807 140263261390720 learning.py:507] global step 2101: loss = 4.1231 (0.450 sec/step)\n",
            "I0707 16:58:28.476585 140263261390720 learning.py:507] global step 2102: loss = 4.2544 (0.347 sec/step)\n",
            "I0707 16:58:28.839573 140263261390720 learning.py:507] global step 2103: loss = 3.8525 (0.361 sec/step)\n",
            "I0707 16:58:29.182337 140263261390720 learning.py:507] global step 2104: loss = 3.2736 (0.341 sec/step)\n",
            "I0707 16:58:29.477329 140263261390720 learning.py:507] global step 2105: loss = 4.7955 (0.293 sec/step)\n",
            "I0707 16:58:29.811857 140263261390720 learning.py:507] global step 2106: loss = 5.6862 (0.333 sec/step)\n",
            "I0707 16:58:30.096285 140263261390720 learning.py:507] global step 2107: loss = 3.8842 (0.283 sec/step)\n",
            "I0707 16:58:30.417535 140263261390720 learning.py:507] global step 2108: loss = 4.3375 (0.320 sec/step)\n",
            "I0707 16:58:30.749502 140263261390720 learning.py:507] global step 2109: loss = 4.4776 (0.330 sec/step)\n",
            "I0707 16:58:31.121435 140263261390720 learning.py:507] global step 2110: loss = 3.6461 (0.370 sec/step)\n",
            "I0707 16:58:31.460783 140263261390720 learning.py:507] global step 2111: loss = 4.3875 (0.338 sec/step)\n",
            "I0707 16:58:31.786776 140263261390720 learning.py:507] global step 2112: loss = 5.7926 (0.324 sec/step)\n",
            "I0707 16:58:32.121350 140263261390720 learning.py:507] global step 2113: loss = 4.4543 (0.333 sec/step)\n",
            "I0707 16:58:32.415629 140263261390720 learning.py:507] global step 2114: loss = 4.0517 (0.293 sec/step)\n",
            "I0707 16:58:32.717118 140263261390720 learning.py:507] global step 2115: loss = 4.7790 (0.300 sec/step)\n",
            "I0707 16:58:33.107120 140263261390720 learning.py:507] global step 2116: loss = 5.8986 (0.388 sec/step)\n",
            "I0707 16:58:33.417345 140263261390720 learning.py:507] global step 2117: loss = 5.2777 (0.309 sec/step)\n",
            "I0707 16:58:33.730192 140263261390720 learning.py:507] global step 2118: loss = 3.8037 (0.311 sec/step)\n",
            "I0707 16:58:34.040499 140263261390720 learning.py:507] global step 2119: loss = 4.6697 (0.309 sec/step)\n",
            "I0707 16:58:34.381885 140263261390720 learning.py:507] global step 2120: loss = 4.5407 (0.340 sec/step)\n",
            "I0707 16:58:34.759395 140263261390720 learning.py:507] global step 2121: loss = 3.1468 (0.376 sec/step)\n",
            "I0707 16:58:35.106747 140263261390720 learning.py:507] global step 2122: loss = 4.5583 (0.346 sec/step)\n",
            "I0707 16:58:35.411877 140263261390720 learning.py:507] global step 2123: loss = 5.0048 (0.303 sec/step)\n",
            "I0707 16:58:35.733257 140263261390720 learning.py:507] global step 2124: loss = 4.8394 (0.320 sec/step)\n",
            "I0707 16:58:36.032191 140263261390720 learning.py:507] global step 2125: loss = 3.7644 (0.297 sec/step)\n",
            "I0707 16:58:36.385946 140263261390720 learning.py:507] global step 2126: loss = 5.0971 (0.352 sec/step)\n",
            "I0707 16:58:36.805157 140263261390720 learning.py:507] global step 2127: loss = 5.6292 (0.418 sec/step)\n",
            "I0707 16:58:37.121334 140263261390720 learning.py:507] global step 2128: loss = 4.7873 (0.314 sec/step)\n",
            "I0707 16:58:37.472201 140263261390720 learning.py:507] global step 2129: loss = 4.6510 (0.349 sec/step)\n",
            "I0707 16:58:37.804138 140263261390720 learning.py:507] global step 2130: loss = 5.3259 (0.330 sec/step)\n",
            "I0707 16:58:38.124459 140263261390720 learning.py:507] global step 2131: loss = 3.7985 (0.319 sec/step)\n",
            "I0707 16:58:38.426388 140263261390720 learning.py:507] global step 2132: loss = 5.2129 (0.300 sec/step)\n",
            "I0707 16:58:38.768132 140263261390720 learning.py:507] global step 2133: loss = 4.0767 (0.340 sec/step)\n",
            "I0707 16:58:39.054013 140263261390720 learning.py:507] global step 2134: loss = 5.6690 (0.284 sec/step)\n",
            "I0707 16:58:39.347552 140263261390720 learning.py:507] global step 2135: loss = 4.8909 (0.292 sec/step)\n",
            "I0707 16:58:39.649183 140263261390720 learning.py:507] global step 2136: loss = 6.1030 (0.300 sec/step)\n",
            "I0707 16:58:39.990098 140263261390720 learning.py:507] global step 2137: loss = 3.9199 (0.339 sec/step)\n",
            "I0707 16:58:40.322927 140263261390720 learning.py:507] global step 2138: loss = 6.5335 (0.331 sec/step)\n",
            "I0707 16:58:40.644874 140263261390720 learning.py:507] global step 2139: loss = 4.5117 (0.320 sec/step)\n",
            "I0707 16:58:40.962753 140263261390720 learning.py:507] global step 2140: loss = 4.9435 (0.316 sec/step)\n",
            "I0707 16:58:41.293713 140263261390720 learning.py:507] global step 2141: loss = 2.8050 (0.329 sec/step)\n",
            "I0707 16:58:41.582931 140263261390720 learning.py:507] global step 2142: loss = 4.5850 (0.288 sec/step)\n",
            "I0707 16:58:41.868453 140263261390720 learning.py:507] global step 2143: loss = 3.8299 (0.284 sec/step)\n",
            "I0707 16:58:42.180629 140263261390720 learning.py:507] global step 2144: loss = 5.0224 (0.310 sec/step)\n",
            "I0707 16:58:42.595422 140263261390720 learning.py:507] global step 2145: loss = 4.7839 (0.413 sec/step)\n",
            "I0707 16:58:42.949567 140263261390720 learning.py:507] global step 2146: loss = 4.2591 (0.353 sec/step)\n",
            "I0707 16:58:43.277353 140263261390720 learning.py:507] global step 2147: loss = 4.8144 (0.326 sec/step)\n",
            "I0707 16:58:43.575014 140263261390720 learning.py:507] global step 2148: loss = 4.3129 (0.296 sec/step)\n",
            "I0707 16:58:43.928138 140263261390720 learning.py:507] global step 2149: loss = 3.6466 (0.351 sec/step)\n",
            "I0707 16:58:44.262608 140263261390720 learning.py:507] global step 2150: loss = 5.0375 (0.333 sec/step)\n",
            "I0707 16:58:44.549374 140263261390720 learning.py:507] global step 2151: loss = 4.0242 (0.285 sec/step)\n",
            "I0707 16:58:44.925738 140263261390720 learning.py:507] global step 2152: loss = 4.7048 (0.375 sec/step)\n",
            "I0707 16:58:45.238389 140263261390720 learning.py:507] global step 2153: loss = 4.1033 (0.310 sec/step)\n",
            "I0707 16:58:45.644096 140263261390720 learning.py:507] global step 2154: loss = 5.0079 (0.404 sec/step)\n",
            "I0707 16:58:46.017375 140263261390720 learning.py:507] global step 2155: loss = 5.5329 (0.372 sec/step)\n",
            "I0707 16:58:46.329153 140263261390720 learning.py:507] global step 2156: loss = 5.6051 (0.310 sec/step)\n",
            "I0707 16:58:46.640101 140263261390720 learning.py:507] global step 2157: loss = 4.8251 (0.309 sec/step)\n",
            "I0707 16:58:47.007863 140263261390720 learning.py:507] global step 2158: loss = 6.1368 (0.366 sec/step)\n",
            "I0707 16:58:47.316186 140263261390720 learning.py:507] global step 2159: loss = 5.0417 (0.306 sec/step)\n",
            "I0707 16:58:47.597326 140263261390720 learning.py:507] global step 2160: loss = 5.1419 (0.279 sec/step)\n",
            "I0707 16:58:47.905376 140263261390720 learning.py:507] global step 2161: loss = 4.8231 (0.306 sec/step)\n",
            "I0707 16:58:48.202694 140263261390720 learning.py:507] global step 2162: loss = 4.0751 (0.295 sec/step)\n",
            "I0707 16:58:48.523544 140263261390720 learning.py:507] global step 2163: loss = 5.0289 (0.319 sec/step)\n",
            "I0707 16:58:48.862977 140263261390720 learning.py:507] global step 2164: loss = 5.0253 (0.338 sec/step)\n",
            "I0707 16:58:49.208467 140263261390720 learning.py:507] global step 2165: loss = 4.4616 (0.344 sec/step)\n",
            "I0707 16:58:49.527022 140263261390720 learning.py:507] global step 2166: loss = 4.0493 (0.317 sec/step)\n",
            "I0707 16:58:49.861332 140263261390720 learning.py:507] global step 2167: loss = 4.8141 (0.333 sec/step)\n",
            "I0707 16:58:50.163264 140263261390720 learning.py:507] global step 2168: loss = 6.8245 (0.300 sec/step)\n",
            "I0707 16:58:50.469648 140263261390720 learning.py:507] global step 2169: loss = 3.8324 (0.305 sec/step)\n",
            "I0707 16:58:50.770750 140263261390720 learning.py:507] global step 2170: loss = 5.0508 (0.299 sec/step)\n",
            "I0707 16:58:51.076978 140263261390720 learning.py:507] global step 2171: loss = 4.0752 (0.304 sec/step)\n",
            "I0707 16:58:51.389097 140263261390720 learning.py:507] global step 2172: loss = 4.6470 (0.310 sec/step)\n",
            "I0707 16:58:51.688719 140263261390720 learning.py:507] global step 2173: loss = 4.3519 (0.298 sec/step)\n",
            "I0707 16:58:52.218864 140263261390720 learning.py:507] global step 2174: loss = 5.3322 (0.528 sec/step)\n",
            "I0707 16:58:52.517650 140263261390720 learning.py:507] global step 2175: loss = 5.9301 (0.297 sec/step)\n",
            "I0707 16:58:52.847754 140263261390720 learning.py:507] global step 2176: loss = 3.4408 (0.329 sec/step)\n",
            "I0707 16:58:53.150302 140263261390720 learning.py:507] global step 2177: loss = 4.3079 (0.301 sec/step)\n",
            "I0707 16:58:53.452520 140263261390720 learning.py:507] global step 2178: loss = 4.2342 (0.300 sec/step)\n",
            "I0707 16:58:53.753354 140263261390720 learning.py:507] global step 2179: loss = 3.8083 (0.299 sec/step)\n",
            "I0707 16:58:54.439399 140263261390720 learning.py:507] global step 2180: loss = 3.4457 (0.684 sec/step)\n",
            "I0707 16:58:54.750643 140263261390720 learning.py:507] global step 2181: loss = 5.3366 (0.309 sec/step)\n",
            "I0707 16:58:55.071594 140263261390720 learning.py:507] global step 2182: loss = 4.0754 (0.319 sec/step)\n",
            "I0707 16:58:55.390870 140263261390720 learning.py:507] global step 2183: loss = 5.8190 (0.318 sec/step)\n",
            "I0707 16:58:55.720432 140263261390720 learning.py:507] global step 2184: loss = 4.4074 (0.328 sec/step)\n",
            "I0707 16:58:56.025460 140263261390720 learning.py:507] global step 2185: loss = 4.3803 (0.303 sec/step)\n",
            "I0707 16:58:56.348946 140263261390720 learning.py:507] global step 2186: loss = 4.1892 (0.322 sec/step)\n",
            "I0707 16:58:56.809351 140263261390720 learning.py:507] global step 2187: loss = 5.9295 (0.459 sec/step)\n",
            "I0707 16:58:57.108867 140263261390720 learning.py:507] global step 2188: loss = 3.9299 (0.298 sec/step)\n",
            "I0707 16:58:57.425171 140263261390720 learning.py:507] global step 2189: loss = 4.1275 (0.315 sec/step)\n",
            "I0707 16:58:57.784754 140263261390720 learning.py:507] global step 2190: loss = 3.7597 (0.358 sec/step)\n",
            "I0707 16:58:58.105114 140263261390720 learning.py:507] global step 2191: loss = 3.4453 (0.319 sec/step)\n",
            "I0707 16:58:58.408943 140263261390720 learning.py:507] global step 2192: loss = 4.4069 (0.302 sec/step)\n",
            "I0707 16:58:58.741277 140263261390720 learning.py:507] global step 2193: loss = 3.9173 (0.331 sec/step)\n",
            "I0707 16:58:59.041280 140263261390720 learning.py:507] global step 2194: loss = 4.8002 (0.298 sec/step)\n",
            "I0707 16:58:59.369088 140263261390720 learning.py:507] global step 2195: loss = 6.4733 (0.325 sec/step)\n",
            "I0707 16:58:59.689087 140263261390720 learning.py:507] global step 2196: loss = 5.9406 (0.318 sec/step)\n",
            "I0707 16:58:59.985628 140263261390720 learning.py:507] global step 2197: loss = 4.1781 (0.295 sec/step)\n",
            "I0707 16:59:00.286639 140263261390720 learning.py:507] global step 2198: loss = 4.4512 (0.299 sec/step)\n",
            "I0707 16:59:00.618160 140263261390720 learning.py:507] global step 2199: loss = 5.4398 (0.330 sec/step)\n",
            "I0707 16:59:00.931809 140263261390720 learning.py:507] global step 2200: loss = 4.6471 (0.312 sec/step)\n",
            "I0707 16:59:01.231167 140263261390720 learning.py:507] global step 2201: loss = 3.9029 (0.298 sec/step)\n",
            "I0707 16:59:01.550338 140263261390720 learning.py:507] global step 2202: loss = 4.3261 (0.317 sec/step)\n",
            "I0707 16:59:01.855672 140263261390720 learning.py:507] global step 2203: loss = 3.9143 (0.304 sec/step)\n",
            "I0707 16:59:02.148864 140263261390720 learning.py:507] global step 2204: loss = 3.8313 (0.291 sec/step)\n",
            "I0707 16:59:02.437786 140263261390720 learning.py:507] global step 2205: loss = 3.8640 (0.287 sec/step)\n",
            "I0707 16:59:02.748671 140263261390720 learning.py:507] global step 2206: loss = 4.9263 (0.309 sec/step)\n",
            "I0707 16:59:03.044787 140263261390720 learning.py:507] global step 2207: loss = 4.1592 (0.294 sec/step)\n",
            "I0707 16:59:03.361595 140263261390720 learning.py:507] global step 2208: loss = 5.0215 (0.315 sec/step)\n",
            "I0707 16:59:03.684742 140263261390720 learning.py:507] global step 2209: loss = 3.1347 (0.321 sec/step)\n",
            "I0707 16:59:03.987653 140263261390720 learning.py:507] global step 2210: loss = 5.0475 (0.301 sec/step)\n",
            "I0707 16:59:04.276229 140263261390720 learning.py:507] global step 2211: loss = 3.8151 (0.287 sec/step)\n",
            "I0707 16:59:04.570975 140263261390720 learning.py:507] global step 2212: loss = 4.3971 (0.293 sec/step)\n",
            "I0707 16:59:04.895472 140263261390720 learning.py:507] global step 2213: loss = 4.0059 (0.323 sec/step)\n",
            "I0707 16:59:05.203426 140263261390720 learning.py:507] global step 2214: loss = 3.1938 (0.306 sec/step)\n",
            "I0707 16:59:05.528991 140263261390720 learning.py:507] global step 2215: loss = 4.7751 (0.324 sec/step)\n",
            "I0707 16:59:05.873009 140263261390720 learning.py:507] global step 2216: loss = 4.5059 (0.342 sec/step)\n",
            "I0707 16:59:06.178283 140263261390720 learning.py:507] global step 2217: loss = 6.4277 (0.304 sec/step)\n",
            "I0707 16:59:06.482440 140263261390720 learning.py:507] global step 2218: loss = 3.5728 (0.302 sec/step)\n",
            "I0707 16:59:06.806441 140263261390720 learning.py:507] global step 2219: loss = 4.2418 (0.321 sec/step)\n",
            "I0707 16:59:07.148850 140263261390720 learning.py:507] global step 2220: loss = 3.7260 (0.340 sec/step)\n",
            "I0707 16:59:07.453677 140263261390720 learning.py:507] global step 2221: loss = 4.6939 (0.303 sec/step)\n",
            "I0707 16:59:07.813916 140263261390720 learning.py:507] global step 2222: loss = 3.8764 (0.359 sec/step)\n",
            "I0707 16:59:08.148100 140263261390720 learning.py:507] global step 2223: loss = 3.9493 (0.332 sec/step)\n",
            "I0707 16:59:08.491602 140263261390720 learning.py:507] global step 2224: loss = 4.0895 (0.342 sec/step)\n",
            "I0707 16:59:08.845180 140263261390720 learning.py:507] global step 2225: loss = 3.9144 (0.352 sec/step)\n",
            "I0707 16:59:09.177231 140263261390720 learning.py:507] global step 2226: loss = 4.1099 (0.330 sec/step)\n",
            "I0707 16:59:09.494491 140263261390720 learning.py:507] global step 2227: loss = 5.3937 (0.315 sec/step)\n",
            "I0707 16:59:09.797354 140263261390720 learning.py:507] global step 2228: loss = 3.5783 (0.301 sec/step)\n",
            "I0707 16:59:10.114144 140263261390720 learning.py:507] global step 2229: loss = 4.7607 (0.315 sec/step)\n",
            "I0707 16:59:10.434449 140263261390720 learning.py:507] global step 2230: loss = 4.8258 (0.319 sec/step)\n",
            "I0707 16:59:10.738898 140263261390720 learning.py:507] global step 2231: loss = 4.0105 (0.303 sec/step)\n",
            "I0707 16:59:11.040485 140263261390720 learning.py:507] global step 2232: loss = 3.9508 (0.300 sec/step)\n",
            "I0707 16:59:11.351670 140263261390720 learning.py:507] global step 2233: loss = 4.9055 (0.309 sec/step)\n",
            "I0707 16:59:11.682616 140263261390720 learning.py:507] global step 2234: loss = 4.5743 (0.329 sec/step)\n",
            "I0707 16:59:12.131041 140263261390720 learning.py:507] global step 2235: loss = 4.2749 (0.447 sec/step)\n",
            "I0707 16:59:12.435387 140263261390720 learning.py:507] global step 2236: loss = 4.0438 (0.303 sec/step)\n",
            "I0707 16:59:12.747865 140263261390720 learning.py:507] global step 2237: loss = 3.9578 (0.311 sec/step)\n",
            "I0707 16:59:13.069063 140263261390720 learning.py:507] global step 2238: loss = 6.2502 (0.319 sec/step)\n",
            "I0707 16:59:13.353963 140263261390720 learning.py:507] global step 2239: loss = 3.1868 (0.283 sec/step)\n",
            "I0707 16:59:13.702921 140263261390720 learning.py:507] global step 2240: loss = 4.8190 (0.347 sec/step)\n",
            "I0707 16:59:14.051220 140263261390720 learning.py:507] global step 2241: loss = 3.4383 (0.347 sec/step)\n",
            "I0707 16:59:14.358513 140263261390720 learning.py:507] global step 2242: loss = 4.8034 (0.306 sec/step)\n",
            "I0707 16:59:14.812618 140263261390720 learning.py:507] global step 2243: loss = 4.1437 (0.452 sec/step)\n",
            "I0707 16:59:15.190670 140263261390720 learning.py:507] global step 2244: loss = 4.4457 (0.376 sec/step)\n",
            "I0707 16:59:15.489758 140263261390720 learning.py:507] global step 2245: loss = 3.8677 (0.297 sec/step)\n",
            "I0707 16:59:15.831153 140263261390720 learning.py:507] global step 2246: loss = 2.8607 (0.340 sec/step)\n",
            "I0707 16:59:16.196142 140263261390720 learning.py:507] global step 2247: loss = 4.0919 (0.363 sec/step)\n",
            "I0707 16:59:16.529465 140263261390720 learning.py:507] global step 2248: loss = 4.8028 (0.331 sec/step)\n",
            "I0707 16:59:16.927279 140263261390720 learning.py:507] global step 2249: loss = 4.6652 (0.396 sec/step)\n",
            "I0707 16:59:17.286757 140263261390720 learning.py:507] global step 2250: loss = 3.9380 (0.358 sec/step)\n",
            "I0707 16:59:17.594645 140263261390720 learning.py:507] global step 2251: loss = 5.4099 (0.306 sec/step)\n",
            "I0707 16:59:17.938440 140263261390720 learning.py:507] global step 2252: loss = 4.3960 (0.342 sec/step)\n",
            "I0707 16:59:18.275886 140263261390720 learning.py:507] global step 2253: loss = 3.6459 (0.335 sec/step)\n",
            "I0707 16:59:18.630762 140263261390720 learning.py:507] global step 2254: loss = 6.6555 (0.353 sec/step)\n",
            "I0707 16:59:18.958213 140263261390720 learning.py:507] global step 2255: loss = 4.8508 (0.326 sec/step)\n",
            "I0707 16:59:19.261682 140263261390720 learning.py:507] global step 2256: loss = 3.6428 (0.301 sec/step)\n",
            "I0707 16:59:19.606596 140263261390720 learning.py:507] global step 2257: loss = 4.6570 (0.343 sec/step)\n",
            "I0707 16:59:19.934225 140263261390720 learning.py:507] global step 2258: loss = 5.2073 (0.326 sec/step)\n",
            "I0707 16:59:20.259342 140263261390720 learning.py:507] global step 2259: loss = 4.7103 (0.323 sec/step)\n",
            "I0707 16:59:20.644253 140263261390720 learning.py:507] global step 2260: loss = 6.1420 (0.383 sec/step)\n",
            "I0707 16:59:20.947018 140263261390720 learning.py:507] global step 2261: loss = 2.9381 (0.301 sec/step)\n",
            "I0707 16:59:21.283982 140263261390720 learning.py:507] global step 2262: loss = 3.4821 (0.335 sec/step)\n",
            "I0707 16:59:21.664683 140263261390720 learning.py:507] global step 2263: loss = 3.8024 (0.379 sec/step)\n",
            "I0707 16:59:21.994664 140263261390720 learning.py:507] global step 2264: loss = 3.3414 (0.328 sec/step)\n",
            "I0707 16:59:22.313613 140263261390720 learning.py:507] global step 2265: loss = 3.5385 (0.317 sec/step)\n",
            "I0707 16:59:22.607962 140263261390720 learning.py:507] global step 2266: loss = 4.9440 (0.293 sec/step)\n",
            "I0707 16:59:22.917409 140263261390720 learning.py:507] global step 2267: loss = 4.0821 (0.308 sec/step)\n",
            "I0707 16:59:23.242908 140263261390720 learning.py:507] global step 2268: loss = 3.6699 (0.324 sec/step)\n",
            "I0707 16:59:23.624018 140263261390720 learning.py:507] global step 2269: loss = 5.0332 (0.379 sec/step)\n",
            "I0707 16:59:23.937422 140263261390720 learning.py:507] global step 2270: loss = 4.5620 (0.312 sec/step)\n",
            "I0707 16:59:24.255264 140263261390720 learning.py:507] global step 2271: loss = 4.2120 (0.316 sec/step)\n",
            "I0707 16:59:24.557334 140263261390720 learning.py:507] global step 2272: loss = 4.2983 (0.300 sec/step)\n",
            "I0707 16:59:24.860854 140263261390720 learning.py:507] global step 2273: loss = 4.0870 (0.302 sec/step)\n",
            "I0707 16:59:25.191083 140263261390720 learning.py:507] global step 2274: loss = 3.5461 (0.328 sec/step)\n",
            "I0707 16:59:25.528239 140263261390720 learning.py:507] global step 2275: loss = 3.3538 (0.335 sec/step)\n",
            "I0707 16:59:25.826064 140263261390720 learning.py:507] global step 2276: loss = 5.6400 (0.296 sec/step)\n",
            "I0707 16:59:26.152795 140263261390720 learning.py:507] global step 2277: loss = 3.6727 (0.325 sec/step)\n",
            "I0707 16:59:26.490566 140263261390720 learning.py:507] global step 2278: loss = 4.0088 (0.336 sec/step)\n",
            "I0707 16:59:26.797651 140263261390720 learning.py:507] global step 2279: loss = 4.1693 (0.305 sec/step)\n",
            "I0707 16:59:27.128343 140263261390720 learning.py:507] global step 2280: loss = 4.9183 (0.329 sec/step)\n",
            "I0707 16:59:27.446265 140263261390720 learning.py:507] global step 2281: loss = 3.7919 (0.315 sec/step)\n",
            "I0707 16:59:27.782923 140263261390720 learning.py:507] global step 2282: loss = 5.5827 (0.334 sec/step)\n",
            "I0707 16:59:28.087983 140263261390720 learning.py:507] global step 2283: loss = 3.9739 (0.303 sec/step)\n",
            "I0707 16:59:28.393317 140263261390720 learning.py:507] global step 2284: loss = 3.7467 (0.303 sec/step)\n",
            "I0707 16:59:28.832787 140263261390720 learning.py:507] global step 2285: loss = 4.4229 (0.438 sec/step)\n",
            "I0707 16:59:29.205869 140263261390720 learning.py:507] global step 2286: loss = 3.6537 (0.371 sec/step)\n",
            "I0707 16:59:29.527529 140263261390720 learning.py:507] global step 2287: loss = 4.6938 (0.320 sec/step)\n",
            "I0707 16:59:29.908041 140263261390720 learning.py:507] global step 2288: loss = 3.7320 (0.379 sec/step)\n",
            "I0707 16:59:30.214701 140263261390720 learning.py:507] global step 2289: loss = 4.2156 (0.305 sec/step)\n",
            "I0707 16:59:30.511304 140263261390720 learning.py:507] global step 2290: loss = 3.8330 (0.295 sec/step)\n",
            "I0707 16:59:30.943022 140263261390720 learning.py:507] global step 2291: loss = 2.6721 (0.430 sec/step)\n",
            "I0707 16:59:31.247925 140263261390720 learning.py:507] global step 2292: loss = 5.6952 (0.303 sec/step)\n",
            "I0707 16:59:31.571182 140263261390720 learning.py:507] global step 2293: loss = 3.7075 (0.322 sec/step)\n",
            "I0707 16:59:31.975029 140263261390720 learning.py:507] global step 2294: loss = 4.9741 (0.402 sec/step)\n",
            "I0707 16:59:32.302256 140263261390720 learning.py:507] global step 2295: loss = 3.6281 (0.326 sec/step)\n",
            "I0707 16:59:32.599470 140263261390720 learning.py:507] global step 2296: loss = 4.7580 (0.296 sec/step)\n",
            "I0707 16:59:32.912214 140263261390720 learning.py:507] global step 2297: loss = 5.7653 (0.311 sec/step)\n",
            "I0707 16:59:33.289788 140263261390720 learning.py:507] global step 2298: loss = 3.9542 (0.376 sec/step)\n",
            "I0707 16:59:33.590399 140263261390720 learning.py:507] global step 2299: loss = 4.2113 (0.295 sec/step)\n",
            "I0707 16:59:34.034968 140263261390720 learning.py:507] global step 2300: loss = 3.8599 (0.443 sec/step)\n",
            "I0707 16:59:34.337776 140263261390720 learning.py:507] global step 2301: loss = 3.4416 (0.301 sec/step)\n",
            "I0707 16:59:34.654220 140263261390720 learning.py:507] global step 2302: loss = 2.4281 (0.315 sec/step)\n",
            "I0707 16:59:35.000411 140263261390720 learning.py:507] global step 2303: loss = 3.0184 (0.345 sec/step)\n",
            "I0707 16:59:35.413884 140263261390720 learning.py:507] global step 2304: loss = 4.0376 (0.412 sec/step)\n",
            "I0707 16:59:35.738778 140263261390720 learning.py:507] global step 2305: loss = 4.1353 (0.323 sec/step)\n",
            "I0707 16:59:36.076993 140263261390720 learning.py:507] global step 2306: loss = 4.6289 (0.336 sec/step)\n",
            "I0707 16:59:36.398035 140263261390720 learning.py:507] global step 2307: loss = 4.2893 (0.319 sec/step)\n",
            "I0707 16:59:36.768522 140263261390720 learning.py:507] global step 2308: loss = 3.6020 (0.369 sec/step)\n",
            "I0707 16:59:37.138075 140263261390720 learning.py:507] global step 2309: loss = 3.4025 (0.368 sec/step)\n",
            "I0707 16:59:37.439720 140263261390720 learning.py:507] global step 2310: loss = 4.1081 (0.300 sec/step)\n",
            "I0707 16:59:37.801633 140263261390720 learning.py:507] global step 2311: loss = 3.2639 (0.360 sec/step)\n",
            "I0707 16:59:38.123607 140263261390720 learning.py:507] global step 2312: loss = 4.0341 (0.320 sec/step)\n",
            "I0707 16:59:38.456622 140263261390720 learning.py:507] global step 2313: loss = 4.3161 (0.331 sec/step)\n",
            "I0707 16:59:39.007670 140263261390720 learning.py:507] global step 2314: loss = 3.6377 (0.550 sec/step)\n",
            "I0707 16:59:39.455070 140263261390720 learning.py:507] global step 2315: loss = 5.9544 (0.446 sec/step)\n",
            "I0707 16:59:39.743262 140263261390720 learning.py:507] global step 2316: loss = 5.4595 (0.287 sec/step)\n",
            "I0707 16:59:40.050916 140263261390720 learning.py:507] global step 2317: loss = 6.9003 (0.306 sec/step)\n",
            "I0707 16:59:40.349771 140263261390720 learning.py:507] global step 2318: loss = 4.6699 (0.297 sec/step)\n",
            "I0707 16:59:40.692171 140263261390720 learning.py:507] global step 2319: loss = 4.1433 (0.340 sec/step)\n",
            "I0707 16:59:41.467915 140263261390720 learning.py:507] global step 2320: loss = 4.4339 (0.774 sec/step)\n",
            "I0707 16:59:41.787474 140263261390720 learning.py:507] global step 2321: loss = 3.8197 (0.318 sec/step)\n",
            "I0707 16:59:42.094034 140263261390720 learning.py:507] global step 2322: loss = 5.1000 (0.305 sec/step)\n",
            "I0707 16:59:42.530722 140263261390720 learning.py:507] global step 2323: loss = 4.5235 (0.435 sec/step)\n",
            "I0707 16:59:42.818901 140263261390720 learning.py:507] global step 2324: loss = 4.9450 (0.287 sec/step)\n",
            "I0707 16:59:43.132052 140263261390720 learning.py:507] global step 2325: loss = 5.1647 (0.312 sec/step)\n",
            "I0707 16:59:43.548381 140263261390720 learning.py:507] global step 2326: loss = 3.8824 (0.415 sec/step)\n",
            "I0707 16:59:43.952097 140263261390720 learning.py:507] global step 2327: loss = 4.3098 (0.402 sec/step)\n",
            "I0707 16:59:44.297156 140263261390720 learning.py:507] global step 2328: loss = 3.5203 (0.343 sec/step)\n",
            "I0707 16:59:44.596100 140263261390720 learning.py:507] global step 2329: loss = 5.2633 (0.297 sec/step)\n",
            "I0707 16:59:44.943737 140263261390720 learning.py:507] global step 2330: loss = 3.8788 (0.346 sec/step)\n",
            "I0707 16:59:45.241403 140263261390720 learning.py:507] global step 2331: loss = 4.8044 (0.295 sec/step)\n",
            "I0707 16:59:45.552081 140263261390720 learning.py:507] global step 2332: loss = 3.3732 (0.309 sec/step)\n",
            "I0707 16:59:45.877217 140263261390720 learning.py:507] global step 2333: loss = 4.7452 (0.323 sec/step)\n",
            "I0707 16:59:46.197279 140263261390720 learning.py:507] global step 2334: loss = 3.8420 (0.318 sec/step)\n",
            "I0707 16:59:46.625310 140263261390720 learning.py:507] global step 2335: loss = 4.8945 (0.426 sec/step)\n",
            "I0707 16:59:46.942396 140263261390720 learning.py:507] global step 2336: loss = 5.0271 (0.316 sec/step)\n",
            "I0707 16:59:47.256401 140263261390720 learning.py:507] global step 2337: loss = 4.9135 (0.312 sec/step)\n",
            "I0707 16:59:47.561534 140263261390720 learning.py:507] global step 2338: loss = 5.2236 (0.303 sec/step)\n",
            "I0707 16:59:47.939785 140263261390720 learning.py:507] global step 2339: loss = 3.9052 (0.376 sec/step)\n",
            "I0707 16:59:48.245151 140263261390720 learning.py:507] global step 2340: loss = 5.0587 (0.304 sec/step)\n",
            "I0707 16:59:48.642288 140263261390720 learning.py:507] global step 2341: loss = 3.9169 (0.395 sec/step)\n",
            "I0707 16:59:48.970134 140263261390720 learning.py:507] global step 2342: loss = 5.0923 (0.326 sec/step)\n",
            "I0707 16:59:49.362159 140263261390720 learning.py:507] global step 2343: loss = 4.5110 (0.390 sec/step)\n",
            "I0707 16:59:49.665736 140263261390720 learning.py:507] global step 2344: loss = 4.3602 (0.302 sec/step)\n",
            "I0707 16:59:50.016803 140263261390720 learning.py:507] global step 2345: loss = 4.5115 (0.349 sec/step)\n",
            "I0707 16:59:50.321742 140263261390720 learning.py:507] global step 2346: loss = 4.7095 (0.303 sec/step)\n",
            "I0707 16:59:50.710029 140263261390720 learning.py:507] global step 2347: loss = 5.4816 (0.387 sec/step)\n",
            "I0707 16:59:51.006191 140263261390720 learning.py:507] global step 2348: loss = 4.7904 (0.294 sec/step)\n",
            "I0707 16:59:51.448599 140263261390720 learning.py:507] global step 2349: loss = 3.9963 (0.441 sec/step)\n",
            "I0707 16:59:51.900856 140263261390720 learning.py:507] global step 2350: loss = 4.9406 (0.451 sec/step)\n",
            "I0707 16:59:52.229952 140263261390720 learning.py:507] global step 2351: loss = 4.6330 (0.327 sec/step)\n",
            "I0707 16:59:52.528911 140263261390720 learning.py:507] global step 2352: loss = 5.0306 (0.297 sec/step)\n",
            "I0707 16:59:52.846456 140263261390720 learning.py:507] global step 2353: loss = 4.5213 (0.316 sec/step)\n",
            "I0707 16:59:53.346859 140263261390720 learning.py:507] global step 2354: loss = 4.2265 (0.363 sec/step)\n",
            "I0707 16:59:53.928566 140260287772416 supervisor.py:1050] Recording summary at step 2354.\n",
            "I0707 16:59:54.020228 140263261390720 learning.py:507] global step 2355: loss = 3.5835 (0.638 sec/step)\n",
            "I0707 16:59:54.344276 140263261390720 learning.py:507] global step 2356: loss = 4.9953 (0.322 sec/step)\n",
            "I0707 16:59:54.645474 140263261390720 learning.py:507] global step 2357: loss = 5.5372 (0.300 sec/step)\n",
            "I0707 16:59:54.843737 140260296165120 supervisor.py:1099] global_step/sec: 2.94212\n",
            "I0707 16:59:54.970799 140263261390720 learning.py:507] global step 2358: loss = 5.2797 (0.324 sec/step)\n",
            "I0707 16:59:55.327795 140263261390720 learning.py:507] global step 2359: loss = 4.3148 (0.355 sec/step)\n",
            "I0707 16:59:55.718792 140263261390720 learning.py:507] global step 2360: loss = 4.7574 (0.389 sec/step)\n",
            "I0707 16:59:56.059932 140263261390720 learning.py:507] global step 2361: loss = 5.0074 (0.339 sec/step)\n",
            "I0707 16:59:56.478287 140263261390720 learning.py:507] global step 2362: loss = 4.4924 (0.417 sec/step)\n",
            "I0707 16:59:56.819235 140263261390720 learning.py:507] global step 2363: loss = 5.1609 (0.339 sec/step)\n",
            "I0707 16:59:57.119288 140263261390720 learning.py:507] global step 2364: loss = 3.8244 (0.299 sec/step)\n",
            "I0707 16:59:57.578913 140263261390720 learning.py:507] global step 2365: loss = 7.1054 (0.458 sec/step)\n",
            "I0707 16:59:57.872974 140263261390720 learning.py:507] global step 2366: loss = 5.5213 (0.292 sec/step)\n",
            "I0707 16:59:58.187285 140263261390720 learning.py:507] global step 2367: loss = 3.8918 (0.313 sec/step)\n",
            "I0707 16:59:58.487093 140263261390720 learning.py:507] global step 2368: loss = 4.3115 (0.298 sec/step)\n",
            "I0707 16:59:58.872796 140263261390720 learning.py:507] global step 2369: loss = 5.0013 (0.384 sec/step)\n",
            "I0707 16:59:59.223177 140263261390720 learning.py:507] global step 2370: loss = 4.4142 (0.349 sec/step)\n",
            "I0707 16:59:59.541601 140263261390720 learning.py:507] global step 2371: loss = 6.5913 (0.317 sec/step)\n",
            "I0707 16:59:59.840962 140263261390720 learning.py:507] global step 2372: loss = 3.5485 (0.298 sec/step)\n",
            "I0707 17:00:00.241462 140263261390720 learning.py:507] global step 2373: loss = 4.4679 (0.399 sec/step)\n",
            "I0707 17:00:00.567443 140263261390720 learning.py:507] global step 2374: loss = 4.8405 (0.324 sec/step)\n",
            "I0707 17:00:01.015557 140263261390720 learning.py:507] global step 2375: loss = 4.7080 (0.446 sec/step)\n",
            "I0707 17:00:01.352706 140263261390720 learning.py:507] global step 2376: loss = 5.2882 (0.335 sec/step)\n",
            "I0707 17:00:01.701029 140263261390720 learning.py:507] global step 2377: loss = 3.9328 (0.346 sec/step)\n",
            "I0707 17:00:02.031390 140263261390720 learning.py:507] global step 2378: loss = 3.4077 (0.329 sec/step)\n",
            "I0707 17:00:02.347680 140263261390720 learning.py:507] global step 2379: loss = 3.8279 (0.314 sec/step)\n",
            "I0707 17:00:02.708847 140263261390720 learning.py:507] global step 2380: loss = 4.8847 (0.359 sec/step)\n",
            "I0707 17:00:03.033181 140263261390720 learning.py:507] global step 2381: loss = 5.7934 (0.322 sec/step)\n",
            "I0707 17:00:03.410184 140263261390720 learning.py:507] global step 2382: loss = 4.1455 (0.375 sec/step)\n",
            "I0707 17:00:03.753708 140263261390720 learning.py:507] global step 2383: loss = 3.2938 (0.342 sec/step)\n",
            "I0707 17:00:04.058255 140263261390720 learning.py:507] global step 2384: loss = 5.3291 (0.303 sec/step)\n",
            "I0707 17:00:04.456859 140263261390720 learning.py:507] global step 2385: loss = 6.2714 (0.397 sec/step)\n",
            "I0707 17:00:04.777291 140263261390720 learning.py:507] global step 2386: loss = 5.2570 (0.318 sec/step)\n",
            "I0707 17:00:05.096205 140263261390720 learning.py:507] global step 2387: loss = 4.1009 (0.317 sec/step)\n",
            "I0707 17:00:05.433068 140263261390720 learning.py:507] global step 2388: loss = 4.0646 (0.335 sec/step)\n",
            "I0707 17:00:05.779121 140263261390720 learning.py:507] global step 2389: loss = 4.4754 (0.344 sec/step)\n",
            "I0707 17:00:06.153583 140263261390720 learning.py:507] global step 2390: loss = 3.9398 (0.372 sec/step)\n",
            "I0707 17:00:06.581119 140263261390720 learning.py:507] global step 2391: loss = 5.6262 (0.426 sec/step)\n",
            "I0707 17:00:06.900646 140263261390720 learning.py:507] global step 2392: loss = 5.0020 (0.318 sec/step)\n",
            "I0707 17:00:07.445121 140263261390720 learning.py:507] global step 2393: loss = 3.0706 (0.543 sec/step)\n",
            "I0707 17:00:07.778356 140263261390720 learning.py:507] global step 2394: loss = 3.7233 (0.332 sec/step)\n",
            "I0707 17:00:08.106225 140263261390720 learning.py:507] global step 2395: loss = 3.9406 (0.326 sec/step)\n",
            "I0707 17:00:08.474102 140263261390720 learning.py:507] global step 2396: loss = 4.1335 (0.366 sec/step)\n",
            "I0707 17:00:08.822990 140263261390720 learning.py:507] global step 2397: loss = 5.8417 (0.347 sec/step)\n",
            "I0707 17:00:09.142242 140263261390720 learning.py:507] global step 2398: loss = 4.2811 (0.318 sec/step)\n",
            "I0707 17:00:09.842342 140263261390720 learning.py:507] global step 2399: loss = 4.0654 (0.698 sec/step)\n",
            "I0707 17:00:10.152893 140263261390720 learning.py:507] global step 2400: loss = 4.6347 (0.309 sec/step)\n",
            "I0707 17:00:10.478196 140263261390720 learning.py:507] global step 2401: loss = 4.7411 (0.324 sec/step)\n",
            "I0707 17:00:10.791682 140263261390720 learning.py:507] global step 2402: loss = 4.8593 (0.312 sec/step)\n",
            "I0707 17:00:11.100884 140263261390720 learning.py:507] global step 2403: loss = 4.1970 (0.307 sec/step)\n",
            "I0707 17:00:11.413367 140263261390720 learning.py:507] global step 2404: loss = 2.8820 (0.310 sec/step)\n",
            "I0707 17:00:11.744879 140263261390720 learning.py:507] global step 2405: loss = 4.4549 (0.328 sec/step)\n",
            "I0707 17:00:12.055356 140263261390720 learning.py:507] global step 2406: loss = 3.8503 (0.308 sec/step)\n",
            "I0707 17:00:12.402884 140263261390720 learning.py:507] global step 2407: loss = 3.8081 (0.346 sec/step)\n",
            "I0707 17:00:12.741182 140263261390720 learning.py:507] global step 2408: loss = 5.4400 (0.337 sec/step)\n",
            "I0707 17:00:13.048302 140263261390720 learning.py:507] global step 2409: loss = 3.8080 (0.305 sec/step)\n",
            "I0707 17:00:13.407126 140263261390720 learning.py:507] global step 2410: loss = 4.3658 (0.357 sec/step)\n",
            "I0707 17:00:13.778447 140263261390720 learning.py:507] global step 2411: loss = 3.9243 (0.370 sec/step)\n",
            "I0707 17:00:14.084946 140263261390720 learning.py:507] global step 2412: loss = 5.4282 (0.305 sec/step)\n",
            "I0707 17:00:14.390900 140263261390720 learning.py:507] global step 2413: loss = 4.6828 (0.304 sec/step)\n",
            "I0707 17:00:14.758367 140263261390720 learning.py:507] global step 2414: loss = 4.0257 (0.366 sec/step)\n",
            "I0707 17:00:15.074593 140263261390720 learning.py:507] global step 2415: loss = 4.6417 (0.315 sec/step)\n",
            "I0707 17:00:15.486777 140263261390720 learning.py:507] global step 2416: loss = 4.1581 (0.411 sec/step)\n",
            "I0707 17:00:15.824704 140263261390720 learning.py:507] global step 2417: loss = 6.9741 (0.336 sec/step)\n",
            "I0707 17:00:16.135383 140263261390720 learning.py:507] global step 2418: loss = 5.0620 (0.309 sec/step)\n",
            "I0707 17:00:16.472933 140263261390720 learning.py:507] global step 2419: loss = 3.7597 (0.336 sec/step)\n",
            "I0707 17:00:16.767487 140263261390720 learning.py:507] global step 2420: loss = 3.2833 (0.293 sec/step)\n",
            "I0707 17:00:17.102208 140263261390720 learning.py:507] global step 2421: loss = 3.6936 (0.333 sec/step)\n",
            "I0707 17:00:17.442268 140263261390720 learning.py:507] global step 2422: loss = 4.7289 (0.338 sec/step)\n",
            "I0707 17:00:17.736417 140263261390720 learning.py:507] global step 2423: loss = 3.5738 (0.292 sec/step)\n",
            "I0707 17:00:18.078799 140263261390720 learning.py:507] global step 2424: loss = 4.8329 (0.341 sec/step)\n",
            "I0707 17:00:18.365733 140263261390720 learning.py:507] global step 2425: loss = 3.4070 (0.285 sec/step)\n",
            "I0707 17:00:18.692641 140263261390720 learning.py:507] global step 2426: loss = 3.1074 (0.325 sec/step)\n",
            "I0707 17:00:19.014420 140263261390720 learning.py:507] global step 2427: loss = 3.3282 (0.320 sec/step)\n",
            "I0707 17:00:19.332621 140263261390720 learning.py:507] global step 2428: loss = 3.8364 (0.317 sec/step)\n",
            "I0707 17:00:19.635403 140263261390720 learning.py:507] global step 2429: loss = 4.8115 (0.301 sec/step)\n",
            "I0707 17:00:19.992700 140263261390720 learning.py:507] global step 2430: loss = 4.0845 (0.355 sec/step)\n",
            "I0707 17:00:20.342704 140263261390720 learning.py:507] global step 2431: loss = 3.2360 (0.348 sec/step)\n",
            "I0707 17:00:20.639004 140263261390720 learning.py:507] global step 2432: loss = 3.9355 (0.295 sec/step)\n",
            "I0707 17:00:21.038373 140263261390720 learning.py:507] global step 2433: loss = 4.6169 (0.398 sec/step)\n",
            "I0707 17:00:21.336344 140263261390720 learning.py:507] global step 2434: loss = 4.0402 (0.296 sec/step)\n",
            "I0707 17:00:21.660176 140263261390720 learning.py:507] global step 2435: loss = 3.4116 (0.322 sec/step)\n",
            "I0707 17:00:21.963179 140263261390720 learning.py:507] global step 2436: loss = 2.9496 (0.301 sec/step)\n",
            "I0707 17:00:22.326151 140263261390720 learning.py:507] global step 2437: loss = 3.6481 (0.361 sec/step)\n",
            "I0707 17:00:22.637272 140263261390720 learning.py:507] global step 2438: loss = 6.6098 (0.309 sec/step)\n",
            "I0707 17:00:22.991912 140263261390720 learning.py:507] global step 2439: loss = 3.7165 (0.353 sec/step)\n",
            "I0707 17:00:23.345947 140263261390720 learning.py:507] global step 2440: loss = 5.9323 (0.352 sec/step)\n",
            "I0707 17:00:23.676604 140263261390720 learning.py:507] global step 2441: loss = 4.6914 (0.329 sec/step)\n",
            "I0707 17:00:24.132230 140263261390720 learning.py:507] global step 2442: loss = 4.7244 (0.454 sec/step)\n",
            "I0707 17:00:24.491390 140263261390720 learning.py:507] global step 2443: loss = 3.6255 (0.358 sec/step)\n",
            "I0707 17:00:24.909738 140263261390720 learning.py:507] global step 2444: loss = 3.4519 (0.417 sec/step)\n",
            "I0707 17:00:25.273645 140263261390720 learning.py:507] global step 2445: loss = 4.7018 (0.362 sec/step)\n",
            "I0707 17:00:25.578666 140263261390720 learning.py:507] global step 2446: loss = 4.9285 (0.304 sec/step)\n",
            "I0707 17:00:25.927944 140263261390720 learning.py:507] global step 2447: loss = 5.0963 (0.348 sec/step)\n",
            "I0707 17:00:26.261085 140263261390720 learning.py:507] global step 2448: loss = 3.3220 (0.331 sec/step)\n",
            "I0707 17:00:26.574317 140263261390720 learning.py:507] global step 2449: loss = 3.9021 (0.311 sec/step)\n",
            "I0707 17:00:26.897226 140263261390720 learning.py:507] global step 2450: loss = 5.0651 (0.321 sec/step)\n",
            "I0707 17:00:27.192647 140263261390720 learning.py:507] global step 2451: loss = 3.9053 (0.293 sec/step)\n",
            "I0707 17:00:27.547734 140263261390720 learning.py:507] global step 2452: loss = 5.8335 (0.353 sec/step)\n",
            "I0707 17:00:27.860362 140263261390720 learning.py:507] global step 2453: loss = 3.2851 (0.311 sec/step)\n",
            "I0707 17:00:28.217632 140263261390720 learning.py:507] global step 2454: loss = 4.9717 (0.356 sec/step)\n",
            "I0707 17:00:28.521567 140263261390720 learning.py:507] global step 2455: loss = 4.8719 (0.302 sec/step)\n",
            "I0707 17:00:28.904404 140263261390720 learning.py:507] global step 2456: loss = 3.9145 (0.381 sec/step)\n",
            "I0707 17:00:29.215149 140263261390720 learning.py:507] global step 2457: loss = 5.1661 (0.309 sec/step)\n",
            "I0707 17:00:29.536179 140263261390720 learning.py:507] global step 2458: loss = 3.5839 (0.319 sec/step)\n",
            "I0707 17:00:29.859633 140263261390720 learning.py:507] global step 2459: loss = 4.2811 (0.322 sec/step)\n",
            "I0707 17:00:30.255062 140263261390720 learning.py:507] global step 2460: loss = 4.0780 (0.394 sec/step)\n",
            "I0707 17:00:30.545636 140263261390720 learning.py:507] global step 2461: loss = 3.7558 (0.289 sec/step)\n",
            "I0707 17:00:31.044062 140263261390720 learning.py:507] global step 2462: loss = 5.6733 (0.497 sec/step)\n",
            "I0707 17:00:31.344866 140263261390720 learning.py:507] global step 2463: loss = 5.0943 (0.299 sec/step)\n",
            "I0707 17:00:31.651453 140263261390720 learning.py:507] global step 2464: loss = 2.5729 (0.305 sec/step)\n",
            "I0707 17:00:31.949453 140263261390720 learning.py:507] global step 2465: loss = 2.5212 (0.296 sec/step)\n",
            "I0707 17:00:32.290720 140263261390720 learning.py:507] global step 2466: loss = 4.7167 (0.340 sec/step)\n",
            "I0707 17:00:32.603725 140263261390720 learning.py:507] global step 2467: loss = 4.3286 (0.311 sec/step)\n",
            "I0707 17:00:33.253709 140263261390720 learning.py:507] global step 2468: loss = 4.8597 (0.648 sec/step)\n",
            "I0707 17:00:33.585003 140263261390720 learning.py:507] global step 2469: loss = 4.6487 (0.330 sec/step)\n",
            "I0707 17:00:33.953883 140263261390720 learning.py:507] global step 2470: loss = 4.6102 (0.367 sec/step)\n",
            "I0707 17:00:34.255673 140263261390720 learning.py:507] global step 2471: loss = 3.9178 (0.300 sec/step)\n",
            "I0707 17:00:34.806327 140263261390720 learning.py:507] global step 2472: loss = 3.8688 (0.549 sec/step)\n",
            "I0707 17:00:35.095848 140263261390720 learning.py:507] global step 2473: loss = 4.0396 (0.287 sec/step)\n",
            "I0707 17:00:35.401989 140263261390720 learning.py:507] global step 2474: loss = 5.2090 (0.303 sec/step)\n",
            "I0707 17:00:35.719129 140263261390720 learning.py:507] global step 2475: loss = 3.5983 (0.315 sec/step)\n",
            "I0707 17:00:36.167763 140263261390720 learning.py:507] global step 2476: loss = 3.7716 (0.447 sec/step)\n",
            "I0707 17:00:36.497031 140263261390720 learning.py:507] global step 2477: loss = 5.2319 (0.327 sec/step)\n",
            "I0707 17:00:37.213213 140263261390720 learning.py:507] global step 2478: loss = 4.4418 (0.714 sec/step)\n",
            "I0707 17:00:37.534960 140263261390720 learning.py:507] global step 2479: loss = 3.9597 (0.320 sec/step)\n",
            "I0707 17:00:37.859731 140263261390720 learning.py:507] global step 2480: loss = 4.0862 (0.321 sec/step)\n",
            "I0707 17:00:38.264152 140263261390720 learning.py:507] global step 2481: loss = 3.8211 (0.403 sec/step)\n",
            "I0707 17:00:38.630150 140263261390720 learning.py:507] global step 2482: loss = 4.1734 (0.364 sec/step)\n",
            "I0707 17:00:38.960682 140263261390720 learning.py:507] global step 2483: loss = 4.2391 (0.329 sec/step)\n",
            "I0707 17:00:39.295308 140263261390720 learning.py:507] global step 2484: loss = 3.8123 (0.333 sec/step)\n",
            "I0707 17:00:39.595183 140263261390720 learning.py:507] global step 2485: loss = 5.0129 (0.298 sec/step)\n",
            "I0707 17:00:39.898577 140263261390720 learning.py:507] global step 2486: loss = 4.5157 (0.301 sec/step)\n",
            "I0707 17:00:40.203955 140263261390720 learning.py:507] global step 2487: loss = 4.0309 (0.304 sec/step)\n",
            "I0707 17:00:40.517377 140263261390720 learning.py:507] global step 2488: loss = 3.7980 (0.312 sec/step)\n",
            "I0707 17:00:40.941332 140263261390720 learning.py:507] global step 2489: loss = 3.7790 (0.422 sec/step)\n",
            "I0707 17:00:41.236682 140263261390720 learning.py:507] global step 2490: loss = 5.0506 (0.294 sec/step)\n",
            "I0707 17:00:41.541339 140263261390720 learning.py:507] global step 2491: loss = 5.9015 (0.303 sec/step)\n",
            "I0707 17:00:41.881668 140263261390720 learning.py:507] global step 2492: loss = 4.4368 (0.338 sec/step)\n",
            "I0707 17:00:42.204978 140263261390720 learning.py:507] global step 2493: loss = 5.0006 (0.320 sec/step)\n",
            "I0707 17:00:42.527229 140263261390720 learning.py:507] global step 2494: loss = 3.2133 (0.321 sec/step)\n",
            "I0707 17:00:42.833613 140263261390720 learning.py:507] global step 2495: loss = 3.8004 (0.305 sec/step)\n",
            "I0707 17:00:43.128602 140263261390720 learning.py:507] global step 2496: loss = 3.3873 (0.293 sec/step)\n",
            "I0707 17:00:43.426705 140263261390720 learning.py:507] global step 2497: loss = 4.6786 (0.297 sec/step)\n",
            "I0707 17:00:43.730015 140263261390720 learning.py:507] global step 2498: loss = 6.3588 (0.301 sec/step)\n",
            "I0707 17:00:44.047252 140263261390720 learning.py:507] global step 2499: loss = 4.8302 (0.316 sec/step)\n",
            "I0707 17:00:44.354370 140263261390720 learning.py:507] global step 2500: loss = 3.1013 (0.306 sec/step)\n",
            "I0707 17:00:44.656485 140263261390720 learning.py:507] global step 2501: loss = 5.3747 (0.301 sec/step)\n",
            "I0707 17:00:45.060344 140263261390720 learning.py:507] global step 2502: loss = 4.9226 (0.402 sec/step)\n",
            "I0707 17:00:45.391530 140263261390720 learning.py:507] global step 2503: loss = 2.9467 (0.330 sec/step)\n",
            "I0707 17:00:45.804683 140263261390720 learning.py:507] global step 2504: loss = 3.9764 (0.411 sec/step)\n",
            "I0707 17:00:46.105347 140263261390720 learning.py:507] global step 2505: loss = 4.7023 (0.299 sec/step)\n",
            "I0707 17:00:46.408178 140263261390720 learning.py:507] global step 2506: loss = 4.2138 (0.301 sec/step)\n",
            "I0707 17:00:46.746666 140263261390720 learning.py:507] global step 2507: loss = 5.4323 (0.337 sec/step)\n",
            "I0707 17:00:47.093908 140263261390720 learning.py:507] global step 2508: loss = 3.3588 (0.346 sec/step)\n",
            "I0707 17:00:47.528946 140263261390720 learning.py:507] global step 2509: loss = 4.0565 (0.433 sec/step)\n",
            "I0707 17:00:47.948228 140263261390720 learning.py:507] global step 2510: loss = 4.7351 (0.418 sec/step)\n",
            "I0707 17:00:48.304762 140263261390720 learning.py:507] global step 2511: loss = 4.9777 (0.355 sec/step)\n",
            "I0707 17:00:48.679255 140263261390720 learning.py:507] global step 2512: loss = 3.0748 (0.373 sec/step)\n",
            "I0707 17:00:48.999670 140263261390720 learning.py:507] global step 2513: loss = 3.9645 (0.319 sec/step)\n",
            "I0707 17:00:49.332123 140263261390720 learning.py:507] global step 2514: loss = 4.4570 (0.331 sec/step)\n",
            "I0707 17:00:49.770231 140263261390720 learning.py:507] global step 2515: loss = 4.0987 (0.436 sec/step)\n",
            "I0707 17:00:50.103424 140263261390720 learning.py:507] global step 2516: loss = 6.2969 (0.332 sec/step)\n",
            "I0707 17:00:50.427444 140263261390720 learning.py:507] global step 2517: loss = 4.1624 (0.322 sec/step)\n",
            "I0707 17:00:50.757444 140263261390720 learning.py:507] global step 2518: loss = 3.8334 (0.328 sec/step)\n",
            "I0707 17:00:51.073532 140263261390720 learning.py:507] global step 2519: loss = 5.3056 (0.314 sec/step)\n",
            "I0707 17:00:51.368046 140263261390720 learning.py:507] global step 2520: loss = 4.2819 (0.293 sec/step)\n",
            "I0707 17:00:51.702660 140263261390720 learning.py:507] global step 2521: loss = 3.9278 (0.333 sec/step)\n",
            "I0707 17:00:52.060750 140263261390720 learning.py:507] global step 2522: loss = 3.4229 (0.356 sec/step)\n",
            "I0707 17:00:52.358460 140263261390720 learning.py:507] global step 2523: loss = 3.9027 (0.296 sec/step)\n",
            "I0707 17:00:52.638598 140263261390720 learning.py:507] global step 2524: loss = 4.0299 (0.279 sec/step)\n",
            "I0707 17:00:52.936331 140263261390720 learning.py:507] global step 2525: loss = 2.8862 (0.296 sec/step)\n",
            "I0707 17:00:53.280471 140263261390720 learning.py:507] global step 2526: loss = 4.9407 (0.343 sec/step)\n",
            "I0707 17:00:53.586098 140263261390720 learning.py:507] global step 2527: loss = 3.7415 (0.304 sec/step)\n",
            "I0707 17:00:53.903672 140263261390720 learning.py:507] global step 2528: loss = 2.7694 (0.316 sec/step)\n",
            "I0707 17:00:54.206770 140263261390720 learning.py:507] global step 2529: loss = 4.6988 (0.301 sec/step)\n",
            "I0707 17:00:54.662851 140263261390720 learning.py:507] global step 2530: loss = 4.1733 (0.454 sec/step)\n",
            "I0707 17:00:55.024600 140263261390720 learning.py:507] global step 2531: loss = 5.3638 (0.360 sec/step)\n",
            "I0707 17:00:55.400727 140263261390720 learning.py:507] global step 2532: loss = 4.0915 (0.374 sec/step)\n",
            "I0707 17:00:55.718936 140263261390720 learning.py:507] global step 2533: loss = 4.2301 (0.316 sec/step)\n",
            "I0707 17:00:56.038068 140263261390720 learning.py:507] global step 2534: loss = 4.2494 (0.317 sec/step)\n",
            "I0707 17:00:56.346147 140263261390720 learning.py:507] global step 2535: loss = 3.5722 (0.306 sec/step)\n",
            "I0707 17:00:56.925609 140263261390720 learning.py:507] global step 2536: loss = 5.5610 (0.577 sec/step)\n",
            "I0707 17:00:57.344903 140263261390720 learning.py:507] global step 2537: loss = 4.7696 (0.418 sec/step)\n",
            "I0707 17:00:57.719769 140263261390720 learning.py:507] global step 2538: loss = 5.5125 (0.373 sec/step)\n",
            "I0707 17:00:58.055949 140263261390720 learning.py:507] global step 2539: loss = 3.8738 (0.334 sec/step)\n",
            "I0707 17:00:58.414595 140263261390720 learning.py:507] global step 2540: loss = 3.4933 (0.357 sec/step)\n",
            "I0707 17:00:58.730269 140263261390720 learning.py:507] global step 2541: loss = 3.5536 (0.314 sec/step)\n",
            "I0707 17:00:59.046809 140263261390720 learning.py:507] global step 2542: loss = 3.7950 (0.315 sec/step)\n",
            "I0707 17:00:59.371745 140263261390720 learning.py:507] global step 2543: loss = 4.8013 (0.323 sec/step)\n",
            "I0707 17:00:59.682428 140263261390720 learning.py:507] global step 2544: loss = 4.5405 (0.309 sec/step)\n",
            "I0707 17:01:00.094318 140263261390720 learning.py:507] global step 2545: loss = 3.3490 (0.410 sec/step)\n",
            "I0707 17:01:00.418058 140263261390720 learning.py:507] global step 2546: loss = 2.9431 (0.322 sec/step)\n",
            "I0707 17:01:00.797989 140263261390720 learning.py:507] global step 2547: loss = 3.3288 (0.378 sec/step)\n",
            "I0707 17:01:01.212922 140263261390720 learning.py:507] global step 2548: loss = 4.2888 (0.412 sec/step)\n",
            "I0707 17:01:01.568521 140263261390720 learning.py:507] global step 2549: loss = 2.7604 (0.354 sec/step)\n",
            "I0707 17:01:01.955588 140263261390720 learning.py:507] global step 2550: loss = 3.1768 (0.385 sec/step)\n",
            "I0707 17:01:02.289451 140263261390720 learning.py:507] global step 2551: loss = 5.0865 (0.332 sec/step)\n",
            "I0707 17:01:02.604485 140263261390720 learning.py:507] global step 2552: loss = 4.1483 (0.313 sec/step)\n",
            "I0707 17:01:02.928353 140263261390720 learning.py:507] global step 2553: loss = 4.1899 (0.322 sec/step)\n",
            "I0707 17:01:03.327184 140263261390720 learning.py:507] global step 2554: loss = 5.4884 (0.397 sec/step)\n",
            "I0707 17:01:03.731122 140263261390720 learning.py:507] global step 2555: loss = 3.5087 (0.402 sec/step)\n",
            "I0707 17:01:04.104221 140263261390720 learning.py:507] global step 2556: loss = 3.4859 (0.372 sec/step)\n",
            "I0707 17:01:04.400344 140263261390720 learning.py:507] global step 2557: loss = 5.3317 (0.294 sec/step)\n",
            "I0707 17:01:04.758226 140263261390720 learning.py:507] global step 2558: loss = 4.2332 (0.356 sec/step)\n",
            "I0707 17:01:05.071564 140263261390720 learning.py:507] global step 2559: loss = 4.9705 (0.312 sec/step)\n",
            "I0707 17:01:05.390797 140263261390720 learning.py:507] global step 2560: loss = 4.8017 (0.318 sec/step)\n",
            "I0707 17:01:05.691351 140263261390720 learning.py:507] global step 2561: loss = 4.4919 (0.299 sec/step)\n",
            "I0707 17:01:06.005285 140263261390720 learning.py:507] global step 2562: loss = 3.9843 (0.312 sec/step)\n",
            "I0707 17:01:06.346707 140263261390720 learning.py:507] global step 2563: loss = 3.5099 (0.340 sec/step)\n",
            "I0707 17:01:06.653974 140263261390720 learning.py:507] global step 2564: loss = 4.4222 (0.306 sec/step)\n",
            "I0707 17:01:06.960114 140263261390720 learning.py:507] global step 2565: loss = 4.1731 (0.304 sec/step)\n",
            "I0707 17:01:07.283604 140263261390720 learning.py:507] global step 2566: loss = 3.4460 (0.322 sec/step)\n",
            "I0707 17:01:07.610917 140263261390720 learning.py:507] global step 2567: loss = 4.6861 (0.326 sec/step)\n",
            "I0707 17:01:08.006475 140263261390720 learning.py:507] global step 2568: loss = 3.5066 (0.394 sec/step)\n",
            "I0707 17:01:08.451565 140263261390720 learning.py:507] global step 2569: loss = 3.9765 (0.443 sec/step)\n",
            "I0707 17:01:08.752072 140263261390720 learning.py:507] global step 2570: loss = 4.3118 (0.299 sec/step)\n",
            "I0707 17:01:09.048435 140263261390720 learning.py:507] global step 2571: loss = 5.0307 (0.294 sec/step)\n",
            "I0707 17:01:09.345281 140263261390720 learning.py:507] global step 2572: loss = 4.8155 (0.295 sec/step)\n",
            "I0707 17:01:09.703145 140263261390720 learning.py:507] global step 2573: loss = 3.8892 (0.356 sec/step)\n",
            "I0707 17:01:10.127343 140263261390720 learning.py:507] global step 2574: loss = 4.2007 (0.423 sec/step)\n",
            "I0707 17:01:10.448350 140263261390720 learning.py:507] global step 2575: loss = 3.9627 (0.319 sec/step)\n",
            "I0707 17:01:10.854291 140263261390720 learning.py:507] global step 2576: loss = 5.6041 (0.404 sec/step)\n",
            "I0707 17:01:11.169307 140263261390720 learning.py:507] global step 2577: loss = 3.1594 (0.313 sec/step)\n",
            "I0707 17:01:11.605068 140263261390720 learning.py:507] global step 2578: loss = 4.5216 (0.434 sec/step)\n",
            "I0707 17:01:11.892968 140263261390720 learning.py:507] global step 2579: loss = 2.5457 (0.286 sec/step)\n",
            "I0707 17:01:12.386426 140263261390720 learning.py:507] global step 2580: loss = 2.8883 (0.491 sec/step)\n",
            "I0707 17:01:12.695404 140263261390720 learning.py:507] global step 2581: loss = 4.3181 (0.307 sec/step)\n",
            "I0707 17:01:13.049319 140263261390720 learning.py:507] global step 2582: loss = 4.0560 (0.351 sec/step)\n",
            "I0707 17:01:13.382122 140263261390720 learning.py:507] global step 2583: loss = 4.2623 (0.330 sec/step)\n",
            "I0707 17:01:13.820091 140263261390720 learning.py:507] global step 2584: loss = 3.1121 (0.436 sec/step)\n",
            "I0707 17:01:14.138315 140263261390720 learning.py:507] global step 2585: loss = 4.9859 (0.317 sec/step)\n",
            "I0707 17:01:14.478968 140263261390720 learning.py:507] global step 2586: loss = 3.6272 (0.339 sec/step)\n",
            "I0707 17:01:14.792926 140263261390720 learning.py:507] global step 2587: loss = 3.8505 (0.312 sec/step)\n",
            "I0707 17:01:15.097016 140263261390720 learning.py:507] global step 2588: loss = 4.9133 (0.303 sec/step)\n",
            "I0707 17:01:15.393959 140263261390720 learning.py:507] global step 2589: loss = 4.2686 (0.295 sec/step)\n",
            "I0707 17:01:15.763765 140263261390720 learning.py:507] global step 2590: loss = 3.8364 (0.368 sec/step)\n",
            "I0707 17:01:16.059399 140263261390720 learning.py:507] global step 2591: loss = 5.7835 (0.294 sec/step)\n",
            "I0707 17:01:16.374572 140263261390720 learning.py:507] global step 2592: loss = 2.9741 (0.314 sec/step)\n",
            "I0707 17:01:16.692988 140263261390720 learning.py:507] global step 2593: loss = 2.6603 (0.317 sec/step)\n",
            "I0707 17:01:17.000676 140263261390720 learning.py:507] global step 2594: loss = 5.3377 (0.306 sec/step)\n",
            "I0707 17:01:17.284273 140263261390720 learning.py:507] global step 2595: loss = 3.3899 (0.282 sec/step)\n",
            "I0707 17:01:17.637891 140263261390720 learning.py:507] global step 2596: loss = 5.6590 (0.352 sec/step)\n",
            "I0707 17:01:17.956027 140263261390720 learning.py:507] global step 2597: loss = 3.7816 (0.317 sec/step)\n",
            "I0707 17:01:18.392094 140263261390720 learning.py:507] global step 2598: loss = 2.9390 (0.434 sec/step)\n",
            "I0707 17:01:18.711206 140263261390720 learning.py:507] global step 2599: loss = 4.4543 (0.318 sec/step)\n",
            "I0707 17:01:19.032159 140263261390720 learning.py:507] global step 2600: loss = 4.0390 (0.319 sec/step)\n",
            "I0707 17:01:19.328567 140263261390720 learning.py:507] global step 2601: loss = 4.6985 (0.295 sec/step)\n",
            "I0707 17:01:19.751361 140263261390720 learning.py:507] global step 2602: loss = 3.4389 (0.421 sec/step)\n",
            "I0707 17:01:20.073182 140263261390720 learning.py:507] global step 2603: loss = 2.9905 (0.320 sec/step)\n",
            "I0707 17:01:20.599242 140263261390720 learning.py:507] global step 2604: loss = 4.5883 (0.524 sec/step)\n",
            "I0707 17:01:20.909673 140263261390720 learning.py:507] global step 2605: loss = 4.3892 (0.309 sec/step)\n",
            "I0707 17:01:21.209178 140263261390720 learning.py:507] global step 2606: loss = 4.3343 (0.298 sec/step)\n",
            "I0707 17:01:21.494889 140263261390720 learning.py:507] global step 2607: loss = 4.0601 (0.284 sec/step)\n",
            "I0707 17:01:21.857717 140263261390720 learning.py:507] global step 2608: loss = 3.3163 (0.361 sec/step)\n",
            "I0707 17:01:22.196027 140263261390720 learning.py:507] global step 2609: loss = 3.8082 (0.337 sec/step)\n",
            "I0707 17:01:22.529633 140263261390720 learning.py:507] global step 2610: loss = 3.6923 (0.332 sec/step)\n",
            "I0707 17:01:22.826790 140263261390720 learning.py:507] global step 2611: loss = 4.0713 (0.296 sec/step)\n",
            "I0707 17:01:23.124892 140263261390720 learning.py:507] global step 2612: loss = 2.8615 (0.296 sec/step)\n",
            "I0707 17:01:23.468790 140263261390720 learning.py:507] global step 2613: loss = 3.1201 (0.342 sec/step)\n",
            "I0707 17:01:23.781993 140263261390720 learning.py:507] global step 2614: loss = 4.3989 (0.311 sec/step)\n",
            "I0707 17:01:24.086985 140263261390720 learning.py:507] global step 2615: loss = 4.2458 (0.303 sec/step)\n",
            "I0707 17:01:24.450244 140263261390720 learning.py:507] global step 2616: loss = 4.3979 (0.362 sec/step)\n",
            "I0707 17:01:24.774893 140263261390720 learning.py:507] global step 2617: loss = 4.5090 (0.323 sec/step)\n",
            "I0707 17:01:25.073532 140263261390720 learning.py:507] global step 2618: loss = 4.0440 (0.297 sec/step)\n",
            "I0707 17:01:25.382536 140263261390720 learning.py:507] global step 2619: loss = 2.8715 (0.307 sec/step)\n",
            "I0707 17:01:25.724485 140263261390720 learning.py:507] global step 2620: loss = 5.0437 (0.340 sec/step)\n",
            "I0707 17:01:26.075449 140263261390720 learning.py:507] global step 2621: loss = 3.6196 (0.349 sec/step)\n",
            "I0707 17:01:26.382748 140263261390720 learning.py:507] global step 2622: loss = 5.0190 (0.306 sec/step)\n",
            "I0707 17:01:26.685302 140263261390720 learning.py:507] global step 2623: loss = 5.3289 (0.300 sec/step)\n",
            "I0707 17:01:26.995841 140263261390720 learning.py:507] global step 2624: loss = 3.1612 (0.309 sec/step)\n",
            "I0707 17:01:27.447218 140263261390720 learning.py:507] global step 2625: loss = 5.0333 (0.450 sec/step)\n",
            "I0707 17:01:27.745750 140263261390720 learning.py:507] global step 2626: loss = 4.1411 (0.297 sec/step)\n",
            "I0707 17:01:28.063796 140263261390720 learning.py:507] global step 2627: loss = 3.9074 (0.316 sec/step)\n",
            "I0707 17:01:28.405543 140263261390720 learning.py:507] global step 2628: loss = 2.9909 (0.340 sec/step)\n",
            "I0707 17:01:28.737165 140263261390720 learning.py:507] global step 2629: loss = 4.3324 (0.330 sec/step)\n",
            "I0707 17:01:29.038967 140263261390720 learning.py:507] global step 2630: loss = 4.4049 (0.300 sec/step)\n",
            "I0707 17:01:29.375251 140263261390720 learning.py:507] global step 2631: loss = 5.6463 (0.335 sec/step)\n",
            "I0707 17:01:29.692524 140263261390720 learning.py:507] global step 2632: loss = 3.6075 (0.316 sec/step)\n",
            "I0707 17:01:30.146988 140263261390720 learning.py:507] global step 2633: loss = 4.9083 (0.453 sec/step)\n",
            "I0707 17:01:30.482390 140263261390720 learning.py:507] global step 2634: loss = 4.4441 (0.334 sec/step)\n",
            "I0707 17:01:30.788904 140263261390720 learning.py:507] global step 2635: loss = 4.4241 (0.305 sec/step)\n",
            "I0707 17:01:31.120967 140263261390720 learning.py:507] global step 2636: loss = 5.9098 (0.330 sec/step)\n",
            "I0707 17:01:31.429214 140263261390720 learning.py:507] global step 2637: loss = 4.3154 (0.306 sec/step)\n",
            "I0707 17:01:31.731443 140263261390720 learning.py:507] global step 2638: loss = 4.4812 (0.301 sec/step)\n",
            "I0707 17:01:32.106522 140263261390720 learning.py:507] global step 2639: loss = 4.7133 (0.373 sec/step)\n",
            "I0707 17:01:32.424673 140263261390720 learning.py:507] global step 2640: loss = 4.7985 (0.316 sec/step)\n",
            "I0707 17:01:32.758210 140263261390720 learning.py:507] global step 2641: loss = 4.4527 (0.332 sec/step)\n",
            "I0707 17:01:33.070198 140263261390720 learning.py:507] global step 2642: loss = 3.8100 (0.310 sec/step)\n",
            "I0707 17:01:33.374403 140263261390720 learning.py:507] global step 2643: loss = 4.5186 (0.303 sec/step)\n",
            "I0707 17:01:33.671048 140263261390720 learning.py:507] global step 2644: loss = 4.5093 (0.295 sec/step)\n",
            "I0707 17:01:34.057207 140263261390720 learning.py:507] global step 2645: loss = 5.7857 (0.384 sec/step)\n",
            "I0707 17:01:34.406849 140263261390720 learning.py:507] global step 2646: loss = 5.0799 (0.348 sec/step)\n",
            "I0707 17:01:34.747102 140263261390720 learning.py:507] global step 2647: loss = 3.8427 (0.338 sec/step)\n",
            "I0707 17:01:35.118638 140263261390720 learning.py:507] global step 2648: loss = 4.4479 (0.369 sec/step)\n",
            "I0707 17:01:35.553946 140263261390720 learning.py:507] global step 2649: loss = 6.1937 (0.433 sec/step)\n",
            "I0707 17:01:35.884471 140263261390720 learning.py:507] global step 2650: loss = 5.5841 (0.329 sec/step)\n",
            "I0707 17:01:36.250817 140263261390720 learning.py:507] global step 2651: loss = 6.0423 (0.365 sec/step)\n",
            "I0707 17:01:36.554262 140263261390720 learning.py:507] global step 2652: loss = 4.6425 (0.302 sec/step)\n",
            "I0707 17:01:37.048020 140263261390720 learning.py:507] global step 2653: loss = 2.8888 (0.492 sec/step)\n",
            "I0707 17:01:37.464681 140263261390720 learning.py:507] global step 2654: loss = 3.7128 (0.415 sec/step)\n",
            "I0707 17:01:37.803494 140263261390720 learning.py:507] global step 2655: loss = 4.7115 (0.337 sec/step)\n",
            "I0707 17:01:38.136340 140263261390720 learning.py:507] global step 2656: loss = 5.1549 (0.331 sec/step)\n",
            "I0707 17:01:38.440153 140263261390720 learning.py:507] global step 2657: loss = 3.9961 (0.302 sec/step)\n",
            "I0707 17:01:38.851993 140263261390720 learning.py:507] global step 2658: loss = 4.2349 (0.410 sec/step)\n",
            "I0707 17:01:39.336397 140263261390720 learning.py:507] global step 2659: loss = 3.9670 (0.482 sec/step)\n",
            "I0707 17:01:39.657307 140263261390720 learning.py:507] global step 2660: loss = 6.0127 (0.315 sec/step)\n",
            "I0707 17:01:39.976625 140263261390720 learning.py:507] global step 2661: loss = 4.6775 (0.317 sec/step)\n",
            "I0707 17:01:40.323149 140263261390720 learning.py:507] global step 2662: loss = 3.5515 (0.345 sec/step)\n",
            "I0707 17:01:40.657483 140263261390720 learning.py:507] global step 2663: loss = 4.0532 (0.333 sec/step)\n",
            "I0707 17:01:40.971774 140263261390720 learning.py:507] global step 2664: loss = 4.3327 (0.313 sec/step)\n",
            "I0707 17:01:41.380019 140263261390720 learning.py:507] global step 2665: loss = 3.5914 (0.407 sec/step)\n",
            "I0707 17:01:41.703907 140263261390720 learning.py:507] global step 2666: loss = 3.2261 (0.322 sec/step)\n",
            "I0707 17:01:42.027703 140263261390720 learning.py:507] global step 2667: loss = 2.8251 (0.322 sec/step)\n",
            "I0707 17:01:42.375053 140263261390720 learning.py:507] global step 2668: loss = 5.5927 (0.346 sec/step)\n",
            "I0707 17:01:42.712629 140263261390720 learning.py:507] global step 2669: loss = 3.0844 (0.336 sec/step)\n",
            "I0707 17:01:42.997059 140263261390720 learning.py:507] global step 2670: loss = 6.0218 (0.283 sec/step)\n",
            "I0707 17:01:43.347156 140263261390720 learning.py:507] global step 2671: loss = 4.6617 (0.348 sec/step)\n",
            "I0707 17:01:43.664183 140263261390720 learning.py:507] global step 2672: loss = 6.2876 (0.315 sec/step)\n",
            "I0707 17:01:44.116622 140263261390720 learning.py:507] global step 2673: loss = 4.3866 (0.451 sec/step)\n",
            "I0707 17:01:44.436866 140263261390720 learning.py:507] global step 2674: loss = 3.6447 (0.319 sec/step)\n",
            "I0707 17:01:44.787948 140263261390720 learning.py:507] global step 2675: loss = 4.9580 (0.349 sec/step)\n",
            "I0707 17:01:45.138524 140263261390720 learning.py:507] global step 2676: loss = 5.6336 (0.349 sec/step)\n",
            "I0707 17:01:45.451232 140263261390720 learning.py:507] global step 2677: loss = 4.8890 (0.311 sec/step)\n",
            "I0707 17:01:45.822268 140263261390720 learning.py:507] global step 2678: loss = 3.9170 (0.369 sec/step)\n",
            "I0707 17:01:46.351871 140263261390720 learning.py:507] global step 2679: loss = 3.8728 (0.528 sec/step)\n",
            "I0707 17:01:46.666985 140263261390720 learning.py:507] global step 2680: loss = 2.9690 (0.314 sec/step)\n",
            "I0707 17:01:46.969812 140263261390720 learning.py:507] global step 2681: loss = 4.7140 (0.301 sec/step)\n",
            "I0707 17:01:47.259447 140263261390720 learning.py:507] global step 2682: loss = 5.1462 (0.288 sec/step)\n",
            "I0707 17:01:47.607454 140263261390720 learning.py:507] global step 2683: loss = 4.9648 (0.346 sec/step)\n",
            "I0707 17:01:48.053135 140263261390720 learning.py:507] global step 2684: loss = 4.2956 (0.444 sec/step)\n",
            "I0707 17:01:48.374190 140263261390720 learning.py:507] global step 2685: loss = 4.4203 (0.319 sec/step)\n",
            "I0707 17:01:48.688312 140263261390720 learning.py:507] global step 2686: loss = 4.4843 (0.312 sec/step)\n",
            "I0707 17:01:49.032711 140263261390720 learning.py:507] global step 2687: loss = 4.6821 (0.343 sec/step)\n",
            "I0707 17:01:49.364451 140263261390720 learning.py:507] global step 2688: loss = 3.7893 (0.330 sec/step)\n",
            "I0707 17:01:49.728045 140263261390720 learning.py:507] global step 2689: loss = 3.7940 (0.362 sec/step)\n",
            "I0707 17:01:50.046587 140263261390720 learning.py:507] global step 2690: loss = 4.3293 (0.317 sec/step)\n",
            "I0707 17:01:50.327492 140263261390720 learning.py:507] global step 2691: loss = 4.8159 (0.279 sec/step)\n",
            "I0707 17:01:50.679588 140263261390720 learning.py:507] global step 2692: loss = 5.7415 (0.350 sec/step)\n",
            "I0707 17:01:50.997476 140263261390720 learning.py:507] global step 2693: loss = 3.2274 (0.316 sec/step)\n",
            "I0707 17:01:51.352415 140263261390720 learning.py:507] global step 2694: loss = 4.0627 (0.353 sec/step)\n",
            "I0707 17:01:51.676547 140263261390720 learning.py:507] global step 2695: loss = 4.4318 (0.322 sec/step)\n",
            "I0707 17:01:52.027591 140263261390720 learning.py:507] global step 2696: loss = 5.6196 (0.349 sec/step)\n",
            "I0707 17:01:52.328258 140263261390720 learning.py:507] global step 2697: loss = 4.7916 (0.299 sec/step)\n",
            "I0707 17:01:52.635965 140263261390720 learning.py:507] global step 2698: loss = 3.7975 (0.306 sec/step)\n",
            "I0707 17:01:52.966871 140263261390720 learning.py:507] global step 2699: loss = 4.0643 (0.329 sec/step)\n",
            "I0707 17:01:53.585690 140263261390720 learning.py:507] global step 2700: loss = 3.8100 (0.539 sec/step)\n",
            "I0707 17:01:53.884505 140260287772416 supervisor.py:1050] Recording summary at step 2700.\n",
            "I0707 17:01:53.981278 140263261390720 learning.py:507] global step 2701: loss = 4.1312 (0.394 sec/step)\n",
            "I0707 17:01:54.308201 140263261390720 learning.py:507] global step 2702: loss = 5.4371 (0.325 sec/step)\n",
            "I0707 17:01:54.601676 140263261390720 learning.py:507] global step 2703: loss = 4.2569 (0.292 sec/step)\n",
            "I0707 17:01:54.889270 140260296165120 supervisor.py:1099] global_step/sec: 2.88224\n",
            "I0707 17:01:54.993599 140263261390720 learning.py:507] global step 2704: loss = 4.3166 (0.390 sec/step)\n",
            "I0707 17:01:55.301606 140263261390720 learning.py:507] global step 2705: loss = 4.2745 (0.306 sec/step)\n",
            "I0707 17:01:55.600796 140263261390720 learning.py:507] global step 2706: loss = 4.2035 (0.297 sec/step)\n",
            "I0707 17:01:55.939921 140263261390720 learning.py:507] global step 2707: loss = 2.9411 (0.337 sec/step)\n",
            "I0707 17:01:56.298408 140263261390720 learning.py:507] global step 2708: loss = 4.5811 (0.357 sec/step)\n",
            "I0707 17:01:56.605447 140263261390720 learning.py:507] global step 2709: loss = 4.3399 (0.305 sec/step)\n",
            "I0707 17:01:56.909796 140263261390720 learning.py:507] global step 2710: loss = 4.0525 (0.303 sec/step)\n",
            "I0707 17:01:57.217706 140263261390720 learning.py:507] global step 2711: loss = 3.3266 (0.306 sec/step)\n",
            "I0707 17:01:57.567466 140263261390720 learning.py:507] global step 2712: loss = 3.9994 (0.348 sec/step)\n",
            "I0707 17:01:57.880851 140263261390720 learning.py:507] global step 2713: loss = 2.9277 (0.312 sec/step)\n",
            "I0707 17:01:58.286720 140263261390720 learning.py:507] global step 2714: loss = 4.7776 (0.404 sec/step)\n",
            "I0707 17:01:58.628007 140263261390720 learning.py:507] global step 2715: loss = 6.4169 (0.340 sec/step)\n",
            "I0707 17:01:59.067467 140263261390720 learning.py:507] global step 2716: loss = 4.6814 (0.438 sec/step)\n",
            "I0707 17:01:59.398941 140263261390720 learning.py:507] global step 2717: loss = 6.4315 (0.330 sec/step)\n",
            "I0707 17:01:59.794077 140263261390720 learning.py:507] global step 2718: loss = 4.6914 (0.394 sec/step)\n",
            "I0707 17:02:00.117524 140263261390720 learning.py:507] global step 2719: loss = 3.0483 (0.322 sec/step)\n",
            "I0707 17:02:00.515317 140263261390720 learning.py:507] global step 2720: loss = 4.5051 (0.396 sec/step)\n",
            "I0707 17:02:00.820816 140263261390720 learning.py:507] global step 2721: loss = 8.0969 (0.304 sec/step)\n",
            "I0707 17:02:01.136332 140263261390720 learning.py:507] global step 2722: loss = 3.8648 (0.314 sec/step)\n",
            "I0707 17:02:01.514420 140263261390720 learning.py:507] global step 2723: loss = 3.7570 (0.376 sec/step)\n",
            "I0707 17:02:01.820923 140263261390720 learning.py:507] global step 2724: loss = 4.0574 (0.305 sec/step)\n",
            "I0707 17:02:02.121743 140263261390720 learning.py:507] global step 2725: loss = 3.8444 (0.299 sec/step)\n",
            "I0707 17:02:02.425662 140263261390720 learning.py:507] global step 2726: loss = 3.9087 (0.302 sec/step)\n",
            "I0707 17:02:02.736158 140263261390720 learning.py:507] global step 2727: loss = 4.3750 (0.308 sec/step)\n",
            "I0707 17:02:03.027026 140263261390720 learning.py:507] global step 2728: loss = 3.2689 (0.289 sec/step)\n",
            "I0707 17:02:03.399993 140263261390720 learning.py:507] global step 2729: loss = 2.9270 (0.371 sec/step)\n",
            "I0707 17:02:03.726908 140263261390720 learning.py:507] global step 2730: loss = 4.1923 (0.325 sec/step)\n",
            "I0707 17:02:04.022337 140263261390720 learning.py:507] global step 2731: loss = 3.9740 (0.294 sec/step)\n",
            "I0707 17:02:04.370795 140263261390720 learning.py:507] global step 2732: loss = 4.8120 (0.347 sec/step)\n",
            "I0707 17:02:04.700358 140263261390720 learning.py:507] global step 2733: loss = 5.4751 (0.328 sec/step)\n",
            "I0707 17:02:04.999002 140263261390720 learning.py:507] global step 2734: loss = 6.8708 (0.297 sec/step)\n",
            "I0707 17:02:05.369694 140263261390720 learning.py:507] global step 2735: loss = 3.3230 (0.369 sec/step)\n",
            "I0707 17:02:05.661855 140263261390720 learning.py:507] global step 2736: loss = 3.8379 (0.291 sec/step)\n",
            "I0707 17:02:06.000566 140263261390720 learning.py:507] global step 2737: loss = 3.0988 (0.337 sec/step)\n",
            "I0707 17:02:06.359203 140263261390720 learning.py:507] global step 2738: loss = 4.6171 (0.357 sec/step)\n",
            "I0707 17:02:06.693930 140263261390720 learning.py:507] global step 2739: loss = 4.7115 (0.333 sec/step)\n",
            "I0707 17:02:06.988741 140263261390720 learning.py:507] global step 2740: loss = 4.0947 (0.293 sec/step)\n",
            "I0707 17:02:07.374935 140263261390720 learning.py:507] global step 2741: loss = 4.8998 (0.385 sec/step)\n",
            "I0707 17:02:07.666412 140263261390720 learning.py:507] global step 2742: loss = 4.2494 (0.290 sec/step)\n",
            "I0707 17:02:08.033686 140263261390720 learning.py:507] global step 2743: loss = 2.7984 (0.366 sec/step)\n",
            "I0707 17:02:08.427231 140263261390720 learning.py:507] global step 2744: loss = 4.4751 (0.392 sec/step)\n",
            "I0707 17:02:08.839086 140263261390720 learning.py:507] global step 2745: loss = 3.3940 (0.410 sec/step)\n",
            "I0707 17:02:09.243304 140263261390720 learning.py:507] global step 2746: loss = 4.4691 (0.400 sec/step)\n",
            "I0707 17:02:09.538546 140263261390720 learning.py:507] global step 2747: loss = 3.4069 (0.294 sec/step)\n",
            "I0707 17:02:09.840652 140263261390720 learning.py:507] global step 2748: loss = 4.1770 (0.300 sec/step)\n",
            "I0707 17:02:10.165177 140263261390720 learning.py:507] global step 2749: loss = 4.2157 (0.323 sec/step)\n",
            "I0707 17:02:10.512646 140263261390720 learning.py:507] global step 2750: loss = 5.7497 (0.346 sec/step)\n",
            "I0707 17:02:10.867527 140263261390720 learning.py:507] global step 2751: loss = 3.6360 (0.353 sec/step)\n",
            "I0707 17:02:11.183700 140263261390720 learning.py:507] global step 2752: loss = 3.9204 (0.314 sec/step)\n",
            "I0707 17:02:11.641297 140263261390720 learning.py:507] global step 2753: loss = 3.1540 (0.456 sec/step)\n",
            "I0707 17:02:12.074661 140263261390720 learning.py:507] global step 2754: loss = 4.1924 (0.432 sec/step)\n",
            "I0707 17:02:12.438524 140263261390720 learning.py:507] global step 2755: loss = 3.7592 (0.362 sec/step)\n",
            "I0707 17:02:12.759374 140263261390720 learning.py:507] global step 2756: loss = 4.1773 (0.319 sec/step)\n",
            "I0707 17:02:13.078118 140263261390720 learning.py:507] global step 2757: loss = 4.1603 (0.317 sec/step)\n",
            "I0707 17:02:13.461289 140263261390720 learning.py:507] global step 2758: loss = 3.8669 (0.382 sec/step)\n",
            "I0707 17:02:13.776978 140263261390720 learning.py:507] global step 2759: loss = 3.8307 (0.314 sec/step)\n",
            "I0707 17:02:14.085909 140263261390720 learning.py:507] global step 2760: loss = 4.3923 (0.307 sec/step)\n",
            "I0707 17:02:14.440450 140263261390720 learning.py:507] global step 2761: loss = 4.1450 (0.353 sec/step)\n",
            "I0707 17:02:14.734346 140263261390720 learning.py:507] global step 2762: loss = 4.5268 (0.292 sec/step)\n",
            "I0707 17:02:15.046199 140263261390720 learning.py:507] global step 2763: loss = 3.8774 (0.310 sec/step)\n",
            "I0707 17:02:15.480130 140263261390720 learning.py:507] global step 2764: loss = 5.5537 (0.432 sec/step)\n",
            "I0707 17:02:15.797047 140263261390720 learning.py:507] global step 2765: loss = 6.4744 (0.315 sec/step)\n",
            "I0707 17:02:16.116641 140263261390720 learning.py:507] global step 2766: loss = 3.8298 (0.318 sec/step)\n",
            "I0707 17:02:16.435122 140263261390720 learning.py:507] global step 2767: loss = 4.1528 (0.317 sec/step)\n",
            "I0707 17:02:16.750051 140263261390720 learning.py:507] global step 2768: loss = 4.3183 (0.313 sec/step)\n",
            "I0707 17:02:17.066657 140263261390720 learning.py:507] global step 2769: loss = 4.0700 (0.315 sec/step)\n",
            "I0707 17:02:17.352369 140263261390720 learning.py:507] global step 2770: loss = 7.1857 (0.283 sec/step)\n",
            "I0707 17:02:17.656979 140263261390720 learning.py:507] global step 2771: loss = 4.2667 (0.302 sec/step)\n",
            "I0707 17:02:17.953002 140263261390720 learning.py:507] global step 2772: loss = 4.4817 (0.294 sec/step)\n",
            "I0707 17:02:18.336202 140263261390720 learning.py:507] global step 2773: loss = 5.5170 (0.381 sec/step)\n",
            "I0707 17:02:18.668401 140263261390720 learning.py:507] global step 2774: loss = 4.7160 (0.330 sec/step)\n",
            "I0707 17:02:18.980806 140263261390720 learning.py:507] global step 2775: loss = 5.5902 (0.311 sec/step)\n",
            "I0707 17:02:19.258113 140263261390720 learning.py:507] global step 2776: loss = 4.4205 (0.276 sec/step)\n",
            "I0707 17:02:19.546782 140263261390720 learning.py:507] global step 2777: loss = 5.4427 (0.287 sec/step)\n",
            "I0707 17:02:19.864694 140263261390720 learning.py:507] global step 2778: loss = 5.6451 (0.316 sec/step)\n",
            "I0707 17:02:20.301897 140263261390720 learning.py:507] global step 2779: loss = 5.2948 (0.435 sec/step)\n",
            "I0707 17:02:20.670993 140263261390720 learning.py:507] global step 2780: loss = 4.0229 (0.367 sec/step)\n",
            "I0707 17:02:20.983194 140263261390720 learning.py:507] global step 2781: loss = 3.6586 (0.311 sec/step)\n",
            "I0707 17:02:21.257299 140263261390720 learning.py:507] global step 2782: loss = 4.1678 (0.272 sec/step)\n",
            "I0707 17:02:21.614330 140263261390720 learning.py:507] global step 2783: loss = 3.5458 (0.355 sec/step)\n",
            "I0707 17:02:21.900435 140263261390720 learning.py:507] global step 2784: loss = 5.4121 (0.284 sec/step)\n",
            "I0707 17:02:22.332073 140263261390720 learning.py:507] global step 2785: loss = 4.6487 (0.430 sec/step)\n",
            "I0707 17:02:22.678469 140263261390720 learning.py:507] global step 2786: loss = 3.9861 (0.345 sec/step)\n",
            "I0707 17:02:22.995980 140263261390720 learning.py:507] global step 2787: loss = 2.6857 (0.316 sec/step)\n",
            "I0707 17:02:23.295503 140263261390720 learning.py:507] global step 2788: loss = 5.3861 (0.298 sec/step)\n",
            "I0707 17:02:23.716538 140263261390720 learning.py:507] global step 2789: loss = 3.3417 (0.419 sec/step)\n",
            "I0707 17:02:24.001537 140263261390720 learning.py:507] global step 2790: loss = 5.1741 (0.281 sec/step)\n",
            "I0707 17:02:24.430990 140263261390720 learning.py:507] global step 2791: loss = 4.7230 (0.428 sec/step)\n",
            "I0707 17:02:24.766564 140263261390720 learning.py:507] global step 2792: loss = 4.2858 (0.334 sec/step)\n",
            "I0707 17:02:25.181503 140263261390720 learning.py:507] global step 2793: loss = 4.8921 (0.413 sec/step)\n",
            "I0707 17:02:25.520251 140263261390720 learning.py:507] global step 2794: loss = 5.6524 (0.337 sec/step)\n",
            "I0707 17:02:25.846717 140263261390720 learning.py:507] global step 2795: loss = 3.5078 (0.325 sec/step)\n",
            "I0707 17:02:26.142897 140263261390720 learning.py:507] global step 2796: loss = 3.2163 (0.294 sec/step)\n",
            "I0707 17:02:26.445811 140263261390720 learning.py:507] global step 2797: loss = 4.0600 (0.301 sec/step)\n",
            "I0707 17:02:26.746755 140263261390720 learning.py:507] global step 2798: loss = 3.0673 (0.299 sec/step)\n",
            "I0707 17:02:27.257375 140263261390720 learning.py:507] global step 2799: loss = 5.3105 (0.509 sec/step)\n",
            "I0707 17:02:27.572898 140263261390720 learning.py:507] global step 2800: loss = 4.3060 (0.314 sec/step)\n",
            "I0707 17:02:27.905468 140263261390720 learning.py:507] global step 2801: loss = 3.5653 (0.331 sec/step)\n",
            "I0707 17:02:28.191875 140263261390720 learning.py:507] global step 2802: loss = 4.0977 (0.285 sec/step)\n",
            "I0707 17:02:28.538090 140263261390720 learning.py:507] global step 2803: loss = 6.5917 (0.344 sec/step)\n",
            "I0707 17:02:28.964166 140263261390720 learning.py:507] global step 2804: loss = 4.3842 (0.424 sec/step)\n",
            "I0707 17:02:29.263937 140263261390720 learning.py:507] global step 2805: loss = 4.6082 (0.298 sec/step)\n",
            "I0707 17:02:29.569599 140263261390720 learning.py:507] global step 2806: loss = 4.3313 (0.304 sec/step)\n",
            "I0707 17:02:29.999758 140263261390720 learning.py:507] global step 2807: loss = 4.2002 (0.428 sec/step)\n",
            "I0707 17:02:30.300868 140263261390720 learning.py:507] global step 2808: loss = 3.7784 (0.299 sec/step)\n",
            "I0707 17:02:30.610242 140263261390720 learning.py:507] global step 2809: loss = 2.5835 (0.308 sec/step)\n",
            "I0707 17:02:30.912348 140263261390720 learning.py:507] global step 2810: loss = 4.7819 (0.300 sec/step)\n",
            "I0707 17:02:31.314006 140263261390720 learning.py:507] global step 2811: loss = 4.5044 (0.400 sec/step)\n",
            "I0707 17:02:31.716530 140263261390720 learning.py:507] global step 2812: loss = 4.2777 (0.401 sec/step)\n",
            "I0707 17:02:32.177275 140263261390720 learning.py:507] global step 2813: loss = 4.9860 (0.459 sec/step)\n",
            "I0707 17:02:32.488487 140263261390720 learning.py:507] global step 2814: loss = 5.6958 (0.310 sec/step)\n",
            "I0707 17:02:32.791506 140263261390720 learning.py:507] global step 2815: loss = 5.0822 (0.301 sec/step)\n",
            "I0707 17:02:33.100013 140263261390720 learning.py:507] global step 2816: loss = 4.1045 (0.307 sec/step)\n",
            "I0707 17:02:33.404892 140263261390720 learning.py:507] global step 2817: loss = 3.3332 (0.303 sec/step)\n",
            "I0707 17:02:33.736871 140263261390720 learning.py:507] global step 2818: loss = 3.7069 (0.330 sec/step)\n",
            "I0707 17:02:34.460296 140263261390720 learning.py:507] global step 2819: loss = 4.3586 (0.722 sec/step)\n",
            "I0707 17:02:34.812304 140263261390720 learning.py:507] global step 2820: loss = 5.9926 (0.350 sec/step)\n",
            "I0707 17:02:35.136551 140263261390720 learning.py:507] global step 2821: loss = 4.9276 (0.323 sec/step)\n",
            "I0707 17:02:35.491952 140263261390720 learning.py:507] global step 2822: loss = 5.5532 (0.354 sec/step)\n",
            "I0707 17:02:35.882343 140263261390720 learning.py:507] global step 2823: loss = 3.5713 (0.389 sec/step)\n",
            "I0707 17:02:36.191217 140263261390720 learning.py:507] global step 2824: loss = 4.4780 (0.307 sec/step)\n",
            "I0707 17:02:36.640374 140263261390720 learning.py:507] global step 2825: loss = 5.8146 (0.447 sec/step)\n",
            "I0707 17:02:36.989610 140263261390720 learning.py:507] global step 2826: loss = 3.9970 (0.348 sec/step)\n",
            "I0707 17:02:37.461163 140263261390720 learning.py:507] global step 2827: loss = 3.2371 (0.470 sec/step)\n",
            "I0707 17:02:37.802082 140263261390720 learning.py:507] global step 2828: loss = 4.1709 (0.339 sec/step)\n",
            "I0707 17:02:38.106291 140263261390720 learning.py:507] global step 2829: loss = 4.7662 (0.302 sec/step)\n",
            "I0707 17:02:38.418271 140263261390720 learning.py:507] global step 2830: loss = 3.7407 (0.310 sec/step)\n",
            "I0707 17:02:38.827837 140263261390720 learning.py:507] global step 2831: loss = 4.5692 (0.408 sec/step)\n",
            "I0707 17:02:39.262629 140263261390720 learning.py:507] global step 2832: loss = 3.4098 (0.433 sec/step)\n",
            "I0707 17:02:39.773670 140263261390720 learning.py:507] global step 2833: loss = 3.8503 (0.509 sec/step)\n",
            "I0707 17:02:40.064878 140263261390720 learning.py:507] global step 2834: loss = 3.6111 (0.290 sec/step)\n",
            "I0707 17:02:40.378493 140263261390720 learning.py:507] global step 2835: loss = 3.4160 (0.312 sec/step)\n",
            "I0707 17:02:40.700699 140263261390720 learning.py:507] global step 2836: loss = 3.7137 (0.320 sec/step)\n",
            "I0707 17:02:41.008629 140263261390720 learning.py:507] global step 2837: loss = 4.1761 (0.306 sec/step)\n",
            "I0707 17:02:41.520921 140263261390720 learning.py:507] global step 2838: loss = 4.2915 (0.510 sec/step)\n",
            "I0707 17:02:41.806814 140263261390720 learning.py:507] global step 2839: loss = 4.9136 (0.284 sec/step)\n",
            "I0707 17:02:42.113263 140263261390720 learning.py:507] global step 2840: loss = 4.3601 (0.305 sec/step)\n",
            "I0707 17:02:42.419971 140263261390720 learning.py:507] global step 2841: loss = 5.9750 (0.305 sec/step)\n",
            "I0707 17:02:42.743700 140263261390720 learning.py:507] global step 2842: loss = 4.1158 (0.322 sec/step)\n",
            "I0707 17:02:43.047990 140263261390720 learning.py:507] global step 2843: loss = 4.1733 (0.302 sec/step)\n",
            "I0707 17:02:43.491261 140263261390720 learning.py:507] global step 2844: loss = 4.9815 (0.441 sec/step)\n",
            "I0707 17:02:43.797265 140263261390720 learning.py:507] global step 2845: loss = 3.3322 (0.304 sec/step)\n",
            "I0707 17:02:44.112174 140263261390720 learning.py:507] global step 2846: loss = 4.3370 (0.313 sec/step)\n",
            "I0707 17:02:44.451936 140263261390720 learning.py:507] global step 2847: loss = 3.9983 (0.338 sec/step)\n",
            "I0707 17:02:44.767188 140263261390720 learning.py:507] global step 2848: loss = 5.9977 (0.314 sec/step)\n",
            "I0707 17:02:45.066927 140263261390720 learning.py:507] global step 2849: loss = 4.7451 (0.298 sec/step)\n",
            "I0707 17:02:45.407613 140263261390720 learning.py:507] global step 2850: loss = 4.0308 (0.339 sec/step)\n",
            "I0707 17:02:45.835414 140263261390720 learning.py:507] global step 2851: loss = 5.1601 (0.426 sec/step)\n",
            "I0707 17:02:46.149669 140263261390720 learning.py:507] global step 2852: loss = 4.5274 (0.313 sec/step)\n",
            "I0707 17:02:46.530566 140263261390720 learning.py:507] global step 2853: loss = 3.6641 (0.379 sec/step)\n",
            "I0707 17:02:46.893448 140263261390720 learning.py:507] global step 2854: loss = 4.1656 (0.361 sec/step)\n",
            "I0707 17:02:47.183048 140263261390720 learning.py:507] global step 2855: loss = 4.4008 (0.288 sec/step)\n",
            "I0707 17:02:47.512382 140263261390720 learning.py:507] global step 2856: loss = 4.3363 (0.328 sec/step)\n",
            "I0707 17:02:48.046117 140263261390720 learning.py:507] global step 2857: loss = 5.9847 (0.532 sec/step)\n",
            "I0707 17:02:48.341871 140263261390720 learning.py:507] global step 2858: loss = 3.4369 (0.293 sec/step)\n",
            "I0707 17:02:48.660140 140263261390720 learning.py:507] global step 2859: loss = 4.3851 (0.316 sec/step)\n",
            "I0707 17:02:48.988650 140263261390720 learning.py:507] global step 2860: loss = 6.9611 (0.327 sec/step)\n",
            "I0707 17:02:49.300636 140263261390720 learning.py:507] global step 2861: loss = 4.8411 (0.310 sec/step)\n",
            "I0707 17:02:49.620289 140263261390720 learning.py:507] global step 2862: loss = 3.8143 (0.318 sec/step)\n",
            "I0707 17:02:49.915340 140263261390720 learning.py:507] global step 2863: loss = 4.3336 (0.293 sec/step)\n",
            "I0707 17:02:50.239743 140263261390720 learning.py:507] global step 2864: loss = 4.8133 (0.322 sec/step)\n",
            "I0707 17:02:50.558665 140263261390720 learning.py:507] global step 2865: loss = 3.8215 (0.317 sec/step)\n",
            "I0707 17:02:50.852932 140263261390720 learning.py:507] global step 2866: loss = 3.9531 (0.292 sec/step)\n",
            "I0707 17:02:51.150682 140263261390720 learning.py:507] global step 2867: loss = 4.9998 (0.296 sec/step)\n",
            "I0707 17:02:51.457200 140263261390720 learning.py:507] global step 2868: loss = 4.3743 (0.304 sec/step)\n",
            "I0707 17:02:51.750273 140263261390720 learning.py:507] global step 2869: loss = 4.6562 (0.292 sec/step)\n",
            "I0707 17:02:52.081512 140263261390720 learning.py:507] global step 2870: loss = 3.4896 (0.330 sec/step)\n",
            "I0707 17:02:52.407009 140263261390720 learning.py:507] global step 2871: loss = 4.8367 (0.324 sec/step)\n",
            "I0707 17:02:52.754963 140263261390720 learning.py:507] global step 2872: loss = 3.9926 (0.346 sec/step)\n",
            "I0707 17:02:53.047524 140263261390720 learning.py:507] global step 2873: loss = 3.4359 (0.291 sec/step)\n",
            "I0707 17:02:53.402017 140263261390720 learning.py:507] global step 2874: loss = 2.9686 (0.353 sec/step)\n",
            "I0707 17:02:53.723226 140263261390720 learning.py:507] global step 2875: loss = 3.6364 (0.319 sec/step)\n",
            "I0707 17:02:54.064381 140263261390720 learning.py:507] global step 2876: loss = 3.7061 (0.340 sec/step)\n",
            "I0707 17:02:54.377104 140263261390720 learning.py:507] global step 2877: loss = 3.9989 (0.311 sec/step)\n",
            "I0707 17:02:54.679036 140263261390720 learning.py:507] global step 2878: loss = 3.3373 (0.300 sec/step)\n",
            "I0707 17:02:55.131805 140263261390720 learning.py:507] global step 2879: loss = 5.8280 (0.451 sec/step)\n",
            "I0707 17:02:55.532519 140263261390720 learning.py:507] global step 2880: loss = 3.5560 (0.399 sec/step)\n",
            "I0707 17:02:55.858393 140263261390720 learning.py:507] global step 2881: loss = 4.1767 (0.324 sec/step)\n",
            "I0707 17:02:56.150626 140263261390720 learning.py:507] global step 2882: loss = 4.3991 (0.290 sec/step)\n",
            "I0707 17:02:56.476473 140263261390720 learning.py:507] global step 2883: loss = 4.5848 (0.324 sec/step)\n",
            "I0707 17:02:56.843171 140263261390720 learning.py:507] global step 2884: loss = 2.7762 (0.365 sec/step)\n",
            "I0707 17:02:57.185303 140263261390720 learning.py:507] global step 2885: loss = 3.2577 (0.340 sec/step)\n",
            "I0707 17:02:57.545323 140263261390720 learning.py:507] global step 2886: loss = 4.6003 (0.358 sec/step)\n",
            "I0707 17:02:58.004913 140263261390720 learning.py:507] global step 2887: loss = 3.3538 (0.458 sec/step)\n",
            "I0707 17:02:58.344703 140263261390720 learning.py:507] global step 2888: loss = 6.0334 (0.338 sec/step)\n",
            "I0707 17:02:58.642405 140263261390720 learning.py:507] global step 2889: loss = 4.9677 (0.296 sec/step)\n",
            "I0707 17:02:58.969278 140263261390720 learning.py:507] global step 2890: loss = 3.7381 (0.325 sec/step)\n",
            "I0707 17:02:59.356564 140263261390720 learning.py:507] global step 2891: loss = 4.2857 (0.386 sec/step)\n",
            "I0707 17:02:59.771971 140263261390720 learning.py:507] global step 2892: loss = 5.1295 (0.414 sec/step)\n",
            "I0707 17:03:00.171306 140263261390720 learning.py:507] global step 2893: loss = 4.6416 (0.398 sec/step)\n",
            "I0707 17:03:00.506821 140263261390720 learning.py:507] global step 2894: loss = 4.7529 (0.334 sec/step)\n",
            "I0707 17:03:00.811270 140263261390720 learning.py:507] global step 2895: loss = 4.3337 (0.303 sec/step)\n",
            "I0707 17:03:01.189075 140263261390720 learning.py:507] global step 2896: loss = 4.2476 (0.376 sec/step)\n",
            "I0707 17:03:01.655265 140263261390720 learning.py:507] global step 2897: loss = 5.3973 (0.464 sec/step)\n",
            "I0707 17:03:02.081686 140263261390720 learning.py:507] global step 2898: loss = 4.7523 (0.424 sec/step)\n",
            "I0707 17:03:02.407794 140263261390720 learning.py:507] global step 2899: loss = 4.5701 (0.324 sec/step)\n",
            "I0707 17:03:02.749555 140263261390720 learning.py:507] global step 2900: loss = 4.4276 (0.340 sec/step)\n",
            "I0707 17:03:03.145076 140263261390720 learning.py:507] global step 2901: loss = 4.9330 (0.394 sec/step)\n",
            "I0707 17:03:03.523702 140263261390720 learning.py:507] global step 2902: loss = 3.6379 (0.377 sec/step)\n",
            "I0707 17:03:03.851843 140263261390720 learning.py:507] global step 2903: loss = 3.9978 (0.327 sec/step)\n",
            "I0707 17:03:04.168789 140263261390720 learning.py:507] global step 2904: loss = 5.5823 (0.315 sec/step)\n",
            "I0707 17:03:04.575898 140263261390720 learning.py:507] global step 2905: loss = 2.7447 (0.405 sec/step)\n",
            "I0707 17:03:04.930820 140263261390720 learning.py:507] global step 2906: loss = 4.3700 (0.353 sec/step)\n",
            "I0707 17:03:05.582863 140263261390720 learning.py:507] global step 2907: loss = 3.2012 (0.650 sec/step)\n",
            "I0707 17:03:05.893289 140263261390720 learning.py:507] global step 2908: loss = 4.1042 (0.308 sec/step)\n",
            "I0707 17:03:06.207678 140263261390720 learning.py:507] global step 2909: loss = 3.7502 (0.311 sec/step)\n",
            "I0707 17:03:06.525720 140263261390720 learning.py:507] global step 2910: loss = 3.3338 (0.316 sec/step)\n",
            "I0707 17:03:06.886192 140263261390720 learning.py:507] global step 2911: loss = 4.8211 (0.359 sec/step)\n",
            "I0707 17:03:07.193280 140263261390720 learning.py:507] global step 2912: loss = 3.6017 (0.305 sec/step)\n",
            "I0707 17:03:07.879570 140263261390720 learning.py:507] global step 2913: loss = 4.8101 (0.685 sec/step)\n",
            "I0707 17:03:08.281879 140263261390720 learning.py:507] global step 2914: loss = 3.2674 (0.401 sec/step)\n",
            "I0707 17:03:08.584631 140263261390720 learning.py:507] global step 2915: loss = 4.7702 (0.301 sec/step)\n",
            "I0707 17:03:08.907851 140263261390720 learning.py:507] global step 2916: loss = 3.9629 (0.322 sec/step)\n",
            "I0707 17:03:09.282004 140263261390720 learning.py:507] global step 2917: loss = 3.0436 (0.372 sec/step)\n",
            "I0707 17:03:09.587996 140263261390720 learning.py:507] global step 2918: loss = 3.9212 (0.304 sec/step)\n",
            "I0707 17:03:09.895328 140263261390720 learning.py:507] global step 2919: loss = 3.3816 (0.306 sec/step)\n",
            "I0707 17:03:10.330586 140263261390720 learning.py:507] global step 2920: loss = 5.1184 (0.434 sec/step)\n",
            "I0707 17:03:10.743708 140263261390720 learning.py:507] global step 2921: loss = 3.5719 (0.411 sec/step)\n",
            "I0707 17:03:11.072962 140263261390720 learning.py:507] global step 2922: loss = 4.4145 (0.328 sec/step)\n",
            "I0707 17:03:11.378214 140263261390720 learning.py:507] global step 2923: loss = 4.7807 (0.304 sec/step)\n",
            "I0707 17:03:11.696546 140263261390720 learning.py:507] global step 2924: loss = 5.3040 (0.317 sec/step)\n",
            "I0707 17:03:12.077377 140263261390720 learning.py:507] global step 2925: loss = 3.9437 (0.378 sec/step)\n",
            "I0707 17:03:12.511396 140263261390720 learning.py:507] global step 2926: loss = 5.1363 (0.431 sec/step)\n",
            "I0707 17:03:12.941383 140263261390720 learning.py:507] global step 2927: loss = 3.8851 (0.428 sec/step)\n",
            "I0707 17:03:13.260324 140263261390720 learning.py:507] global step 2928: loss = 3.8757 (0.317 sec/step)\n",
            "I0707 17:03:13.599073 140263261390720 learning.py:507] global step 2929: loss = 2.8278 (0.337 sec/step)\n",
            "I0707 17:03:13.927882 140263261390720 learning.py:507] global step 2930: loss = 4.7082 (0.327 sec/step)\n",
            "I0707 17:03:14.385311 140263261390720 learning.py:507] global step 2931: loss = 2.6667 (0.456 sec/step)\n",
            "I0707 17:03:14.727894 140263261390720 learning.py:507] global step 2932: loss = 2.9846 (0.341 sec/step)\n",
            "I0707 17:03:15.028685 140263261390720 learning.py:507] global step 2933: loss = 3.2167 (0.299 sec/step)\n",
            "I0707 17:03:15.330112 140263261390720 learning.py:507] global step 2934: loss = 3.8061 (0.300 sec/step)\n",
            "I0707 17:03:15.623866 140263261390720 learning.py:507] global step 2935: loss = 3.5557 (0.292 sec/step)\n",
            "I0707 17:03:15.916452 140263261390720 learning.py:507] global step 2936: loss = 3.9281 (0.291 sec/step)\n",
            "I0707 17:03:16.250463 140263261390720 learning.py:507] global step 2937: loss = 3.5088 (0.332 sec/step)\n",
            "I0707 17:03:16.561308 140263261390720 learning.py:507] global step 2938: loss = 2.7033 (0.309 sec/step)\n",
            "I0707 17:03:16.859999 140263261390720 learning.py:507] global step 2939: loss = 4.9063 (0.297 sec/step)\n",
            "I0707 17:03:17.168873 140263261390720 learning.py:507] global step 2940: loss = 4.8981 (0.307 sec/step)\n",
            "I0707 17:03:17.509212 140263261390720 learning.py:507] global step 2941: loss = 4.0833 (0.339 sec/step)\n",
            "I0707 17:03:17.808314 140263261390720 learning.py:507] global step 2942: loss = 4.1810 (0.297 sec/step)\n",
            "I0707 17:03:18.100609 140263261390720 learning.py:507] global step 2943: loss = 3.2987 (0.290 sec/step)\n",
            "I0707 17:03:18.430163 140263261390720 learning.py:507] global step 2944: loss = 4.9574 (0.328 sec/step)\n",
            "I0707 17:03:18.742818 140263261390720 learning.py:507] global step 2945: loss = 4.6568 (0.311 sec/step)\n",
            "I0707 17:03:19.072276 140263261390720 learning.py:507] global step 2946: loss = 2.9635 (0.328 sec/step)\n",
            "I0707 17:03:19.397507 140263261390720 learning.py:507] global step 2947: loss = 4.0715 (0.324 sec/step)\n",
            "I0707 17:03:19.706785 140263261390720 learning.py:507] global step 2948: loss = 4.3916 (0.308 sec/step)\n",
            "I0707 17:03:20.009018 140263261390720 learning.py:507] global step 2949: loss = 4.5687 (0.301 sec/step)\n",
            "I0707 17:03:20.377145 140263261390720 learning.py:507] global step 2950: loss = 3.7426 (0.366 sec/step)\n",
            "I0707 17:03:20.672080 140263261390720 learning.py:507] global step 2951: loss = 3.7073 (0.293 sec/step)\n",
            "I0707 17:03:21.006380 140263261390720 learning.py:507] global step 2952: loss = 6.9062 (0.332 sec/step)\n",
            "I0707 17:03:21.303953 140263261390720 learning.py:507] global step 2953: loss = 3.9773 (0.296 sec/step)\n",
            "I0707 17:03:21.653734 140263261390720 learning.py:507] global step 2954: loss = 4.9065 (0.348 sec/step)\n",
            "I0707 17:03:21.964592 140263261390720 learning.py:507] global step 2955: loss = 3.9327 (0.309 sec/step)\n",
            "I0707 17:03:22.294781 140263261390720 learning.py:507] global step 2956: loss = 6.3033 (0.328 sec/step)\n",
            "I0707 17:03:22.637573 140263261390720 learning.py:507] global step 2957: loss = 3.1373 (0.341 sec/step)\n",
            "I0707 17:03:22.941905 140263261390720 learning.py:507] global step 2958: loss = 4.1211 (0.303 sec/step)\n",
            "I0707 17:03:23.276812 140263261390720 learning.py:507] global step 2959: loss = 4.5068 (0.333 sec/step)\n",
            "I0707 17:03:23.614173 140263261390720 learning.py:507] global step 2960: loss = 3.7143 (0.336 sec/step)\n",
            "I0707 17:03:23.921177 140263261390720 learning.py:507] global step 2961: loss = 4.0921 (0.305 sec/step)\n",
            "I0707 17:03:24.202333 140263261390720 learning.py:507] global step 2962: loss = 4.9322 (0.280 sec/step)\n",
            "I0707 17:03:24.664168 140263261390720 learning.py:507] global step 2963: loss = 3.3297 (0.460 sec/step)\n",
            "I0707 17:03:24.977908 140263261390720 learning.py:507] global step 2964: loss = 4.4656 (0.312 sec/step)\n",
            "I0707 17:03:25.293249 140263261390720 learning.py:507] global step 2965: loss = 3.7072 (0.314 sec/step)\n",
            "I0707 17:03:25.631413 140263261390720 learning.py:507] global step 2966: loss = 4.4922 (0.337 sec/step)\n",
            "I0707 17:03:25.970195 140263261390720 learning.py:507] global step 2967: loss = 5.7178 (0.337 sec/step)\n",
            "I0707 17:03:26.272444 140263261390720 learning.py:507] global step 2968: loss = 3.6999 (0.301 sec/step)\n",
            "I0707 17:03:26.568046 140263261390720 learning.py:507] global step 2969: loss = 4.7158 (0.294 sec/step)\n",
            "I0707 17:03:26.864076 140263261390720 learning.py:507] global step 2970: loss = 3.6108 (0.294 sec/step)\n",
            "I0707 17:03:27.239026 140263261390720 learning.py:507] global step 2971: loss = 4.1055 (0.373 sec/step)\n",
            "I0707 17:03:27.563739 140263261390720 learning.py:507] global step 2972: loss = 3.9445 (0.323 sec/step)\n",
            "I0707 17:03:27.875249 140263261390720 learning.py:507] global step 2973: loss = 4.5429 (0.310 sec/step)\n",
            "I0707 17:03:28.177122 140263261390720 learning.py:507] global step 2974: loss = 3.7721 (0.300 sec/step)\n",
            "I0707 17:03:28.488777 140263261390720 learning.py:507] global step 2975: loss = 4.7266 (0.310 sec/step)\n",
            "I0707 17:03:28.826941 140263261390720 learning.py:507] global step 2976: loss = 4.0429 (0.336 sec/step)\n",
            "I0707 17:03:29.139691 140263261390720 learning.py:507] global step 2977: loss = 3.2632 (0.311 sec/step)\n",
            "I0707 17:03:29.450839 140263261390720 learning.py:507] global step 2978: loss = 2.8922 (0.310 sec/step)\n",
            "I0707 17:03:29.867390 140263261390720 learning.py:507] global step 2979: loss = 5.4546 (0.415 sec/step)\n",
            "I0707 17:03:30.187903 140263261390720 learning.py:507] global step 2980: loss = 4.2084 (0.319 sec/step)\n",
            "I0707 17:03:30.497923 140263261390720 learning.py:507] global step 2981: loss = 4.8755 (0.308 sec/step)\n",
            "I0707 17:03:30.850767 140263261390720 learning.py:507] global step 2982: loss = 4.4543 (0.351 sec/step)\n",
            "I0707 17:03:31.161357 140263261390720 learning.py:507] global step 2983: loss = 3.6816 (0.309 sec/step)\n",
            "I0707 17:03:31.480223 140263261390720 learning.py:507] global step 2984: loss = 4.3971 (0.317 sec/step)\n",
            "I0707 17:03:31.801729 140263261390720 learning.py:507] global step 2985: loss = 3.8005 (0.320 sec/step)\n",
            "I0707 17:03:32.084084 140263261390720 learning.py:507] global step 2986: loss = 5.2937 (0.281 sec/step)\n",
            "I0707 17:03:32.473541 140263261390720 learning.py:507] global step 2987: loss = 4.9495 (0.388 sec/step)\n",
            "I0707 17:03:32.774911 140263261390720 learning.py:507] global step 2988: loss = 3.0648 (0.298 sec/step)\n",
            "I0707 17:03:33.106409 140263261390720 learning.py:507] global step 2989: loss = 4.4887 (0.329 sec/step)\n",
            "I0707 17:03:33.415181 140263261390720 learning.py:507] global step 2990: loss = 3.5081 (0.307 sec/step)\n",
            "I0707 17:03:33.770694 140263261390720 learning.py:507] global step 2991: loss = 3.7432 (0.354 sec/step)\n",
            "I0707 17:03:34.102908 140263261390720 learning.py:507] global step 2992: loss = 2.8699 (0.331 sec/step)\n",
            "I0707 17:03:34.416400 140263261390720 learning.py:507] global step 2993: loss = 3.4474 (0.312 sec/step)\n",
            "I0707 17:03:34.704151 140263261390720 learning.py:507] global step 2994: loss = 5.9896 (0.286 sec/step)\n",
            "I0707 17:03:35.044887 140263261390720 learning.py:507] global step 2995: loss = 3.7052 (0.339 sec/step)\n",
            "I0707 17:03:35.338251 140263261390720 learning.py:507] global step 2996: loss = 4.4923 (0.292 sec/step)\n",
            "I0707 17:03:35.836821 140263261390720 learning.py:507] global step 2997: loss = 4.0443 (0.497 sec/step)\n",
            "I0707 17:03:36.172720 140263261390720 learning.py:507] global step 2998: loss = 3.1198 (0.334 sec/step)\n",
            "I0707 17:03:36.470144 140263261390720 learning.py:507] global step 2999: loss = 4.5843 (0.296 sec/step)\n",
            "I0707 17:03:36.782640 140263261390720 learning.py:507] global step 3000: loss = 4.4977 (0.311 sec/step)\n",
            "I0707 17:03:37.116595 140263261390720 learning.py:507] global step 3001: loss = 4.8809 (0.332 sec/step)\n",
            "I0707 17:03:37.489445 140263261390720 learning.py:507] global step 3002: loss = 4.2899 (0.371 sec/step)\n",
            "I0707 17:03:37.788591 140263261390720 learning.py:507] global step 3003: loss = 4.9953 (0.297 sec/step)\n",
            "I0707 17:03:38.100477 140263261390720 learning.py:507] global step 3004: loss = 4.1407 (0.310 sec/step)\n",
            "I0707 17:03:38.417466 140263261390720 learning.py:507] global step 3005: loss = 4.8576 (0.315 sec/step)\n",
            "I0707 17:03:38.851843 140263261390720 learning.py:507] global step 3006: loss = 4.6097 (0.433 sec/step)\n",
            "I0707 17:03:39.253746 140263261390720 learning.py:507] global step 3007: loss = 5.3307 (0.400 sec/step)\n",
            "I0707 17:03:39.660511 140263261390720 learning.py:507] global step 3008: loss = 3.4659 (0.405 sec/step)\n",
            "I0707 17:03:39.973120 140263261390720 learning.py:507] global step 3009: loss = 5.4386 (0.311 sec/step)\n",
            "I0707 17:03:40.324510 140263261390720 learning.py:507] global step 3010: loss = 5.0818 (0.350 sec/step)\n",
            "I0707 17:03:40.664756 140263261390720 learning.py:507] global step 3011: loss = 4.4028 (0.339 sec/step)\n",
            "I0707 17:03:40.964596 140263261390720 learning.py:507] global step 3012: loss = 4.8519 (0.298 sec/step)\n",
            "I0707 17:03:41.412286 140263261390720 learning.py:507] global step 3013: loss = 6.1399 (0.446 sec/step)\n",
            "I0707 17:03:41.718513 140263261390720 learning.py:507] global step 3014: loss = 3.6358 (0.304 sec/step)\n",
            "I0707 17:03:42.077442 140263261390720 learning.py:507] global step 3015: loss = 4.1762 (0.357 sec/step)\n",
            "I0707 17:03:42.403400 140263261390720 learning.py:507] global step 3016: loss = 4.5580 (0.324 sec/step)\n",
            "I0707 17:03:42.761045 140263261390720 learning.py:507] global step 3017: loss = 3.6610 (0.356 sec/step)\n",
            "I0707 17:03:43.052288 140263261390720 learning.py:507] global step 3018: loss = 4.1631 (0.289 sec/step)\n",
            "I0707 17:03:43.376734 140263261390720 learning.py:507] global step 3019: loss = 3.5705 (0.323 sec/step)\n",
            "I0707 17:03:43.690633 140263261390720 learning.py:507] global step 3020: loss = 3.2728 (0.312 sec/step)\n",
            "I0707 17:03:44.003377 140263261390720 learning.py:507] global step 3021: loss = 4.1337 (0.311 sec/step)\n",
            "I0707 17:03:44.306608 140263261390720 learning.py:507] global step 3022: loss = 5.2351 (0.301 sec/step)\n",
            "I0707 17:03:44.706691 140263261390720 learning.py:507] global step 3023: loss = 4.9110 (0.398 sec/step)\n",
            "I0707 17:03:45.026757 140263261390720 learning.py:507] global step 3024: loss = 6.2555 (0.318 sec/step)\n",
            "I0707 17:03:45.347788 140263261390720 learning.py:507] global step 3025: loss = 3.5125 (0.319 sec/step)\n",
            "I0707 17:03:45.798131 140263261390720 learning.py:507] global step 3026: loss = 3.1396 (0.449 sec/step)\n",
            "I0707 17:03:46.096271 140263261390720 learning.py:507] global step 3027: loss = 5.6418 (0.296 sec/step)\n",
            "I0707 17:03:46.433071 140263261390720 learning.py:507] global step 3028: loss = 4.1878 (0.332 sec/step)\n",
            "I0707 17:03:46.731722 140263261390720 learning.py:507] global step 3029: loss = 6.5918 (0.297 sec/step)\n",
            "I0707 17:03:47.076763 140263261390720 learning.py:507] global step 3030: loss = 2.3858 (0.343 sec/step)\n",
            "I0707 17:03:47.401351 140263261390720 learning.py:507] global step 3031: loss = 4.0515 (0.323 sec/step)\n",
            "I0707 17:03:47.946497 140263261390720 learning.py:507] global step 3032: loss = 3.8634 (0.544 sec/step)\n",
            "I0707 17:03:48.243409 140263261390720 learning.py:507] global step 3033: loss = 3.3695 (0.295 sec/step)\n",
            "I0707 17:03:48.579374 140263261390720 learning.py:507] global step 3034: loss = 3.7452 (0.334 sec/step)\n",
            "I0707 17:03:48.868206 140263261390720 learning.py:507] global step 3035: loss = 3.3593 (0.287 sec/step)\n",
            "I0707 17:03:49.206659 140263261390720 learning.py:507] global step 3036: loss = 4.8064 (0.336 sec/step)\n",
            "I0707 17:03:49.529081 140263261390720 learning.py:507] global step 3037: loss = 3.9210 (0.321 sec/step)\n",
            "I0707 17:03:49.838479 140263261390720 learning.py:507] global step 3038: loss = 4.3207 (0.308 sec/step)\n",
            "I0707 17:03:50.135589 140263261390720 learning.py:507] global step 3039: loss = 4.0988 (0.295 sec/step)\n",
            "I0707 17:03:50.550591 140263261390720 learning.py:507] global step 3040: loss = 3.9255 (0.413 sec/step)\n",
            "I0707 17:03:50.844719 140263261390720 learning.py:507] global step 3041: loss = 4.5867 (0.292 sec/step)\n",
            "I0707 17:03:51.159502 140263261390720 learning.py:507] global step 3042: loss = 5.2210 (0.313 sec/step)\n",
            "I0707 17:03:51.600051 140263261390720 learning.py:507] global step 3043: loss = 3.7937 (0.439 sec/step)\n",
            "I0707 17:03:51.929165 140263261390720 learning.py:507] global step 3044: loss = 3.9817 (0.327 sec/step)\n",
            "I0707 17:03:52.346999 140263261390720 learning.py:507] global step 3045: loss = 4.2198 (0.416 sec/step)\n",
            "I0707 17:03:52.644315 140263261390720 learning.py:507] global step 3046: loss = 4.8197 (0.295 sec/step)\n",
            "I0707 17:03:52.959397 140263261390720 learning.py:507] global step 3047: loss = 3.9001 (0.313 sec/step)\n",
            "I0707 17:03:53.737871 140263261390720 learning.py:507] global step 3048: loss = 5.2776 (0.774 sec/step)\n",
            "I0707 17:03:53.821757 140260287772416 supervisor.py:1050] Recording summary at step 3048.\n",
            "I0707 17:03:54.057455 140263261390720 learning.py:507] global step 3049: loss = 3.3937 (0.318 sec/step)\n",
            "I0707 17:03:54.523657 140263261390720 learning.py:507] global step 3050: loss = 4.1198 (0.465 sec/step)\n",
            "I0707 17:03:54.817169 140263261390720 learning.py:507] global step 3051: loss = 3.8680 (0.292 sec/step)\n",
            "I0707 17:03:54.842810 140260296165120 supervisor.py:1099] global_step/sec: 2.90112\n",
            "I0707 17:03:55.236651 140263261390720 learning.py:507] global step 3052: loss = 3.0909 (0.418 sec/step)\n",
            "I0707 17:03:55.558580 140263261390720 learning.py:507] global step 3053: loss = 4.1211 (0.320 sec/step)\n",
            "I0707 17:03:55.882757 140263261390720 learning.py:507] global step 3054: loss = 4.9324 (0.323 sec/step)\n",
            "I0707 17:03:56.175492 140263261390720 learning.py:507] global step 3055: loss = 4.3524 (0.290 sec/step)\n",
            "I0707 17:03:56.491019 140263261390720 learning.py:507] global step 3056: loss = 3.8146 (0.313 sec/step)\n",
            "I0707 17:03:56.850648 140263261390720 learning.py:507] global step 3057: loss = 3.9812 (0.358 sec/step)\n",
            "I0707 17:03:57.158303 140263261390720 learning.py:507] global step 3058: loss = 5.0332 (0.306 sec/step)\n",
            "I0707 17:03:57.454132 140263261390720 learning.py:507] global step 3059: loss = 4.6236 (0.294 sec/step)\n",
            "I0707 17:03:57.750780 140263261390720 learning.py:507] global step 3060: loss = 3.6286 (0.295 sec/step)\n",
            "I0707 17:03:58.056951 140263261390720 learning.py:507] global step 3061: loss = 4.6754 (0.304 sec/step)\n",
            "I0707 17:03:58.375683 140263261390720 learning.py:507] global step 3062: loss = 5.2096 (0.317 sec/step)\n",
            "I0707 17:03:58.809206 140263261390720 learning.py:507] global step 3063: loss = 3.3876 (0.432 sec/step)\n",
            "I0707 17:03:59.110529 140263261390720 learning.py:507] global step 3064: loss = 3.2225 (0.299 sec/step)\n",
            "I0707 17:03:59.397651 140263261390720 learning.py:507] global step 3065: loss = 3.1419 (0.285 sec/step)\n",
            "I0707 17:03:59.772812 140263261390720 learning.py:507] global step 3066: loss = 3.2656 (0.374 sec/step)\n",
            "I0707 17:04:00.070022 140263261390720 learning.py:507] global step 3067: loss = 4.9252 (0.295 sec/step)\n",
            "I0707 17:04:00.376125 140263261390720 learning.py:507] global step 3068: loss = 2.9309 (0.304 sec/step)\n",
            "I0707 17:04:00.733603 140263261390720 learning.py:507] global step 3069: loss = 4.3186 (0.356 sec/step)\n",
            "I0707 17:04:01.148625 140263261390720 learning.py:507] global step 3070: loss = 3.1001 (0.413 sec/step)\n",
            "I0707 17:04:01.447743 140263261390720 learning.py:507] global step 3071: loss = 3.5665 (0.297 sec/step)\n",
            "I0707 17:04:01.897402 140263261390720 learning.py:507] global step 3072: loss = 3.9461 (0.448 sec/step)\n",
            "I0707 17:04:02.184294 140263261390720 learning.py:507] global step 3073: loss = 2.4351 (0.285 sec/step)\n",
            "I0707 17:04:02.478822 140263261390720 learning.py:507] global step 3074: loss = 4.5844 (0.293 sec/step)\n",
            "I0707 17:04:02.785418 140263261390720 learning.py:507] global step 3075: loss = 4.1497 (0.304 sec/step)\n",
            "I0707 17:04:03.329073 140263261390720 learning.py:507] global step 3076: loss = 3.8259 (0.542 sec/step)\n",
            "I0707 17:04:03.732474 140263261390720 learning.py:507] global step 3077: loss = 3.8134 (0.402 sec/step)\n",
            "I0707 17:04:04.128562 140263261390720 learning.py:507] global step 3078: loss = 4.9655 (0.394 sec/step)\n",
            "I0707 17:04:04.451503 140263261390720 learning.py:507] global step 3079: loss = 3.6028 (0.321 sec/step)\n",
            "I0707 17:04:04.779099 140263261390720 learning.py:507] global step 3080: loss = 4.5239 (0.325 sec/step)\n",
            "I0707 17:04:05.063140 140263261390720 learning.py:507] global step 3081: loss = 2.8712 (0.282 sec/step)\n",
            "I0707 17:04:05.371158 140263261390720 learning.py:507] global step 3082: loss = 4.9724 (0.306 sec/step)\n",
            "I0707 17:04:05.693449 140263261390720 learning.py:507] global step 3083: loss = 4.4671 (0.321 sec/step)\n",
            "I0707 17:04:05.987716 140263261390720 learning.py:507] global step 3084: loss = 3.6184 (0.293 sec/step)\n",
            "I0707 17:04:06.372732 140263261390720 learning.py:507] global step 3085: loss = 4.0909 (0.384 sec/step)\n",
            "I0707 17:04:06.673529 140263261390720 learning.py:507] global step 3086: loss = 5.4644 (0.299 sec/step)\n",
            "I0707 17:04:06.997514 140263261390720 learning.py:507] global step 3087: loss = 4.3691 (0.322 sec/step)\n",
            "I0707 17:04:07.292163 140263261390720 learning.py:507] global step 3088: loss = 5.3151 (0.293 sec/step)\n",
            "I0707 17:04:07.686138 140263261390720 learning.py:507] global step 3089: loss = 4.4786 (0.392 sec/step)\n",
            "I0707 17:04:08.013598 140263261390720 learning.py:507] global step 3090: loss = 3.7173 (0.326 sec/step)\n",
            "I0707 17:04:08.355265 140263261390720 learning.py:507] global step 3091: loss = 4.1013 (0.340 sec/step)\n",
            "I0707 17:04:08.683950 140263261390720 learning.py:507] global step 3092: loss = 3.6516 (0.327 sec/step)\n",
            "I0707 17:04:09.002117 140263261390720 learning.py:507] global step 3093: loss = 4.5998 (0.316 sec/step)\n",
            "I0707 17:04:09.285786 140263261390720 learning.py:507] global step 3094: loss = 3.3615 (0.282 sec/step)\n",
            "I0707 17:04:09.711299 140263261390720 learning.py:507] global step 3095: loss = 4.2545 (0.422 sec/step)\n",
            "I0707 17:04:10.019297 140263261390720 learning.py:507] global step 3096: loss = 5.7900 (0.306 sec/step)\n",
            "I0707 17:04:10.362938 140263261390720 learning.py:507] global step 3097: loss = 4.1812 (0.342 sec/step)\n",
            "I0707 17:04:10.682200 140263261390720 learning.py:507] global step 3098: loss = 4.0509 (0.318 sec/step)\n",
            "I0707 17:04:11.071467 140263261390720 learning.py:507] global step 3099: loss = 3.6579 (0.388 sec/step)\n",
            "I0707 17:04:11.379323 140263261390720 learning.py:507] global step 3100: loss = 2.8572 (0.306 sec/step)\n",
            "I0707 17:04:11.661666 140263261390720 learning.py:507] global step 3101: loss = 3.8905 (0.280 sec/step)\n",
            "I0707 17:04:11.981729 140263261390720 learning.py:507] global step 3102: loss = 5.0807 (0.318 sec/step)\n",
            "I0707 17:04:12.293251 140263261390720 learning.py:507] global step 3103: loss = 5.0552 (0.310 sec/step)\n",
            "I0707 17:04:12.692475 140263261390720 learning.py:507] global step 3104: loss = 3.1298 (0.398 sec/step)\n",
            "I0707 17:04:13.333952 140263261390720 learning.py:507] global step 3105: loss = 3.1071 (0.640 sec/step)\n",
            "I0707 17:04:13.681779 140263261390720 learning.py:507] global step 3106: loss = 3.8072 (0.346 sec/step)\n",
            "I0707 17:04:13.999608 140263261390720 learning.py:507] global step 3107: loss = 3.8791 (0.316 sec/step)\n",
            "I0707 17:04:14.293045 140263261390720 learning.py:507] global step 3108: loss = 3.5671 (0.292 sec/step)\n",
            "I0707 17:04:14.592483 140263261390720 learning.py:507] global step 3109: loss = 3.1224 (0.298 sec/step)\n",
            "I0707 17:04:14.895674 140263261390720 learning.py:507] global step 3110: loss = 3.8500 (0.302 sec/step)\n",
            "I0707 17:04:15.603477 140263261390720 learning.py:507] global step 3111: loss = 3.1141 (0.706 sec/step)\n",
            "I0707 17:04:16.041631 140263261390720 learning.py:507] global step 3112: loss = 4.3425 (0.436 sec/step)\n",
            "I0707 17:04:16.372332 140263261390720 learning.py:507] global step 3113: loss = 4.5505 (0.329 sec/step)\n",
            "I0707 17:04:16.674180 140263261390720 learning.py:507] global step 3114: loss = 4.6582 (0.300 sec/step)\n",
            "I0707 17:04:16.980508 140263261390720 learning.py:507] global step 3115: loss = 4.0507 (0.305 sec/step)\n",
            "I0707 17:04:17.311392 140263261390720 learning.py:507] global step 3116: loss = 3.9433 (0.329 sec/step)\n",
            "I0707 17:04:17.624345 140263261390720 learning.py:507] global step 3117: loss = 3.4150 (0.311 sec/step)\n",
            "I0707 17:04:17.958677 140263261390720 learning.py:507] global step 3118: loss = 4.1025 (0.333 sec/step)\n",
            "I0707 17:04:18.337133 140263261390720 learning.py:507] global step 3119: loss = 4.3637 (0.377 sec/step)\n",
            "I0707 17:04:18.653444 140263261390720 learning.py:507] global step 3120: loss = 3.6700 (0.315 sec/step)\n",
            "I0707 17:04:18.966300 140263261390720 learning.py:507] global step 3121: loss = 3.8190 (0.311 sec/step)\n",
            "I0707 17:04:19.294040 140263261390720 learning.py:507] global step 3122: loss = 5.3986 (0.326 sec/step)\n",
            "I0707 17:04:19.616797 140263261390720 learning.py:507] global step 3123: loss = 3.9674 (0.321 sec/step)\n",
            "I0707 17:04:19.948128 140263261390720 learning.py:507] global step 3124: loss = 3.7126 (0.329 sec/step)\n",
            "I0707 17:04:20.372389 140263261390720 learning.py:507] global step 3125: loss = 3.7461 (0.423 sec/step)\n",
            "I0707 17:04:20.746445 140263261390720 learning.py:507] global step 3126: loss = 3.9588 (0.372 sec/step)\n",
            "I0707 17:04:21.044639 140263261390720 learning.py:507] global step 3127: loss = 3.7873 (0.297 sec/step)\n",
            "I0707 17:04:21.376316 140263261390720 learning.py:507] global step 3128: loss = 4.8320 (0.330 sec/step)\n",
            "I0707 17:04:21.773145 140263261390720 learning.py:507] global step 3129: loss = 4.2081 (0.395 sec/step)\n",
            "I0707 17:04:22.086963 140263261390720 learning.py:507] global step 3130: loss = 3.7540 (0.312 sec/step)\n",
            "I0707 17:04:22.454730 140263261390720 learning.py:507] global step 3131: loss = 4.2744 (0.366 sec/step)\n",
            "I0707 17:04:22.928658 140263261390720 learning.py:507] global step 3132: loss = 3.4392 (0.472 sec/step)\n",
            "I0707 17:04:23.231662 140263261390720 learning.py:507] global step 3133: loss = 2.6082 (0.302 sec/step)\n",
            "I0707 17:04:23.527803 140263261390720 learning.py:507] global step 3134: loss = 6.8826 (0.294 sec/step)\n",
            "I0707 17:04:23.846059 140263261390720 learning.py:507] global step 3135: loss = 4.4860 (0.317 sec/step)\n",
            "I0707 17:04:24.165954 140263261390720 learning.py:507] global step 3136: loss = 4.8428 (0.318 sec/step)\n",
            "I0707 17:04:24.564873 140263261390720 learning.py:507] global step 3137: loss = 4.5003 (0.397 sec/step)\n",
            "I0707 17:04:25.002690 140263261390720 learning.py:507] global step 3138: loss = 4.7354 (0.436 sec/step)\n",
            "I0707 17:04:25.291591 140263261390720 learning.py:507] global step 3139: loss = 2.5461 (0.287 sec/step)\n",
            "I0707 17:04:25.591547 140263261390720 learning.py:507] global step 3140: loss = 3.7034 (0.299 sec/step)\n",
            "I0707 17:04:25.947304 140263261390720 learning.py:507] global step 3141: loss = 3.7420 (0.354 sec/step)\n",
            "I0707 17:04:26.286897 140263261390720 learning.py:507] global step 3142: loss = 2.9464 (0.336 sec/step)\n",
            "I0707 17:04:26.588810 140263261390720 learning.py:507] global step 3143: loss = 4.3494 (0.300 sec/step)\n",
            "I0707 17:04:26.917089 140263261390720 learning.py:507] global step 3144: loss = 3.1720 (0.327 sec/step)\n",
            "I0707 17:04:27.235248 140263261390720 learning.py:507] global step 3145: loss = 2.9625 (0.317 sec/step)\n",
            "I0707 17:04:27.563442 140263261390720 learning.py:507] global step 3146: loss = 4.4859 (0.327 sec/step)\n",
            "I0707 17:04:27.980697 140263261390720 learning.py:507] global step 3147: loss = 3.3101 (0.416 sec/step)\n",
            "I0707 17:04:28.301961 140263261390720 learning.py:507] global step 3148: loss = 4.3328 (0.320 sec/step)\n",
            "I0707 17:04:28.628471 140263261390720 learning.py:507] global step 3149: loss = 3.9849 (0.325 sec/step)\n",
            "I0707 17:04:28.962757 140263261390720 learning.py:507] global step 3150: loss = 3.9055 (0.333 sec/step)\n",
            "I0707 17:04:29.272160 140263261390720 learning.py:507] global step 3151: loss = 3.2163 (0.308 sec/step)\n",
            "I0707 17:04:29.584236 140263261390720 learning.py:507] global step 3152: loss = 4.6413 (0.311 sec/step)\n",
            "I0707 17:04:29.879459 140263261390720 learning.py:507] global step 3153: loss = 3.5846 (0.294 sec/step)\n",
            "I0707 17:04:30.199787 140263261390720 learning.py:507] global step 3154: loss = 3.7081 (0.319 sec/step)\n",
            "I0707 17:04:30.542757 140263261390720 learning.py:507] global step 3155: loss = 3.9044 (0.341 sec/step)\n",
            "I0707 17:04:30.963289 140263261390720 learning.py:507] global step 3156: loss = 4.3980 (0.419 sec/step)\n",
            "I0707 17:04:31.306883 140263261390720 learning.py:507] global step 3157: loss = 3.4135 (0.342 sec/step)\n",
            "I0707 17:04:31.641947 140263261390720 learning.py:507] global step 3158: loss = 3.8212 (0.333 sec/step)\n",
            "I0707 17:04:31.935352 140263261390720 learning.py:507] global step 3159: loss = 4.0457 (0.292 sec/step)\n",
            "I0707 17:04:32.241078 140263261390720 learning.py:507] global step 3160: loss = 4.6822 (0.304 sec/step)\n",
            "I0707 17:04:32.557269 140263261390720 learning.py:507] global step 3161: loss = 5.3534 (0.315 sec/step)\n",
            "I0707 17:04:32.997976 140263261390720 learning.py:507] global step 3162: loss = 3.0583 (0.439 sec/step)\n",
            "I0707 17:04:33.450279 140263261390720 learning.py:507] global step 3163: loss = 3.2233 (0.451 sec/step)\n",
            "I0707 17:04:33.802906 140263261390720 learning.py:507] global step 3164: loss = 3.6717 (0.351 sec/step)\n",
            "I0707 17:04:34.101126 140263261390720 learning.py:507] global step 3165: loss = 5.1520 (0.297 sec/step)\n",
            "I0707 17:04:34.405689 140263261390720 learning.py:507] global step 3166: loss = 4.0258 (0.303 sec/step)\n",
            "I0707 17:04:34.738794 140263261390720 learning.py:507] global step 3167: loss = 3.4997 (0.331 sec/step)\n",
            "I0707 17:04:35.036917 140263261390720 learning.py:507] global step 3168: loss = 4.0748 (0.296 sec/step)\n",
            "I0707 17:04:35.331897 140263261390720 learning.py:507] global step 3169: loss = 3.0599 (0.293 sec/step)\n",
            "I0707 17:04:35.697748 140263261390720 learning.py:507] global step 3170: loss = 3.9311 (0.364 sec/step)\n",
            "I0707 17:04:36.088532 140263261390720 learning.py:507] global step 3171: loss = 4.2449 (0.389 sec/step)\n",
            "I0707 17:04:36.409941 140263261390720 learning.py:507] global step 3172: loss = 2.9987 (0.320 sec/step)\n",
            "I0707 17:04:36.761039 140263261390720 learning.py:507] global step 3173: loss = 4.9323 (0.349 sec/step)\n",
            "I0707 17:04:37.107947 140263261390720 learning.py:507] global step 3174: loss = 5.5133 (0.345 sec/step)\n",
            "I0707 17:04:37.486354 140263261390720 learning.py:507] global step 3175: loss = 3.7616 (0.377 sec/step)\n",
            "I0707 17:04:37.834691 140263261390720 learning.py:507] global step 3176: loss = 4.2822 (0.347 sec/step)\n",
            "I0707 17:04:38.154390 140263261390720 learning.py:507] global step 3177: loss = 4.1532 (0.318 sec/step)\n",
            "I0707 17:04:38.454394 140263261390720 learning.py:507] global step 3178: loss = 5.2228 (0.298 sec/step)\n",
            "I0707 17:04:38.773242 140263261390720 learning.py:507] global step 3179: loss = 4.8939 (0.317 sec/step)\n",
            "I0707 17:04:39.087001 140263261390720 learning.py:507] global step 3180: loss = 4.2219 (0.312 sec/step)\n",
            "I0707 17:04:39.438086 140263261390720 learning.py:507] global step 3181: loss = 4.5184 (0.350 sec/step)\n",
            "I0707 17:04:39.807356 140263261390720 learning.py:507] global step 3182: loss = 4.1424 (0.368 sec/step)\n",
            "I0707 17:04:40.121927 140263261390720 learning.py:507] global step 3183: loss = 5.9236 (0.313 sec/step)\n",
            "I0707 17:04:40.422242 140263261390720 learning.py:507] global step 3184: loss = 4.2803 (0.299 sec/step)\n",
            "I0707 17:04:40.741708 140263261390720 learning.py:507] global step 3185: loss = 4.2558 (0.318 sec/step)\n",
            "I0707 17:04:41.055100 140263261390720 learning.py:507] global step 3186: loss = 4.6017 (0.312 sec/step)\n",
            "I0707 17:04:41.362927 140263261390720 learning.py:507] global step 3187: loss = 3.5397 (0.306 sec/step)\n",
            "I0707 17:04:41.708662 140263261390720 learning.py:507] global step 3188: loss = 4.7023 (0.344 sec/step)\n",
            "I0707 17:04:42.042042 140263261390720 learning.py:507] global step 3189: loss = 3.1282 (0.331 sec/step)\n",
            "I0707 17:04:42.432021 140263261390720 learning.py:507] global step 3190: loss = 2.7337 (0.388 sec/step)\n",
            "I0707 17:04:42.831518 140263261390720 learning.py:507] global step 3191: loss = 5.0802 (0.398 sec/step)\n",
            "I0707 17:04:43.131767 140263261390720 learning.py:507] global step 3192: loss = 3.9479 (0.299 sec/step)\n",
            "I0707 17:04:43.450647 140263261390720 learning.py:507] global step 3193: loss = 3.3707 (0.317 sec/step)\n",
            "I0707 17:04:43.788332 140263261390720 learning.py:507] global step 3194: loss = 3.3252 (0.336 sec/step)\n",
            "I0707 17:04:44.095710 140263261390720 learning.py:507] global step 3195: loss = 3.9552 (0.306 sec/step)\n",
            "I0707 17:04:44.443228 140263261390720 learning.py:507] global step 3196: loss = 4.5345 (0.346 sec/step)\n",
            "I0707 17:04:44.861719 140263261390720 learning.py:507] global step 3197: loss = 3.7044 (0.417 sec/step)\n",
            "I0707 17:04:45.275101 140263261390720 learning.py:507] global step 3198: loss = 4.9706 (0.412 sec/step)\n",
            "I0707 17:04:45.580032 140263261390720 learning.py:507] global step 3199: loss = 4.9339 (0.303 sec/step)\n",
            "I0707 17:04:45.994885 140263261390720 learning.py:507] global step 3200: loss = 4.1568 (0.413 sec/step)\n",
            "I0707 17:04:46.331551 140263261390720 learning.py:507] global step 3201: loss = 4.6263 (0.335 sec/step)\n",
            "I0707 17:04:46.660061 140263261390720 learning.py:507] global step 3202: loss = 3.8385 (0.327 sec/step)\n",
            "I0707 17:04:46.960449 140263261390720 learning.py:507] global step 3203: loss = 4.6302 (0.299 sec/step)\n",
            "I0707 17:04:47.254209 140263261390720 learning.py:507] global step 3204: loss = 3.8128 (0.292 sec/step)\n",
            "I0707 17:04:47.563703 140263261390720 learning.py:507] global step 3205: loss = 3.8209 (0.308 sec/step)\n",
            "I0707 17:04:47.982347 140263261390720 learning.py:507] global step 3206: loss = 5.4938 (0.417 sec/step)\n",
            "I0707 17:04:48.299731 140263261390720 learning.py:507] global step 3207: loss = 6.5388 (0.316 sec/step)\n",
            "I0707 17:04:48.648852 140263261390720 learning.py:507] global step 3208: loss = 3.9533 (0.347 sec/step)\n",
            "I0707 17:04:48.954311 140263261390720 learning.py:507] global step 3209: loss = 3.9235 (0.304 sec/step)\n",
            "I0707 17:04:49.278099 140263261390720 learning.py:507] global step 3210: loss = 3.7998 (0.322 sec/step)\n",
            "I0707 17:04:49.595450 140263261390720 learning.py:507] global step 3211: loss = 3.3505 (0.315 sec/step)\n",
            "I0707 17:04:49.890939 140263261390720 learning.py:507] global step 3212: loss = 3.4371 (0.294 sec/step)\n",
            "I0707 17:04:50.198385 140263261390720 learning.py:507] global step 3213: loss = 3.2700 (0.306 sec/step)\n",
            "I0707 17:04:50.492668 140263261390720 learning.py:507] global step 3214: loss = 3.0552 (0.292 sec/step)\n",
            "I0707 17:04:50.784467 140263261390720 learning.py:507] global step 3215: loss = 4.5914 (0.290 sec/step)\n",
            "I0707 17:04:51.190116 140263261390720 learning.py:507] global step 3216: loss = 4.0235 (0.404 sec/step)\n",
            "I0707 17:04:51.498464 140263261390720 learning.py:507] global step 3217: loss = 3.0532 (0.307 sec/step)\n",
            "I0707 17:04:51.808142 140263261390720 learning.py:507] global step 3218: loss = 5.1697 (0.308 sec/step)\n",
            "I0707 17:04:52.115417 140263261390720 learning.py:507] global step 3219: loss = 4.6346 (0.306 sec/step)\n",
            "I0707 17:04:52.422013 140263261390720 learning.py:507] global step 3220: loss = 4.7017 (0.305 sec/step)\n",
            "I0707 17:04:52.758393 140263261390720 learning.py:507] global step 3221: loss = 2.8996 (0.335 sec/step)\n",
            "I0707 17:04:53.062404 140263261390720 learning.py:507] global step 3222: loss = 3.5489 (0.302 sec/step)\n",
            "I0707 17:04:53.374751 140263261390720 learning.py:507] global step 3223: loss = 3.5587 (0.310 sec/step)\n",
            "I0707 17:04:53.680412 140263261390720 learning.py:507] global step 3224: loss = 4.6502 (0.304 sec/step)\n",
            "I0707 17:04:54.009919 140263261390720 learning.py:507] global step 3225: loss = 5.6718 (0.327 sec/step)\n",
            "I0707 17:04:54.526742 140263261390720 learning.py:507] global step 3226: loss = 6.0354 (0.514 sec/step)\n",
            "I0707 17:04:54.858855 140263261390720 learning.py:507] global step 3227: loss = 4.4230 (0.330 sec/step)\n",
            "I0707 17:04:55.229449 140263261390720 learning.py:507] global step 3228: loss = 4.4627 (0.369 sec/step)\n",
            "I0707 17:04:55.532863 140263261390720 learning.py:507] global step 3229: loss = 2.9891 (0.302 sec/step)\n",
            "I0707 17:04:55.851839 140263261390720 learning.py:507] global step 3230: loss = 2.7813 (0.317 sec/step)\n",
            "I0707 17:04:56.151646 140263261390720 learning.py:507] global step 3231: loss = 4.8708 (0.298 sec/step)\n",
            "I0707 17:04:56.686260 140263261390720 learning.py:507] global step 3232: loss = 3.5814 (0.533 sec/step)\n",
            "I0707 17:04:56.974042 140263261390720 learning.py:507] global step 3233: loss = 2.8272 (0.286 sec/step)\n",
            "I0707 17:04:57.339085 140263261390720 learning.py:507] global step 3234: loss = 2.9275 (0.363 sec/step)\n",
            "I0707 17:04:57.737440 140263261390720 learning.py:507] global step 3235: loss = 3.7806 (0.396 sec/step)\n",
            "I0707 17:04:58.101621 140263261390720 learning.py:507] global step 3236: loss = 3.8664 (0.362 sec/step)\n",
            "I0707 17:04:58.417502 140263261390720 learning.py:507] global step 3237: loss = 3.8260 (0.313 sec/step)\n",
            "I0707 17:04:58.736300 140263261390720 learning.py:507] global step 3238: loss = 3.9831 (0.317 sec/step)\n",
            "I0707 17:04:59.051899 140263261390720 learning.py:507] global step 3239: loss = 4.9628 (0.314 sec/step)\n",
            "I0707 17:04:59.478774 140263261390720 learning.py:507] global step 3240: loss = 4.1154 (0.425 sec/step)\n",
            "I0707 17:04:59.775661 140263261390720 learning.py:507] global step 3241: loss = 4.1818 (0.295 sec/step)\n",
            "I0707 17:05:00.218598 140263261390720 learning.py:507] global step 3242: loss = 5.0066 (0.441 sec/step)\n",
            "I0707 17:05:00.514099 140263261390720 learning.py:507] global step 3243: loss = 3.3526 (0.294 sec/step)\n",
            "I0707 17:05:00.937646 140263261390720 learning.py:507] global step 3244: loss = 6.2335 (0.422 sec/step)\n",
            "I0707 17:05:01.240794 140263261390720 learning.py:507] global step 3245: loss = 5.0915 (0.301 sec/step)\n",
            "I0707 17:05:01.560256 140263261390720 learning.py:507] global step 3246: loss = 4.6814 (0.317 sec/step)\n",
            "I0707 17:05:01.871589 140263261390720 learning.py:507] global step 3247: loss = 4.5888 (0.310 sec/step)\n",
            "I0707 17:05:02.281621 140263261390720 learning.py:507] global step 3248: loss = 4.2925 (0.408 sec/step)\n",
            "I0707 17:05:02.616245 140263261390720 learning.py:507] global step 3249: loss = 4.4143 (0.333 sec/step)\n",
            "I0707 17:05:02.962297 140263261390720 learning.py:507] global step 3250: loss = 4.4317 (0.344 sec/step)\n",
            "I0707 17:05:03.362900 140263261390720 learning.py:507] global step 3251: loss = 2.8142 (0.399 sec/step)\n",
            "I0707 17:05:03.759372 140263261390720 learning.py:507] global step 3252: loss = 3.3741 (0.395 sec/step)\n",
            "I0707 17:05:04.062232 140263261390720 learning.py:507] global step 3253: loss = 3.0685 (0.301 sec/step)\n",
            "I0707 17:05:04.368275 140263261390720 learning.py:507] global step 3254: loss = 3.9736 (0.304 sec/step)\n",
            "I0707 17:05:04.784948 140263261390720 learning.py:507] global step 3255: loss = 4.1244 (0.415 sec/step)\n",
            "I0707 17:05:05.098037 140263261390720 learning.py:507] global step 3256: loss = 4.0432 (0.312 sec/step)\n",
            "I0707 17:05:05.403260 140263261390720 learning.py:507] global step 3257: loss = 3.3792 (0.302 sec/step)\n",
            "I0707 17:05:05.730687 140263261390720 learning.py:507] global step 3258: loss = 2.4631 (0.326 sec/step)\n",
            "I0707 17:05:06.036763 140263261390720 learning.py:507] global step 3259: loss = 3.8180 (0.305 sec/step)\n",
            "I0707 17:05:06.353488 140263261390720 learning.py:507] global step 3260: loss = 3.9556 (0.315 sec/step)\n",
            "I0707 17:05:06.741074 140263261390720 learning.py:507] global step 3261: loss = 3.7998 (0.386 sec/step)\n",
            "I0707 17:05:07.060616 140263261390720 learning.py:507] global step 3262: loss = 3.3424 (0.317 sec/step)\n",
            "I0707 17:05:07.377884 140263261390720 learning.py:507] global step 3263: loss = 3.8703 (0.315 sec/step)\n",
            "I0707 17:05:07.699238 140263261390720 learning.py:507] global step 3264: loss = 3.5763 (0.319 sec/step)\n",
            "I0707 17:05:08.019715 140263261390720 learning.py:507] global step 3265: loss = 4.1427 (0.319 sec/step)\n",
            "I0707 17:05:08.325650 140263261390720 learning.py:507] global step 3266: loss = 3.6559 (0.304 sec/step)\n",
            "I0707 17:05:08.639738 140263261390720 learning.py:507] global step 3267: loss = 2.8926 (0.312 sec/step)\n",
            "I0707 17:05:08.970108 140263261390720 learning.py:507] global step 3268: loss = 3.4830 (0.328 sec/step)\n",
            "I0707 17:05:09.267568 140263261390720 learning.py:507] global step 3269: loss = 4.4736 (0.296 sec/step)\n",
            "I0707 17:05:09.572550 140263261390720 learning.py:507] global step 3270: loss = 3.2360 (0.303 sec/step)\n",
            "I0707 17:05:09.883800 140263261390720 learning.py:507] global step 3271: loss = 2.9996 (0.310 sec/step)\n",
            "I0707 17:05:10.239537 140263261390720 learning.py:507] global step 3272: loss = 4.2709 (0.354 sec/step)\n",
            "I0707 17:05:10.540728 140263261390720 learning.py:507] global step 3273: loss = 5.5407 (0.300 sec/step)\n",
            "I0707 17:05:10.922691 140263261390720 learning.py:507] global step 3274: loss = 3.6423 (0.380 sec/step)\n",
            "I0707 17:05:11.218472 140263261390720 learning.py:507] global step 3275: loss = 5.1905 (0.294 sec/step)\n",
            "I0707 17:05:11.551370 140263261390720 learning.py:507] global step 3276: loss = 5.8015 (0.331 sec/step)\n",
            "I0707 17:05:11.855267 140263261390720 learning.py:507] global step 3277: loss = 3.6242 (0.302 sec/step)\n",
            "I0707 17:05:12.274263 140263261390720 learning.py:507] global step 3278: loss = 3.8384 (0.417 sec/step)\n",
            "I0707 17:05:12.580265 140263261390720 learning.py:507] global step 3279: loss = 5.2457 (0.304 sec/step)\n",
            "I0707 17:05:12.952526 140263261390720 learning.py:507] global step 3280: loss = 3.4485 (0.371 sec/step)\n",
            "I0707 17:05:13.270287 140263261390720 learning.py:507] global step 3281: loss = 3.9352 (0.316 sec/step)\n",
            "I0707 17:05:13.597243 140263261390720 learning.py:507] global step 3282: loss = 3.8687 (0.325 sec/step)\n",
            "I0707 17:05:13.927215 140263261390720 learning.py:507] global step 3283: loss = 5.0284 (0.328 sec/step)\n",
            "I0707 17:05:14.247432 140263261390720 learning.py:507] global step 3284: loss = 3.1573 (0.318 sec/step)\n",
            "I0707 17:05:14.613277 140263261390720 learning.py:507] global step 3285: loss = 5.4423 (0.364 sec/step)\n",
            "I0707 17:05:14.918023 140263261390720 learning.py:507] global step 3286: loss = 2.9811 (0.303 sec/step)\n",
            "I0707 17:05:15.252244 140263261390720 learning.py:507] global step 3287: loss = 5.7852 (0.332 sec/step)\n",
            "I0707 17:05:15.571644 140263261390720 learning.py:507] global step 3288: loss = 4.4715 (0.317 sec/step)\n",
            "I0707 17:05:15.923487 140263261390720 learning.py:507] global step 3289: loss = 3.9068 (0.350 sec/step)\n",
            "I0707 17:05:16.234188 140263261390720 learning.py:507] global step 3290: loss = 3.9329 (0.309 sec/step)\n",
            "I0707 17:05:16.739163 140263261390720 learning.py:507] global step 3291: loss = 3.9501 (0.502 sec/step)\n",
            "I0707 17:05:17.040949 140263261390720 learning.py:507] global step 3292: loss = 5.1890 (0.300 sec/step)\n",
            "I0707 17:05:17.375523 140263261390720 learning.py:507] global step 3293: loss = 4.7262 (0.333 sec/step)\n",
            "I0707 17:05:17.674106 140263261390720 learning.py:507] global step 3294: loss = 4.1761 (0.297 sec/step)\n",
            "I0707 17:05:18.016655 140263261390720 learning.py:507] global step 3295: loss = 4.5963 (0.341 sec/step)\n",
            "I0707 17:05:18.347555 140263261390720 learning.py:507] global step 3296: loss = 4.6715 (0.329 sec/step)\n",
            "I0707 17:05:18.662326 140263261390720 learning.py:507] global step 3297: loss = 3.1621 (0.313 sec/step)\n",
            "I0707 17:05:18.983349 140263261390720 learning.py:507] global step 3298: loss = 5.0246 (0.319 sec/step)\n",
            "I0707 17:05:19.449790 140263261390720 learning.py:507] global step 3299: loss = 4.3836 (0.464 sec/step)\n",
            "I0707 17:05:19.739332 140263261390720 learning.py:507] global step 3300: loss = 3.1561 (0.288 sec/step)\n",
            "I0707 17:05:20.060852 140263261390720 learning.py:507] global step 3301: loss = 3.4030 (0.319 sec/step)\n",
            "I0707 17:05:20.361519 140263261390720 learning.py:507] global step 3302: loss = 6.2109 (0.299 sec/step)\n",
            "I0707 17:05:20.681990 140263261390720 learning.py:507] global step 3303: loss = 2.9721 (0.318 sec/step)\n",
            "I0707 17:05:20.986236 140263261390720 learning.py:507] global step 3304: loss = 2.8072 (0.302 sec/step)\n",
            "I0707 17:05:21.297445 140263261390720 learning.py:507] global step 3305: loss = 4.0700 (0.309 sec/step)\n",
            "I0707 17:05:21.649014 140263261390720 learning.py:507] global step 3306: loss = 3.6380 (0.350 sec/step)\n",
            "I0707 17:05:21.954513 140263261390720 learning.py:507] global step 3307: loss = 5.7211 (0.304 sec/step)\n",
            "I0707 17:05:22.261749 140263261390720 learning.py:507] global step 3308: loss = 6.3519 (0.305 sec/step)\n",
            "I0707 17:05:22.579645 140263261390720 learning.py:507] global step 3309: loss = 4.7599 (0.316 sec/step)\n",
            "I0707 17:05:22.920181 140263261390720 learning.py:507] global step 3310: loss = 2.5781 (0.338 sec/step)\n",
            "I0707 17:05:23.259943 140263261390720 learning.py:507] global step 3311: loss = 3.6260 (0.338 sec/step)\n",
            "I0707 17:05:23.567206 140263261390720 learning.py:507] global step 3312: loss = 3.0043 (0.305 sec/step)\n",
            "I0707 17:05:23.892462 140263261390720 learning.py:507] global step 3313: loss = 3.9294 (0.324 sec/step)\n",
            "I0707 17:05:24.246294 140263261390720 learning.py:507] global step 3314: loss = 4.3425 (0.352 sec/step)\n",
            "I0707 17:05:24.541111 140263261390720 learning.py:507] global step 3315: loss = 4.7886 (0.292 sec/step)\n",
            "I0707 17:05:24.843929 140263261390720 learning.py:507] global step 3316: loss = 4.3409 (0.298 sec/step)\n",
            "I0707 17:05:25.159320 140263261390720 learning.py:507] global step 3317: loss = 5.8313 (0.313 sec/step)\n",
            "I0707 17:05:25.600288 140263261390720 learning.py:507] global step 3318: loss = 2.8553 (0.439 sec/step)\n",
            "I0707 17:05:26.069410 140263261390720 learning.py:507] global step 3319: loss = 4.9265 (0.467 sec/step)\n",
            "I0707 17:05:26.425280 140263261390720 learning.py:507] global step 3320: loss = 4.6595 (0.351 sec/step)\n",
            "I0707 17:05:26.754382 140263261390720 learning.py:507] global step 3321: loss = 6.0147 (0.327 sec/step)\n",
            "I0707 17:05:27.064965 140263261390720 learning.py:507] global step 3322: loss = 5.4832 (0.308 sec/step)\n",
            "I0707 17:05:27.392721 140263261390720 learning.py:507] global step 3323: loss = 3.8590 (0.326 sec/step)\n",
            "I0707 17:05:27.709436 140263261390720 learning.py:507] global step 3324: loss = 3.4795 (0.315 sec/step)\n",
            "I0707 17:05:28.166475 140263261390720 learning.py:507] global step 3325: loss = 3.9522 (0.455 sec/step)\n",
            "I0707 17:05:28.647310 140263261390720 learning.py:507] global step 3326: loss = 3.2322 (0.479 sec/step)\n",
            "I0707 17:05:29.028562 140263261390720 learning.py:507] global step 3327: loss = 5.0109 (0.380 sec/step)\n",
            "I0707 17:05:29.329357 140263261390720 learning.py:507] global step 3328: loss = 3.6962 (0.299 sec/step)\n",
            "I0707 17:05:29.677647 140263261390720 learning.py:507] global step 3329: loss = 3.5500 (0.347 sec/step)\n",
            "I0707 17:05:30.004812 140263261390720 learning.py:507] global step 3330: loss = 4.3097 (0.326 sec/step)\n",
            "I0707 17:05:30.339155 140263261390720 learning.py:507] global step 3331: loss = 3.4846 (0.333 sec/step)\n",
            "I0707 17:05:30.667962 140263261390720 learning.py:507] global step 3332: loss = 3.2513 (0.327 sec/step)\n",
            "I0707 17:05:31.017000 140263261390720 learning.py:507] global step 3333: loss = 3.7736 (0.346 sec/step)\n",
            "I0707 17:05:31.337325 140263261390720 learning.py:507] global step 3334: loss = 3.2775 (0.319 sec/step)\n",
            "I0707 17:05:31.656032 140263261390720 learning.py:507] global step 3335: loss = 5.2652 (0.317 sec/step)\n",
            "I0707 17:05:31.959283 140263261390720 learning.py:507] global step 3336: loss = 4.5466 (0.302 sec/step)\n",
            "I0707 17:05:32.306217 140263261390720 learning.py:507] global step 3337: loss = 3.4587 (0.345 sec/step)\n",
            "I0707 17:05:32.636378 140263261390720 learning.py:507] global step 3338: loss = 3.9725 (0.328 sec/step)\n",
            "I0707 17:05:32.985064 140263261390720 learning.py:507] global step 3339: loss = 2.8684 (0.347 sec/step)\n",
            "I0707 17:05:33.341437 140263261390720 learning.py:507] global step 3340: loss = 5.2403 (0.355 sec/step)\n",
            "I0707 17:05:33.655530 140263261390720 learning.py:507] global step 3341: loss = 3.5940 (0.312 sec/step)\n",
            "I0707 17:05:33.965599 140263261390720 learning.py:507] global step 3342: loss = 4.2198 (0.309 sec/step)\n",
            "I0707 17:05:34.283634 140263261390720 learning.py:507] global step 3343: loss = 3.9933 (0.316 sec/step)\n",
            "I0707 17:05:34.616772 140263261390720 learning.py:507] global step 3344: loss = 4.8400 (0.331 sec/step)\n",
            "I0707 17:05:34.911378 140263261390720 learning.py:507] global step 3345: loss = 2.7963 (0.293 sec/step)\n",
            "I0707 17:05:35.384366 140263261390720 learning.py:507] global step 3346: loss = 3.7359 (0.471 sec/step)\n",
            "I0707 17:05:35.875876 140263261390720 learning.py:507] global step 3347: loss = 4.5146 (0.490 sec/step)\n",
            "I0707 17:05:36.181298 140263261390720 learning.py:507] global step 3348: loss = 3.6805 (0.304 sec/step)\n",
            "I0707 17:05:36.491025 140263261390720 learning.py:507] global step 3349: loss = 3.8142 (0.308 sec/step)\n",
            "I0707 17:05:36.786135 140263261390720 learning.py:507] global step 3350: loss = 2.9647 (0.293 sec/step)\n",
            "I0707 17:05:37.078043 140263261390720 learning.py:507] global step 3351: loss = 4.3509 (0.290 sec/step)\n",
            "I0707 17:05:37.673778 140263261390720 learning.py:507] global step 3352: loss = 3.5866 (0.594 sec/step)\n",
            "I0707 17:05:38.131690 140263261390720 learning.py:507] global step 3353: loss = 4.6888 (0.456 sec/step)\n",
            "I0707 17:05:38.414194 140263261390720 learning.py:507] global step 3354: loss = 4.1671 (0.281 sec/step)\n",
            "I0707 17:05:38.834008 140263261390720 learning.py:507] global step 3355: loss = 4.5368 (0.418 sec/step)\n",
            "I0707 17:05:39.160712 140263261390720 learning.py:507] global step 3356: loss = 6.4348 (0.325 sec/step)\n",
            "I0707 17:05:39.446023 140263261390720 learning.py:507] global step 3357: loss = 4.7271 (0.284 sec/step)\n",
            "I0707 17:05:39.762486 140263261390720 learning.py:507] global step 3358: loss = 3.5521 (0.315 sec/step)\n",
            "I0707 17:05:40.067949 140263261390720 learning.py:507] global step 3359: loss = 3.5058 (0.304 sec/step)\n",
            "I0707 17:05:40.368714 140263261390720 learning.py:507] global step 3360: loss = 3.8930 (0.299 sec/step)\n",
            "I0707 17:05:40.671095 140263261390720 learning.py:507] global step 3361: loss = 5.7600 (0.301 sec/step)\n",
            "I0707 17:05:40.974148 140263261390720 learning.py:507] global step 3362: loss = 2.9055 (0.301 sec/step)\n",
            "I0707 17:05:41.367368 140263261390720 learning.py:507] global step 3363: loss = 6.4781 (0.392 sec/step)\n",
            "I0707 17:05:41.705925 140263261390720 learning.py:507] global step 3364: loss = 3.5660 (0.337 sec/step)\n",
            "I0707 17:05:42.012843 140263261390720 learning.py:507] global step 3365: loss = 3.5195 (0.305 sec/step)\n",
            "I0707 17:05:42.329918 140263261390720 learning.py:507] global step 3366: loss = 3.4872 (0.315 sec/step)\n",
            "I0707 17:05:42.623016 140263261390720 learning.py:507] global step 3367: loss = 3.7639 (0.291 sec/step)\n",
            "I0707 17:05:42.911944 140263261390720 learning.py:507] global step 3368: loss = 5.0244 (0.287 sec/step)\n",
            "I0707 17:05:43.218486 140263261390720 learning.py:507] global step 3369: loss = 4.6547 (0.305 sec/step)\n",
            "I0707 17:05:43.527990 140263261390720 learning.py:507] global step 3370: loss = 4.0542 (0.308 sec/step)\n",
            "I0707 17:05:43.952863 140263261390720 learning.py:507] global step 3371: loss = 5.2062 (0.422 sec/step)\n",
            "I0707 17:05:44.260350 140263261390720 learning.py:507] global step 3372: loss = 3.2129 (0.306 sec/step)\n",
            "I0707 17:05:44.571344 140263261390720 learning.py:507] global step 3373: loss = 3.0986 (0.309 sec/step)\n",
            "I0707 17:05:44.902365 140263261390720 learning.py:507] global step 3374: loss = 5.1233 (0.329 sec/step)\n",
            "I0707 17:05:45.347175 140263261390720 learning.py:507] global step 3375: loss = 4.7405 (0.443 sec/step)\n",
            "I0707 17:05:45.641022 140263261390720 learning.py:507] global step 3376: loss = 2.9691 (0.292 sec/step)\n",
            "I0707 17:05:45.945894 140263261390720 learning.py:507] global step 3377: loss = 5.4509 (0.303 sec/step)\n",
            "I0707 17:05:46.269672 140263261390720 learning.py:507] global step 3378: loss = 5.2622 (0.322 sec/step)\n",
            "I0707 17:05:46.553504 140263261390720 learning.py:507] global step 3379: loss = 4.7931 (0.282 sec/step)\n",
            "I0707 17:05:46.849105 140263261390720 learning.py:507] global step 3380: loss = 4.1633 (0.294 sec/step)\n",
            "I0707 17:05:47.386030 140263261390720 learning.py:507] global step 3381: loss = 4.7748 (0.535 sec/step)\n",
            "I0707 17:05:47.687732 140263261390720 learning.py:507] global step 3382: loss = 3.3482 (0.300 sec/step)\n",
            "I0707 17:05:47.991094 140263261390720 learning.py:507] global step 3383: loss = 4.1801 (0.302 sec/step)\n",
            "I0707 17:05:48.311344 140263261390720 learning.py:507] global step 3384: loss = 6.2270 (0.319 sec/step)\n",
            "I0707 17:05:48.608712 140263261390720 learning.py:507] global step 3385: loss = 4.0108 (0.296 sec/step)\n",
            "I0707 17:05:48.912276 140263261390720 learning.py:507] global step 3386: loss = 2.9318 (0.302 sec/step)\n",
            "I0707 17:05:49.196089 140263261390720 learning.py:507] global step 3387: loss = 4.3564 (0.282 sec/step)\n",
            "I0707 17:05:49.487305 140263261390720 learning.py:507] global step 3388: loss = 4.4828 (0.289 sec/step)\n",
            "I0707 17:05:49.803621 140263261390720 learning.py:507] global step 3389: loss = 4.1766 (0.315 sec/step)\n",
            "I0707 17:05:50.113349 140263261390720 learning.py:507] global step 3390: loss = 3.7836 (0.308 sec/step)\n",
            "I0707 17:05:50.578279 140263261390720 learning.py:507] global step 3391: loss = 3.1062 (0.463 sec/step)\n",
            "I0707 17:05:50.936256 140263261390720 learning.py:507] global step 3392: loss = 5.5290 (0.356 sec/step)\n",
            "I0707 17:05:51.272329 140263261390720 learning.py:507] global step 3393: loss = 3.6698 (0.334 sec/step)\n",
            "I0707 17:05:51.632075 140263261390720 learning.py:507] global step 3394: loss = 4.4884 (0.358 sec/step)\n",
            "I0707 17:05:51.934276 140263261390720 learning.py:507] global step 3395: loss = 3.5116 (0.301 sec/step)\n",
            "I0707 17:05:52.257767 140263261390720 learning.py:507] global step 3396: loss = 4.3800 (0.322 sec/step)\n",
            "I0707 17:05:52.789648 140263261390720 learning.py:507] global step 3397: loss = 4.3350 (0.530 sec/step)\n",
            "I0707 17:05:52.977123 140260304557824 supervisor.py:1117] Saving checkpoint to path training/model.ckpt\n",
            "I0707 17:05:53.695429 140263261390720 learning.py:507] global step 3398: loss = 3.9813 (0.806 sec/step)\n",
            "I0707 17:05:53.862210 140260287772416 supervisor.py:1050] Recording summary at step 3398.\n",
            "I0707 17:05:54.361198 140263261390720 learning.py:507] global step 3399: loss = 3.6992 (0.607 sec/step)\n",
            "I0707 17:05:54.976994 140263261390720 learning.py:507] global step 3400: loss = 4.6136 (0.603 sec/step)\n",
            "I0707 17:05:55.381901 140263261390720 learning.py:507] global step 3401: loss = 3.5978 (0.334 sec/step)\n",
            "I0707 17:05:55.730381 140263261390720 learning.py:507] global step 3402: loss = 4.2259 (0.347 sec/step)\n",
            "I0707 17:05:56.042492 140263261390720 learning.py:507] global step 3403: loss = 4.8661 (0.310 sec/step)\n",
            "I0707 17:05:56.381545 140263261390720 learning.py:507] global step 3404: loss = 4.4238 (0.337 sec/step)\n",
            "I0707 17:05:56.680544 140263261390720 learning.py:507] global step 3405: loss = 4.4551 (0.297 sec/step)\n",
            "I0707 17:05:56.973086 140263261390720 learning.py:507] global step 3406: loss = 4.0785 (0.291 sec/step)\n",
            "I0707 17:05:57.268776 140263261390720 learning.py:507] global step 3407: loss = 3.8093 (0.294 sec/step)\n",
            "I0707 17:05:57.603668 140263261390720 learning.py:507] global step 3408: loss = 5.0262 (0.333 sec/step)\n",
            "I0707 17:05:57.963298 140263261390720 learning.py:507] global step 3409: loss = 5.4511 (0.357 sec/step)\n",
            "I0707 17:05:58.299985 140263261390720 learning.py:507] global step 3410: loss = 4.3671 (0.335 sec/step)\n",
            "I0707 17:05:58.609624 140263261390720 learning.py:507] global step 3411: loss = 5.2261 (0.308 sec/step)\n",
            "I0707 17:05:58.967593 140263261390720 learning.py:507] global step 3412: loss = 4.7344 (0.356 sec/step)\n",
            "I0707 17:05:59.328295 140263261390720 learning.py:507] global step 3413: loss = 4.0410 (0.359 sec/step)\n",
            "I0707 17:05:59.646458 140263261390720 learning.py:507] global step 3414: loss = 5.4961 (0.316 sec/step)\n",
            "I0707 17:06:00.004236 140263261390720 learning.py:507] global step 3415: loss = 3.9217 (0.356 sec/step)\n",
            "I0707 17:06:00.382307 140263261390720 learning.py:507] global step 3416: loss = 3.3280 (0.376 sec/step)\n",
            "I0707 17:06:00.682130 140263261390720 learning.py:507] global step 3417: loss = 4.5026 (0.298 sec/step)\n",
            "I0707 17:06:00.986088 140263261390720 learning.py:507] global step 3418: loss = 5.0888 (0.302 sec/step)\n",
            "I0707 17:06:01.385179 140263261390720 learning.py:507] global step 3419: loss = 4.0535 (0.397 sec/step)\n",
            "I0707 17:06:01.691422 140263261390720 learning.py:507] global step 3420: loss = 4.0092 (0.305 sec/step)\n",
            "I0707 17:06:02.016424 140263261390720 learning.py:507] global step 3421: loss = 4.8799 (0.323 sec/step)\n",
            "I0707 17:06:02.455523 140263261390720 learning.py:507] global step 3422: loss = 4.8851 (0.437 sec/step)\n",
            "I0707 17:06:02.815391 140263261390720 learning.py:507] global step 3423: loss = 3.7430 (0.358 sec/step)\n",
            "I0707 17:06:03.110210 140263261390720 learning.py:507] global step 3424: loss = 4.3715 (0.293 sec/step)\n",
            "I0707 17:06:03.546661 140263261390720 learning.py:507] global step 3425: loss = 3.1925 (0.435 sec/step)\n",
            "I0707 17:06:03.896046 140263261390720 learning.py:507] global step 3426: loss = 4.3176 (0.348 sec/step)\n",
            "I0707 17:06:04.294554 140263261390720 learning.py:507] global step 3427: loss = 2.6268 (0.397 sec/step)\n",
            "I0707 17:06:04.610410 140263261390720 learning.py:507] global step 3428: loss = 4.9839 (0.314 sec/step)\n",
            "I0707 17:06:04.949587 140263261390720 learning.py:507] global step 3429: loss = 3.4289 (0.337 sec/step)\n",
            "I0707 17:06:05.259487 140263261390720 learning.py:507] global step 3430: loss = 4.2359 (0.308 sec/step)\n",
            "I0707 17:06:05.551782 140263261390720 learning.py:507] global step 3431: loss = 3.5701 (0.291 sec/step)\n",
            "I0707 17:06:05.894043 140263261390720 learning.py:507] global step 3432: loss = 6.7148 (0.340 sec/step)\n",
            "I0707 17:06:06.246446 140263261390720 learning.py:507] global step 3433: loss = 4.6537 (0.350 sec/step)\n",
            "I0707 17:06:06.540519 140263261390720 learning.py:507] global step 3434: loss = 3.7565 (0.291 sec/step)\n",
            "I0707 17:06:06.873776 140263261390720 learning.py:507] global step 3435: loss = 3.6543 (0.331 sec/step)\n",
            "I0707 17:06:07.186920 140263261390720 learning.py:507] global step 3436: loss = 4.1491 (0.312 sec/step)\n",
            "I0707 17:06:07.516551 140263261390720 learning.py:507] global step 3437: loss = 4.3270 (0.328 sec/step)\n",
            "I0707 17:06:07.869407 140263261390720 learning.py:507] global step 3438: loss = 4.6543 (0.351 sec/step)\n",
            "I0707 17:06:08.180989 140263261390720 learning.py:507] global step 3439: loss = 4.0198 (0.310 sec/step)\n",
            "I0707 17:06:08.710037 140263261390720 learning.py:507] global step 3440: loss = 3.3473 (0.527 sec/step)\n",
            "I0707 17:06:09.058895 140263261390720 learning.py:507] global step 3441: loss = 4.7333 (0.347 sec/step)\n",
            "I0707 17:06:09.360141 140263261390720 learning.py:507] global step 3442: loss = 3.8891 (0.300 sec/step)\n",
            "I0707 17:06:09.662487 140263261390720 learning.py:507] global step 3443: loss = 4.2579 (0.301 sec/step)\n",
            "I0707 17:06:09.977178 140263261390720 learning.py:507] global step 3444: loss = 4.2673 (0.313 sec/step)\n",
            "I0707 17:06:10.281274 140263261390720 learning.py:507] global step 3445: loss = 5.0191 (0.302 sec/step)\n",
            "I0707 17:06:10.696031 140263261390720 learning.py:507] global step 3446: loss = 3.5699 (0.413 sec/step)\n",
            "I0707 17:06:11.004791 140263261390720 learning.py:507] global step 3447: loss = 3.9645 (0.307 sec/step)\n",
            "I0707 17:06:11.327928 140263261390720 learning.py:507] global step 3448: loss = 5.2335 (0.321 sec/step)\n",
            "I0707 17:06:11.659885 140263261390720 learning.py:507] global step 3449: loss = 3.6889 (0.330 sec/step)\n",
            "I0707 17:06:11.998127 140263261390720 learning.py:507] global step 3450: loss = 3.7202 (0.337 sec/step)\n",
            "I0707 17:06:12.275409 140263261390720 learning.py:507] global step 3451: loss = 3.7905 (0.276 sec/step)\n",
            "I0707 17:06:12.598201 140263261390720 learning.py:507] global step 3452: loss = 4.0810 (0.321 sec/step)\n",
            "I0707 17:06:12.916219 140263261390720 learning.py:507] global step 3453: loss = 4.9003 (0.316 sec/step)\n",
            "I0707 17:06:13.241141 140263261390720 learning.py:507] global step 3454: loss = 3.7524 (0.323 sec/step)\n",
            "I0707 17:06:13.544645 140263261390720 learning.py:507] global step 3455: loss = 3.1653 (0.302 sec/step)\n",
            "I0707 17:06:13.865159 140263261390720 learning.py:507] global step 3456: loss = 3.8729 (0.318 sec/step)\n",
            "I0707 17:06:14.157284 140263261390720 learning.py:507] global step 3457: loss = 4.8229 (0.290 sec/step)\n",
            "I0707 17:06:14.475959 140263261390720 learning.py:507] global step 3458: loss = 2.4223 (0.315 sec/step)\n",
            "I0707 17:06:14.801713 140263261390720 learning.py:507] global step 3459: loss = 2.8622 (0.324 sec/step)\n",
            "I0707 17:06:15.184521 140263261390720 learning.py:507] global step 3460: loss = 3.7417 (0.381 sec/step)\n",
            "I0707 17:06:15.491735 140263261390720 learning.py:507] global step 3461: loss = 3.8493 (0.306 sec/step)\n",
            "I0707 17:06:15.835543 140263261390720 learning.py:507] global step 3462: loss = 4.0788 (0.342 sec/step)\n",
            "I0707 17:06:16.158149 140263261390720 learning.py:507] global step 3463: loss = 3.3181 (0.321 sec/step)\n",
            "I0707 17:06:16.467182 140263261390720 learning.py:507] global step 3464: loss = 4.2962 (0.307 sec/step)\n",
            "I0707 17:06:16.771858 140263261390720 learning.py:507] global step 3465: loss = 4.3182 (0.303 sec/step)\n",
            "I0707 17:06:17.164161 140263261390720 learning.py:507] global step 3466: loss = 3.4578 (0.391 sec/step)\n",
            "I0707 17:06:17.492472 140263261390720 learning.py:507] global step 3467: loss = 4.0872 (0.327 sec/step)\n",
            "I0707 17:06:17.816410 140263261390720 learning.py:507] global step 3468: loss = 3.0244 (0.322 sec/step)\n",
            "I0707 17:06:18.159575 140263261390720 learning.py:507] global step 3469: loss = 4.0558 (0.342 sec/step)\n",
            "I0707 17:06:18.500231 140263261390720 learning.py:507] global step 3470: loss = 3.0081 (0.339 sec/step)\n",
            "I0707 17:06:18.801803 140263261390720 learning.py:507] global step 3471: loss = 3.1659 (0.300 sec/step)\n",
            "I0707 17:06:19.097501 140263261390720 learning.py:507] global step 3472: loss = 3.0760 (0.294 sec/step)\n",
            "I0707 17:06:19.409305 140263261390720 learning.py:507] global step 3473: loss = 5.0056 (0.310 sec/step)\n",
            "I0707 17:06:19.716537 140263261390720 learning.py:507] global step 3474: loss = 3.5303 (0.305 sec/step)\n",
            "I0707 17:06:20.064174 140263261390720 learning.py:507] global step 3475: loss = 4.2742 (0.346 sec/step)\n",
            "I0707 17:06:20.363247 140263261390720 learning.py:507] global step 3476: loss = 3.6242 (0.297 sec/step)\n",
            "I0707 17:06:20.673011 140263261390720 learning.py:507] global step 3477: loss = 4.0019 (0.308 sec/step)\n",
            "I0707 17:06:20.982595 140263261390720 learning.py:507] global step 3478: loss = 4.6916 (0.308 sec/step)\n",
            "I0707 17:06:21.268188 140263261390720 learning.py:507] global step 3479: loss = 3.2194 (0.284 sec/step)\n",
            "I0707 17:06:21.574452 140263261390720 learning.py:507] global step 3480: loss = 4.4039 (0.304 sec/step)\n",
            "I0707 17:06:21.879665 140263261390720 learning.py:507] global step 3481: loss = 4.4090 (0.303 sec/step)\n",
            "I0707 17:06:22.196763 140263261390720 learning.py:507] global step 3482: loss = 3.1746 (0.315 sec/step)\n",
            "I0707 17:06:22.499746 140263261390720 learning.py:507] global step 3483: loss = 3.4088 (0.301 sec/step)\n",
            "I0707 17:06:22.819013 140263261390720 learning.py:507] global step 3484: loss = 3.9026 (0.318 sec/step)\n",
            "I0707 17:06:23.113495 140263261390720 learning.py:507] global step 3485: loss = 4.2650 (0.293 sec/step)\n",
            "I0707 17:06:23.441194 140263261390720 learning.py:507] global step 3486: loss = 3.6370 (0.326 sec/step)\n",
            "I0707 17:06:23.734348 140263261390720 learning.py:507] global step 3487: loss = 4.2392 (0.291 sec/step)\n",
            "I0707 17:06:24.072760 140263261390720 learning.py:507] global step 3488: loss = 4.1167 (0.336 sec/step)\n",
            "I0707 17:06:24.374330 140263261390720 learning.py:507] global step 3489: loss = 4.0127 (0.300 sec/step)\n",
            "I0707 17:06:24.681149 140263261390720 learning.py:507] global step 3490: loss = 3.4871 (0.305 sec/step)\n",
            "I0707 17:06:24.978771 140263261390720 learning.py:507] global step 3491: loss = 3.3691 (0.296 sec/step)\n",
            "I0707 17:06:25.327129 140263261390720 learning.py:507] global step 3492: loss = 3.7751 (0.347 sec/step)\n",
            "I0707 17:06:25.613245 140263261390720 learning.py:507] global step 3493: loss = 3.9613 (0.285 sec/step)\n",
            "I0707 17:06:25.943572 140263261390720 learning.py:507] global step 3494: loss = 4.0746 (0.329 sec/step)\n",
            "I0707 17:06:26.390467 140263261390720 learning.py:507] global step 3495: loss = 5.4470 (0.445 sec/step)\n",
            "I0707 17:06:26.726480 140263261390720 learning.py:507] global step 3496: loss = 3.8343 (0.334 sec/step)\n",
            "I0707 17:06:27.029301 140263261390720 learning.py:507] global step 3497: loss = 3.7375 (0.301 sec/step)\n",
            "I0707 17:06:27.403558 140263261390720 learning.py:507] global step 3498: loss = 3.2190 (0.373 sec/step)\n",
            "I0707 17:06:27.701477 140263261390720 learning.py:507] global step 3499: loss = 3.8183 (0.296 sec/step)\n",
            "I0707 17:06:27.998147 140263261390720 learning.py:507] global step 3500: loss = 3.1980 (0.295 sec/step)\n",
            "I0707 17:06:28.453033 140263261390720 learning.py:507] global step 3501: loss = 4.6921 (0.453 sec/step)\n",
            "I0707 17:06:28.775962 140263261390720 learning.py:507] global step 3502: loss = 3.9004 (0.321 sec/step)\n",
            "I0707 17:06:29.229361 140263261390720 learning.py:507] global step 3503: loss = 4.8567 (0.452 sec/step)\n",
            "I0707 17:06:29.518955 140263261390720 learning.py:507] global step 3504: loss = 4.4629 (0.288 sec/step)\n",
            "I0707 17:06:29.852279 140263261390720 learning.py:507] global step 3505: loss = 4.0814 (0.332 sec/step)\n",
            "I0707 17:06:30.196492 140263261390720 learning.py:507] global step 3506: loss = 4.5524 (0.342 sec/step)\n",
            "I0707 17:06:30.517616 140263261390720 learning.py:507] global step 3507: loss = 4.6857 (0.319 sec/step)\n",
            "I0707 17:06:30.823794 140263261390720 learning.py:507] global step 3508: loss = 3.4774 (0.304 sec/step)\n",
            "I0707 17:06:31.158438 140263261390720 learning.py:507] global step 3509: loss = 4.5560 (0.333 sec/step)\n",
            "I0707 17:06:31.496283 140263261390720 learning.py:507] global step 3510: loss = 3.3651 (0.336 sec/step)\n",
            "I0707 17:06:31.815510 140263261390720 learning.py:507] global step 3511: loss = 3.3975 (0.318 sec/step)\n",
            "I0707 17:06:32.166419 140263261390720 learning.py:507] global step 3512: loss = 4.4990 (0.349 sec/step)\n",
            "I0707 17:06:32.469790 140263261390720 learning.py:507] global step 3513: loss = 3.4634 (0.302 sec/step)\n",
            "I0707 17:06:32.772112 140263261390720 learning.py:507] global step 3514: loss = 5.1895 (0.301 sec/step)\n",
            "I0707 17:06:33.100557 140263261390720 learning.py:507] global step 3515: loss = 4.8313 (0.327 sec/step)\n",
            "I0707 17:06:33.418762 140263261390720 learning.py:507] global step 3516: loss = 3.2750 (0.316 sec/step)\n",
            "I0707 17:06:33.729192 140263261390720 learning.py:507] global step 3517: loss = 4.7631 (0.309 sec/step)\n",
            "I0707 17:06:34.060530 140263261390720 learning.py:507] global step 3518: loss = 5.2296 (0.328 sec/step)\n",
            "I0707 17:06:34.396619 140263261390720 learning.py:507] global step 3519: loss = 5.3404 (0.334 sec/step)\n",
            "I0707 17:06:34.689451 140263261390720 learning.py:507] global step 3520: loss = 4.8826 (0.291 sec/step)\n",
            "I0707 17:06:34.991640 140263261390720 learning.py:507] global step 3521: loss = 3.6388 (0.300 sec/step)\n",
            "I0707 17:06:35.316443 140263261390720 learning.py:507] global step 3522: loss = 5.0659 (0.323 sec/step)\n",
            "I0707 17:06:35.662447 140263261390720 learning.py:507] global step 3523: loss = 4.6107 (0.344 sec/step)\n",
            "I0707 17:06:36.015034 140263261390720 learning.py:507] global step 3524: loss = 3.3043 (0.351 sec/step)\n",
            "I0707 17:06:36.327657 140263261390720 learning.py:507] global step 3525: loss = 3.8368 (0.311 sec/step)\n",
            "I0707 17:06:36.658585 140263261390720 learning.py:507] global step 3526: loss = 3.8064 (0.329 sec/step)\n",
            "I0707 17:06:36.952321 140263261390720 learning.py:507] global step 3527: loss = 4.2758 (0.292 sec/step)\n",
            "I0707 17:06:37.329262 140263261390720 learning.py:507] global step 3528: loss = 4.3782 (0.375 sec/step)\n",
            "I0707 17:06:37.746847 140263261390720 learning.py:507] global step 3529: loss = 4.1424 (0.416 sec/step)\n",
            "I0707 17:06:38.167524 140263261390720 learning.py:507] global step 3530: loss = 2.5983 (0.419 sec/step)\n",
            "I0707 17:06:38.490174 140263261390720 learning.py:507] global step 3531: loss = 2.9855 (0.321 sec/step)\n",
            "I0707 17:06:38.822793 140263261390720 learning.py:507] global step 3532: loss = 4.2130 (0.331 sec/step)\n",
            "I0707 17:06:39.117860 140263261390720 learning.py:507] global step 3533: loss = 3.6810 (0.293 sec/step)\n",
            "I0707 17:06:39.419812 140263261390720 learning.py:507] global step 3534: loss = 3.4933 (0.300 sec/step)\n",
            "I0707 17:06:39.795799 140263261390720 learning.py:507] global step 3535: loss = 4.8349 (0.374 sec/step)\n",
            "I0707 17:06:40.222058 140263261390720 learning.py:507] global step 3536: loss = 4.4262 (0.424 sec/step)\n",
            "I0707 17:06:40.517692 140263261390720 learning.py:507] global step 3537: loss = 4.6972 (0.294 sec/step)\n",
            "I0707 17:06:40.851594 140263261390720 learning.py:507] global step 3538: loss = 4.2153 (0.332 sec/step)\n",
            "I0707 17:06:41.275464 140263261390720 learning.py:507] global step 3539: loss = 5.1643 (0.422 sec/step)\n",
            "I0707 17:06:41.580197 140263261390720 learning.py:507] global step 3540: loss = 3.6113 (0.303 sec/step)\n",
            "I0707 17:06:41.920765 140263261390720 learning.py:507] global step 3541: loss = 3.6816 (0.339 sec/step)\n",
            "I0707 17:06:42.232352 140263261390720 learning.py:507] global step 3542: loss = 3.8595 (0.310 sec/step)\n",
            "I0707 17:06:42.595735 140263261390720 learning.py:507] global step 3543: loss = 5.0112 (0.362 sec/step)\n",
            "I0707 17:06:42.926165 140263261390720 learning.py:507] global step 3544: loss = 5.9204 (0.329 sec/step)\n",
            "I0707 17:06:43.245236 140263261390720 learning.py:507] global step 3545: loss = 4.0277 (0.317 sec/step)\n",
            "I0707 17:06:43.552122 140263261390720 learning.py:507] global step 3546: loss = 4.1711 (0.305 sec/step)\n",
            "I0707 17:06:43.905620 140263261390720 learning.py:507] global step 3547: loss = 4.5874 (0.352 sec/step)\n",
            "I0707 17:06:44.356653 140263261390720 learning.py:507] global step 3548: loss = 3.6494 (0.449 sec/step)\n",
            "I0707 17:06:44.705224 140263261390720 learning.py:507] global step 3549: loss = 4.0828 (0.346 sec/step)\n",
            "I0707 17:06:45.032797 140263261390720 learning.py:507] global step 3550: loss = 3.7441 (0.326 sec/step)\n",
            "I0707 17:06:45.361498 140263261390720 learning.py:507] global step 3551: loss = 3.7530 (0.327 sec/step)\n",
            "I0707 17:06:45.699903 140263261390720 learning.py:507] global step 3552: loss = 3.7676 (0.336 sec/step)\n",
            "I0707 17:06:46.068856 140263261390720 learning.py:507] global step 3553: loss = 3.2046 (0.367 sec/step)\n",
            "I0707 17:06:46.371076 140263261390720 learning.py:507] global step 3554: loss = 3.4712 (0.300 sec/step)\n",
            "I0707 17:06:46.655138 140263261390720 learning.py:507] global step 3555: loss = 5.2832 (0.282 sec/step)\n",
            "I0707 17:06:47.001059 140263261390720 learning.py:507] global step 3556: loss = 5.1636 (0.344 sec/step)\n",
            "I0707 17:06:47.319733 140263261390720 learning.py:507] global step 3557: loss = 2.9451 (0.317 sec/step)\n",
            "I0707 17:06:47.683366 140263261390720 learning.py:507] global step 3558: loss = 3.9633 (0.362 sec/step)\n",
            "I0707 17:06:48.007256 140263261390720 learning.py:507] global step 3559: loss = 4.8979 (0.322 sec/step)\n",
            "I0707 17:06:48.323863 140263261390720 learning.py:507] global step 3560: loss = 4.1188 (0.315 sec/step)\n",
            "I0707 17:06:48.639955 140263261390720 learning.py:507] global step 3561: loss = 3.6373 (0.314 sec/step)\n",
            "I0707 17:06:49.008278 140263261390720 learning.py:507] global step 3562: loss = 4.9073 (0.367 sec/step)\n",
            "I0707 17:06:49.304515 140263261390720 learning.py:507] global step 3563: loss = 3.1604 (0.294 sec/step)\n",
            "I0707 17:06:49.629284 140263261390720 learning.py:507] global step 3564: loss = 4.3351 (0.323 sec/step)\n",
            "I0707 17:06:49.941750 140263261390720 learning.py:507] global step 3565: loss = 3.3622 (0.310 sec/step)\n",
            "I0707 17:06:50.259250 140263261390720 learning.py:507] global step 3566: loss = 3.5855 (0.316 sec/step)\n",
            "I0707 17:06:50.627629 140263261390720 learning.py:507] global step 3567: loss = 2.7862 (0.367 sec/step)\n",
            "I0707 17:06:51.072660 140263261390720 learning.py:507] global step 3568: loss = 3.6272 (0.443 sec/step)\n",
            "I0707 17:06:51.381672 140263261390720 learning.py:507] global step 3569: loss = 3.4049 (0.307 sec/step)\n",
            "I0707 17:06:51.720629 140263261390720 learning.py:507] global step 3570: loss = 3.0085 (0.337 sec/step)\n",
            "I0707 17:06:52.053746 140263261390720 learning.py:507] global step 3571: loss = 3.8277 (0.332 sec/step)\n",
            "I0707 17:06:52.351812 140263261390720 learning.py:507] global step 3572: loss = 3.0421 (0.296 sec/step)\n",
            "I0707 17:06:52.737508 140263261390720 learning.py:507] global step 3573: loss = 3.1814 (0.384 sec/step)\n",
            "I0707 17:06:53.321340 140263261390720 learning.py:507] global step 3574: loss = 3.3510 (0.582 sec/step)\n",
            "I0707 17:06:53.658627 140263261390720 learning.py:507] global step 3575: loss = 3.8723 (0.336 sec/step)\n",
            "I0707 17:06:53.967408 140263261390720 learning.py:507] global step 3576: loss = 3.8727 (0.307 sec/step)\n",
            "I0707 17:06:54.337620 140263261390720 learning.py:507] global step 3577: loss = 4.3654 (0.369 sec/step)\n",
            "I0707 17:06:54.672150 140263261390720 learning.py:507] global step 3578: loss = 3.7216 (0.333 sec/step)\n",
            "I0707 17:06:54.966149 140263261390720 learning.py:507] global step 3579: loss = 3.7339 (0.292 sec/step)\n",
            "I0707 17:06:55.280444 140263261390720 learning.py:507] global step 3580: loss = 3.4297 (0.313 sec/step)\n",
            "I0707 17:06:55.589874 140263261390720 learning.py:507] global step 3581: loss = 2.9727 (0.308 sec/step)\n",
            "I0707 17:06:55.925187 140263261390720 learning.py:507] global step 3582: loss = 4.6624 (0.333 sec/step)\n",
            "I0707 17:06:56.211704 140263261390720 learning.py:507] global step 3583: loss = 4.0616 (0.285 sec/step)\n",
            "I0707 17:06:56.552622 140263261390720 learning.py:507] global step 3584: loss = 3.7741 (0.339 sec/step)\n",
            "I0707 17:06:56.888031 140263261390720 learning.py:507] global step 3585: loss = 4.4579 (0.334 sec/step)\n",
            "I0707 17:06:57.200755 140263261390720 learning.py:507] global step 3586: loss = 3.6173 (0.311 sec/step)\n",
            "I0707 17:06:57.627119 140263261390720 learning.py:507] global step 3587: loss = 5.1296 (0.425 sec/step)\n",
            "I0707 17:06:57.988591 140263261390720 learning.py:507] global step 3588: loss = 4.5278 (0.360 sec/step)\n",
            "I0707 17:06:58.283962 140263261390720 learning.py:507] global step 3589: loss = 3.9994 (0.294 sec/step)\n",
            "I0707 17:06:58.591527 140263261390720 learning.py:507] global step 3590: loss = 5.7875 (0.306 sec/step)\n",
            "I0707 17:06:58.904895 140263261390720 learning.py:507] global step 3591: loss = 3.8753 (0.312 sec/step)\n",
            "I0707 17:06:59.217562 140263261390720 learning.py:507] global step 3592: loss = 3.5707 (0.311 sec/step)\n",
            "I0707 17:06:59.561298 140263261390720 learning.py:507] global step 3593: loss = 3.5131 (0.342 sec/step)\n",
            "I0707 17:06:59.955715 140263261390720 learning.py:507] global step 3594: loss = 2.8734 (0.393 sec/step)\n",
            "I0707 17:07:00.265651 140263261390720 learning.py:507] global step 3595: loss = 3.2914 (0.308 sec/step)\n",
            "I0707 17:07:00.688645 140263261390720 learning.py:507] global step 3596: loss = 4.3460 (0.421 sec/step)\n",
            "I0707 17:07:00.996796 140263261390720 learning.py:507] global step 3597: loss = 5.0850 (0.307 sec/step)\n",
            "I0707 17:07:01.312534 140263261390720 learning.py:507] global step 3598: loss = 3.4255 (0.313 sec/step)\n",
            "I0707 17:07:01.650468 140263261390720 learning.py:507] global step 3599: loss = 4.3197 (0.336 sec/step)\n",
            "I0707 17:07:02.036736 140263261390720 learning.py:507] global step 3600: loss = 3.3641 (0.385 sec/step)\n",
            "I0707 17:07:02.371795 140263261390720 learning.py:507] global step 3601: loss = 3.2439 (0.333 sec/step)\n",
            "I0707 17:07:02.702044 140263261390720 learning.py:507] global step 3602: loss = 3.5247 (0.329 sec/step)\n",
            "I0707 17:07:03.010126 140263261390720 learning.py:507] global step 3603: loss = 3.5316 (0.306 sec/step)\n",
            "I0707 17:07:03.316638 140263261390720 learning.py:507] global step 3604: loss = 3.2664 (0.305 sec/step)\n",
            "I0707 17:07:03.649158 140263261390720 learning.py:507] global step 3605: loss = 4.7858 (0.331 sec/step)\n",
            "I0707 17:07:03.946714 140263261390720 learning.py:507] global step 3606: loss = 3.1986 (0.296 sec/step)\n",
            "I0707 17:07:04.277446 140263261390720 learning.py:507] global step 3607: loss = 3.1312 (0.329 sec/step)\n",
            "I0707 17:07:04.668783 140263261390720 learning.py:507] global step 3608: loss = 5.2311 (0.390 sec/step)\n",
            "I0707 17:07:05.027454 140263261390720 learning.py:507] global step 3609: loss = 4.3866 (0.357 sec/step)\n",
            "I0707 17:07:05.342720 140263261390720 learning.py:507] global step 3610: loss = 4.2776 (0.314 sec/step)\n",
            "I0707 17:07:05.684585 140263261390720 learning.py:507] global step 3611: loss = 4.2700 (0.340 sec/step)\n",
            "I0707 17:07:06.004573 140263261390720 learning.py:507] global step 3612: loss = 4.6933 (0.318 sec/step)\n",
            "I0707 17:07:06.341912 140263261390720 learning.py:507] global step 3613: loss = 3.7258 (0.335 sec/step)\n",
            "I0707 17:07:06.773792 140263261390720 learning.py:507] global step 3614: loss = 5.1555 (0.430 sec/step)\n",
            "I0707 17:07:07.074133 140263261390720 learning.py:507] global step 3615: loss = 3.6827 (0.299 sec/step)\n",
            "I0707 17:07:07.440367 140263261390720 learning.py:507] global step 3616: loss = 4.0209 (0.364 sec/step)\n",
            "I0707 17:07:07.746181 140263261390720 learning.py:507] global step 3617: loss = 4.1755 (0.304 sec/step)\n",
            "I0707 17:07:08.078935 140263261390720 learning.py:507] global step 3618: loss = 4.3763 (0.331 sec/step)\n",
            "I0707 17:07:08.393684 140263261390720 learning.py:507] global step 3619: loss = 4.0238 (0.313 sec/step)\n",
            "I0707 17:07:08.684085 140263261390720 learning.py:507] global step 3620: loss = 2.5249 (0.289 sec/step)\n",
            "I0707 17:07:08.982294 140263261390720 learning.py:507] global step 3621: loss = 2.5110 (0.296 sec/step)\n",
            "I0707 17:07:09.426395 140263261390720 learning.py:507] global step 3622: loss = 2.9313 (0.442 sec/step)\n",
            "I0707 17:07:09.722740 140263261390720 learning.py:507] global step 3623: loss = 3.3581 (0.295 sec/step)\n",
            "I0707 17:07:10.052611 140263261390720 learning.py:507] global step 3624: loss = 4.3216 (0.328 sec/step)\n",
            "I0707 17:07:10.380200 140263261390720 learning.py:507] global step 3625: loss = 4.6911 (0.326 sec/step)\n",
            "I0707 17:07:10.703559 140263261390720 learning.py:507] global step 3626: loss = 3.7932 (0.322 sec/step)\n",
            "I0707 17:07:10.995169 140263261390720 learning.py:507] global step 3627: loss = 3.6874 (0.290 sec/step)\n",
            "I0707 17:07:11.309374 140263261390720 learning.py:507] global step 3628: loss = 4.3305 (0.312 sec/step)\n",
            "I0707 17:07:11.756456 140263261390720 learning.py:507] global step 3629: loss = 3.5169 (0.445 sec/step)\n",
            "I0707 17:07:12.057603 140263261390720 learning.py:507] global step 3630: loss = 3.2239 (0.300 sec/step)\n",
            "I0707 17:07:12.363729 140263261390720 learning.py:507] global step 3631: loss = 3.9794 (0.304 sec/step)\n",
            "I0707 17:07:12.686623 140263261390720 learning.py:507] global step 3632: loss = 5.2108 (0.321 sec/step)\n",
            "I0707 17:07:12.996511 140263261390720 learning.py:507] global step 3633: loss = 4.2868 (0.308 sec/step)\n",
            "I0707 17:07:13.432963 140263261390720 learning.py:507] global step 3634: loss = 3.3762 (0.435 sec/step)\n",
            "I0707 17:07:13.795239 140263261390720 learning.py:507] global step 3635: loss = 3.9163 (0.361 sec/step)\n",
            "I0707 17:07:14.090526 140263261390720 learning.py:507] global step 3636: loss = 3.4859 (0.293 sec/step)\n",
            "I0707 17:07:14.468763 140263261390720 learning.py:507] global step 3637: loss = 3.3806 (0.376 sec/step)\n",
            "I0707 17:07:14.804721 140263261390720 learning.py:507] global step 3638: loss = 3.4417 (0.334 sec/step)\n",
            "I0707 17:07:15.124059 140263261390720 learning.py:507] global step 3639: loss = 5.0182 (0.318 sec/step)\n",
            "I0707 17:07:15.441355 140263261390720 learning.py:507] global step 3640: loss = 3.7250 (0.316 sec/step)\n",
            "I0707 17:07:15.747031 140263261390720 learning.py:507] global step 3641: loss = 3.4313 (0.304 sec/step)\n",
            "I0707 17:07:16.191229 140263261390720 learning.py:507] global step 3642: loss = 3.4124 (0.443 sec/step)\n",
            "I0707 17:07:16.587883 140263261390720 learning.py:507] global step 3643: loss = 3.3469 (0.395 sec/step)\n",
            "I0707 17:07:16.883789 140263261390720 learning.py:507] global step 3644: loss = 4.1962 (0.294 sec/step)\n",
            "I0707 17:07:17.177084 140263261390720 learning.py:507] global step 3645: loss = 3.9672 (0.292 sec/step)\n",
            "I0707 17:07:17.519392 140263261390720 learning.py:507] global step 3646: loss = 4.0842 (0.341 sec/step)\n",
            "I0707 17:07:17.838573 140263261390720 learning.py:507] global step 3647: loss = 3.7315 (0.317 sec/step)\n",
            "I0707 17:07:18.152162 140263261390720 learning.py:507] global step 3648: loss = 3.4517 (0.312 sec/step)\n",
            "I0707 17:07:18.475052 140263261390720 learning.py:507] global step 3649: loss = 4.3445 (0.321 sec/step)\n",
            "I0707 17:07:18.783910 140263261390720 learning.py:507] global step 3650: loss = 4.5008 (0.307 sec/step)\n",
            "I0707 17:07:19.094481 140263261390720 learning.py:507] global step 3651: loss = 3.8345 (0.309 sec/step)\n",
            "I0707 17:07:19.458195 140263261390720 learning.py:507] global step 3652: loss = 16.3611 (0.362 sec/step)\n",
            "I0707 17:07:19.777891 140263261390720 learning.py:507] global step 3653: loss = 3.3323 (0.317 sec/step)\n",
            "I0707 17:07:20.074200 140263261390720 learning.py:507] global step 3654: loss = 2.9866 (0.295 sec/step)\n",
            "I0707 17:07:20.374141 140263261390720 learning.py:507] global step 3655: loss = 4.2467 (0.298 sec/step)\n",
            "I0707 17:07:20.677767 140263261390720 learning.py:507] global step 3656: loss = 3.4798 (0.302 sec/step)\n",
            "I0707 17:07:20.994370 140263261390720 learning.py:507] global step 3657: loss = 3.9931 (0.315 sec/step)\n",
            "I0707 17:07:21.348325 140263261390720 learning.py:507] global step 3658: loss = 3.5361 (0.352 sec/step)\n",
            "I0707 17:07:21.687464 140263261390720 learning.py:507] global step 3659: loss = 4.4580 (0.337 sec/step)\n",
            "I0707 17:07:21.996247 140263261390720 learning.py:507] global step 3660: loss = 3.5443 (0.307 sec/step)\n",
            "I0707 17:07:22.299442 140263261390720 learning.py:507] global step 3661: loss = 3.5756 (0.302 sec/step)\n",
            "I0707 17:07:22.772928 140263261390720 learning.py:507] global step 3662: loss = 4.0744 (0.472 sec/step)\n",
            "I0707 17:07:23.141726 140263261390720 learning.py:507] global step 3663: loss = 4.7496 (0.367 sec/step)\n",
            "I0707 17:07:23.437210 140263261390720 learning.py:507] global step 3664: loss = 5.4004 (0.294 sec/step)\n",
            "I0707 17:07:23.747166 140263261390720 learning.py:507] global step 3665: loss = 3.5970 (0.308 sec/step)\n",
            "I0707 17:07:24.082093 140263261390720 learning.py:507] global step 3666: loss = 4.1250 (0.333 sec/step)\n",
            "I0707 17:07:24.407215 140263261390720 learning.py:507] global step 3667: loss = 3.5429 (0.324 sec/step)\n",
            "I0707 17:07:24.973735 140263261390720 learning.py:507] global step 3668: loss = 3.2438 (0.565 sec/step)\n",
            "I0707 17:07:25.347739 140263261390720 learning.py:507] global step 3669: loss = 4.5565 (0.372 sec/step)\n",
            "I0707 17:07:25.653566 140263261390720 learning.py:507] global step 3670: loss = 3.7316 (0.304 sec/step)\n",
            "I0707 17:07:25.961361 140263261390720 learning.py:507] global step 3671: loss = 3.6204 (0.306 sec/step)\n",
            "I0707 17:07:26.322817 140263261390720 learning.py:507] global step 3672: loss = 3.9319 (0.360 sec/step)\n",
            "I0707 17:07:26.623694 140263261390720 learning.py:507] global step 3673: loss = 4.6806 (0.297 sec/step)\n",
            "I0707 17:07:26.923372 140263261390720 learning.py:507] global step 3674: loss = 4.7959 (0.298 sec/step)\n",
            "I0707 17:07:27.236188 140263261390720 learning.py:507] global step 3675: loss = 4.1133 (0.311 sec/step)\n",
            "I0707 17:07:27.544377 140263261390720 learning.py:507] global step 3676: loss = 3.2416 (0.306 sec/step)\n",
            "I0707 17:07:27.846960 140263261390720 learning.py:507] global step 3677: loss = 4.3257 (0.301 sec/step)\n",
            "I0707 17:07:28.169782 140263261390720 learning.py:507] global step 3678: loss = 4.9598 (0.321 sec/step)\n",
            "I0707 17:07:28.471265 140263261390720 learning.py:507] global step 3679: loss = 4.4859 (0.300 sec/step)\n",
            "I0707 17:07:28.777460 140263261390720 learning.py:507] global step 3680: loss = 5.3813 (0.305 sec/step)\n",
            "I0707 17:07:29.088639 140263261390720 learning.py:507] global step 3681: loss = 4.5430 (0.310 sec/step)\n",
            "I0707 17:07:29.391492 140263261390720 learning.py:507] global step 3682: loss = 3.8586 (0.301 sec/step)\n",
            "I0707 17:07:29.685739 140263261390720 learning.py:507] global step 3683: loss = 3.5432 (0.292 sec/step)\n",
            "I0707 17:07:30.062726 140263261390720 learning.py:507] global step 3684: loss = 2.1463 (0.375 sec/step)\n",
            "I0707 17:07:30.365616 140263261390720 learning.py:507] global step 3685: loss = 5.3080 (0.301 sec/step)\n",
            "I0707 17:07:30.660949 140263261390720 learning.py:507] global step 3686: loss = 2.9121 (0.294 sec/step)\n",
            "I0707 17:07:31.007238 140263261390720 learning.py:507] global step 3687: loss = 3.5210 (0.345 sec/step)\n",
            "I0707 17:07:31.408115 140263261390720 learning.py:507] global step 3688: loss = 3.7535 (0.399 sec/step)\n",
            "I0707 17:07:31.700073 140263261390720 learning.py:507] global step 3689: loss = 3.3545 (0.290 sec/step)\n",
            "I0707 17:07:32.021169 140263261390720 learning.py:507] global step 3690: loss = 4.6162 (0.320 sec/step)\n",
            "I0707 17:07:32.457149 140263261390720 learning.py:507] global step 3691: loss = 3.2507 (0.434 sec/step)\n",
            "I0707 17:07:32.793631 140263261390720 learning.py:507] global step 3692: loss = 3.7787 (0.335 sec/step)\n",
            "I0707 17:07:33.132662 140263261390720 learning.py:507] global step 3693: loss = 6.9224 (0.336 sec/step)\n",
            "I0707 17:07:33.447500 140263261390720 learning.py:507] global step 3694: loss = 4.1231 (0.313 sec/step)\n",
            "I0707 17:07:33.778751 140263261390720 learning.py:507] global step 3695: loss = 3.8223 (0.330 sec/step)\n",
            "I0707 17:07:34.217766 140263261390720 learning.py:507] global step 3696: loss = 4.1637 (0.437 sec/step)\n",
            "I0707 17:07:34.539291 140263261390720 learning.py:507] global step 3697: loss = 3.4945 (0.320 sec/step)\n",
            "I0707 17:07:34.858138 140263261390720 learning.py:507] global step 3698: loss = 3.6370 (0.317 sec/step)\n",
            "I0707 17:07:35.272671 140263261390720 learning.py:507] global step 3699: loss = 4.0269 (0.413 sec/step)\n",
            "I0707 17:07:35.597623 140263261390720 learning.py:507] global step 3700: loss = 4.4298 (0.323 sec/step)\n",
            "I0707 17:07:35.949146 140263261390720 learning.py:507] global step 3701: loss = 4.2037 (0.350 sec/step)\n",
            "I0707 17:07:36.272478 140263261390720 learning.py:507] global step 3702: loss = 6.0144 (0.321 sec/step)\n",
            "I0707 17:07:36.607853 140263261390720 learning.py:507] global step 3703: loss = 3.0858 (0.334 sec/step)\n",
            "I0707 17:07:36.920863 140263261390720 learning.py:507] global step 3704: loss = 4.3533 (0.311 sec/step)\n",
            "I0707 17:07:37.231845 140263261390720 learning.py:507] global step 3705: loss = 4.3196 (0.309 sec/step)\n",
            "I0707 17:07:37.526193 140263261390720 learning.py:507] global step 3706: loss = 4.7794 (0.293 sec/step)\n",
            "I0707 17:07:37.840843 140263261390720 learning.py:507] global step 3707: loss = 3.0169 (0.313 sec/step)\n",
            "I0707 17:07:38.141507 140263261390720 learning.py:507] global step 3708: loss = 3.5130 (0.299 sec/step)\n",
            "I0707 17:07:38.437415 140263261390720 learning.py:507] global step 3709: loss = 3.6618 (0.294 sec/step)\n",
            "I0707 17:07:38.743149 140263261390720 learning.py:507] global step 3710: loss = 5.1998 (0.304 sec/step)\n",
            "I0707 17:07:39.150098 140263261390720 learning.py:507] global step 3711: loss = 4.0022 (0.405 sec/step)\n",
            "I0707 17:07:39.452035 140263261390720 learning.py:507] global step 3712: loss = 4.1358 (0.300 sec/step)\n",
            "I0707 17:07:39.783265 140263261390720 learning.py:507] global step 3713: loss = 3.7216 (0.330 sec/step)\n",
            "I0707 17:07:40.086504 140263261390720 learning.py:507] global step 3714: loss = 4.2847 (0.302 sec/step)\n",
            "I0707 17:07:40.415417 140263261390720 learning.py:507] global step 3715: loss = 4.2739 (0.327 sec/step)\n",
            "I0707 17:07:40.885437 140263261390720 learning.py:507] global step 3716: loss = 5.7167 (0.468 sec/step)\n",
            "I0707 17:07:41.317120 140263261390720 learning.py:507] global step 3717: loss = 3.9770 (0.430 sec/step)\n",
            "I0707 17:07:41.629951 140263261390720 learning.py:507] global step 3718: loss = 3.8178 (0.311 sec/step)\n",
            "I0707 17:07:42.042400 140263261390720 learning.py:507] global step 3719: loss = 5.5724 (0.411 sec/step)\n",
            "I0707 17:07:42.514515 140263261390720 learning.py:507] global step 3720: loss = 2.5638 (0.471 sec/step)\n",
            "I0707 17:07:42.816501 140263261390720 learning.py:507] global step 3721: loss = 4.3018 (0.300 sec/step)\n",
            "I0707 17:07:43.404663 140263261390720 learning.py:507] global step 3722: loss = 4.9080 (0.587 sec/step)\n",
            "I0707 17:07:43.706247 140263261390720 learning.py:507] global step 3723: loss = 4.2131 (0.300 sec/step)\n",
            "I0707 17:07:44.055892 140263261390720 learning.py:507] global step 3724: loss = 3.5244 (0.348 sec/step)\n",
            "I0707 17:07:44.539404 140263261390720 learning.py:507] global step 3725: loss = 3.4433 (0.482 sec/step)\n",
            "I0707 17:07:44.877282 140263261390720 learning.py:507] global step 3726: loss = 3.4771 (0.336 sec/step)\n",
            "I0707 17:07:45.197460 140263261390720 learning.py:507] global step 3727: loss = 4.9832 (0.319 sec/step)\n",
            "I0707 17:07:45.524293 140263261390720 learning.py:507] global step 3728: loss = 3.8530 (0.325 sec/step)\n",
            "I0707 17:07:45.871085 140263261390720 learning.py:507] global step 3729: loss = 3.5869 (0.345 sec/step)\n",
            "I0707 17:07:46.169579 140263261390720 learning.py:507] global step 3730: loss = 3.3115 (0.297 sec/step)\n",
            "I0707 17:07:46.500739 140263261390720 learning.py:507] global step 3731: loss = 4.2176 (0.329 sec/step)\n",
            "I0707 17:07:46.874476 140263261390720 learning.py:507] global step 3732: loss = 5.1255 (0.372 sec/step)\n",
            "I0707 17:07:47.190758 140263261390720 learning.py:507] global step 3733: loss = 3.9191 (0.314 sec/step)\n",
            "I0707 17:07:47.625453 140263261390720 learning.py:507] global step 3734: loss = 5.1743 (0.433 sec/step)\n",
            "I0707 17:07:47.948907 140263261390720 learning.py:507] global step 3735: loss = 5.2970 (0.322 sec/step)\n",
            "I0707 17:07:48.268170 140263261390720 learning.py:507] global step 3736: loss = 3.1590 (0.318 sec/step)\n",
            "I0707 17:07:48.627102 140263261390720 learning.py:507] global step 3737: loss = 3.3963 (0.357 sec/step)\n",
            "I0707 17:07:48.954279 140263261390720 learning.py:507] global step 3738: loss = 4.1142 (0.326 sec/step)\n",
            "I0707 17:07:49.250358 140263261390720 learning.py:507] global step 3739: loss = 4.9792 (0.294 sec/step)\n",
            "I0707 17:07:49.731500 140263261390720 learning.py:507] global step 3740: loss = 3.1417 (0.479 sec/step)\n",
            "I0707 17:07:50.050270 140263261390720 learning.py:507] global step 3741: loss = 3.4299 (0.317 sec/step)\n",
            "I0707 17:07:50.518174 140263261390720 learning.py:507] global step 3742: loss = 4.3442 (0.466 sec/step)\n",
            "I0707 17:07:50.822036 140263261390720 learning.py:507] global step 3743: loss = 3.8900 (0.302 sec/step)\n",
            "I0707 17:07:51.139898 140263261390720 learning.py:507] global step 3744: loss = 4.5880 (0.315 sec/step)\n",
            "I0707 17:07:51.568947 140263261390720 learning.py:507] global step 3745: loss = 4.0260 (0.427 sec/step)\n",
            "I0707 17:07:52.178659 140263261390720 learning.py:507] global step 3746: loss = 3.8832 (0.608 sec/step)\n",
            "I0707 17:07:52.497483 140263261390720 learning.py:507] global step 3747: loss = 4.1431 (0.317 sec/step)\n",
            "I0707 17:07:52.874507 140263261390720 learning.py:507] global step 3748: loss = 4.1819 (0.375 sec/step)\n",
            "I0707 17:07:53.307347 140263261390720 learning.py:507] global step 3749: loss = 3.3172 (0.426 sec/step)\n",
            "I0707 17:07:53.970524 140260287772416 supervisor.py:1050] Recording summary at step 3749.\n",
            "I0707 17:07:53.988763 140263261390720 learning.py:507] global step 3750: loss = 4.8082 (0.673 sec/step)\n",
            "I0707 17:07:54.324388 140263261390720 learning.py:507] global step 3751: loss = 3.3246 (0.334 sec/step)\n",
            "I0707 17:07:54.691996 140263261390720 learning.py:507] global step 3752: loss = 3.8872 (0.366 sec/step)\n",
            "I0707 17:07:55.082121 140263261390720 learning.py:507] global step 3753: loss = 4.0813 (0.389 sec/step)\n",
            "I0707 17:07:55.386349 140263261390720 learning.py:507] global step 3754: loss = 4.8441 (0.303 sec/step)\n",
            "I0707 17:07:55.801139 140263261390720 learning.py:507] global step 3755: loss = 5.8606 (0.413 sec/step)\n",
            "I0707 17:07:56.155702 140263261390720 learning.py:507] global step 3756: loss = 4.1489 (0.353 sec/step)\n",
            "I0707 17:07:56.562758 140263261390720 learning.py:507] global step 3757: loss = 4.4381 (0.405 sec/step)\n",
            "I0707 17:07:56.897882 140263261390720 learning.py:507] global step 3758: loss = 3.4446 (0.333 sec/step)\n",
            "I0707 17:07:57.190528 140263261390720 learning.py:507] global step 3759: loss = 3.3857 (0.291 sec/step)\n",
            "I0707 17:07:57.515055 140263261390720 learning.py:507] global step 3760: loss = 3.8646 (0.323 sec/step)\n",
            "I0707 17:07:57.991993 140263261390720 learning.py:507] global step 3761: loss = 4.8697 (0.475 sec/step)\n",
            "I0707 17:07:58.337905 140263261390720 learning.py:507] global step 3762: loss = 3.3144 (0.344 sec/step)\n",
            "I0707 17:07:58.698136 140263261390720 learning.py:507] global step 3763: loss = 3.0551 (0.358 sec/step)\n",
            "I0707 17:07:59.000071 140263261390720 learning.py:507] global step 3764: loss = 3.1965 (0.300 sec/step)\n",
            "I0707 17:07:59.304880 140263261390720 learning.py:507] global step 3765: loss = 3.7601 (0.303 sec/step)\n",
            "I0707 17:07:59.586375 140263261390720 learning.py:507] global step 3766: loss = 4.3000 (0.280 sec/step)\n",
            "I0707 17:08:00.078272 140263261390720 learning.py:507] global step 3767: loss = 3.6164 (0.488 sec/step)\n",
            "I0707 17:08:00.450205 140263261390720 learning.py:507] global step 3768: loss = 4.6093 (0.370 sec/step)\n",
            "I0707 17:08:00.826733 140263261390720 learning.py:507] global step 3769: loss = 3.8750 (0.375 sec/step)\n",
            "I0707 17:08:01.142084 140263261390720 learning.py:507] global step 3770: loss = 5.0614 (0.314 sec/step)\n",
            "I0707 17:08:01.461780 140263261390720 learning.py:507] global step 3771: loss = 4.3150 (0.318 sec/step)\n",
            "I0707 17:08:01.752282 140263261390720 learning.py:507] global step 3772: loss = 3.5792 (0.289 sec/step)\n",
            "I0707 17:08:02.058346 140263261390720 learning.py:507] global step 3773: loss = 2.9703 (0.304 sec/step)\n",
            "I0707 17:08:02.505792 140263261390720 learning.py:507] global step 3774: loss = 3.9884 (0.446 sec/step)\n",
            "I0707 17:08:02.898995 140263261390720 learning.py:507] global step 3775: loss = 2.8316 (0.392 sec/step)\n",
            "I0707 17:08:03.211533 140263261390720 learning.py:507] global step 3776: loss = 3.0038 (0.311 sec/step)\n",
            "I0707 17:08:03.551968 140263261390720 learning.py:507] global step 3777: loss = 4.3852 (0.339 sec/step)\n",
            "I0707 17:08:03.870286 140263261390720 learning.py:507] global step 3778: loss = 4.1271 (0.317 sec/step)\n",
            "I0707 17:08:04.183543 140263261390720 learning.py:507] global step 3779: loss = 3.8109 (0.311 sec/step)\n",
            "I0707 17:08:04.505602 140263261390720 learning.py:507] global step 3780: loss = 3.0096 (0.320 sec/step)\n",
            "I0707 17:08:04.941635 140263261390720 learning.py:507] global step 3781: loss = 4.2779 (0.434 sec/step)\n",
            "I0707 17:08:05.477859 140263261390720 learning.py:507] global step 3782: loss = 2.6766 (0.535 sec/step)\n",
            "I0707 17:08:05.802426 140263261390720 learning.py:507] global step 3783: loss = 4.5369 (0.323 sec/step)\n",
            "I0707 17:08:06.117648 140263261390720 learning.py:507] global step 3784: loss = 3.0495 (0.313 sec/step)\n",
            "I0707 17:08:06.418524 140263261390720 learning.py:507] global step 3785: loss = 2.7627 (0.299 sec/step)\n",
            "I0707 17:08:06.721161 140263261390720 learning.py:507] global step 3786: loss = 5.1239 (0.301 sec/step)\n",
            "I0707 17:08:07.119735 140263261390720 learning.py:507] global step 3787: loss = 3.4394 (0.397 sec/step)\n",
            "I0707 17:08:07.430264 140263261390720 learning.py:507] global step 3788: loss = 3.6378 (0.309 sec/step)\n",
            "I0707 17:08:07.765606 140263261390720 learning.py:507] global step 3789: loss = 5.2008 (0.334 sec/step)\n",
            "I0707 17:08:08.267029 140263261390720 learning.py:507] global step 3790: loss = 3.7380 (0.500 sec/step)\n",
            "I0707 17:08:08.566344 140263261390720 learning.py:507] global step 3791: loss = 3.3613 (0.298 sec/step)\n",
            "I0707 17:08:08.850172 140263261390720 learning.py:507] global step 3792: loss = 3.5901 (0.282 sec/step)\n",
            "I0707 17:08:09.186297 140263261390720 learning.py:507] global step 3793: loss = 4.7735 (0.335 sec/step)\n",
            "I0707 17:08:09.512634 140263261390720 learning.py:507] global step 3794: loss = 3.8255 (0.325 sec/step)\n",
            "I0707 17:08:09.817288 140263261390720 learning.py:507] global step 3795: loss = 3.3230 (0.303 sec/step)\n",
            "I0707 17:08:10.433113 140263261390720 learning.py:507] global step 3796: loss = 4.9871 (0.614 sec/step)\n",
            "I0707 17:08:10.754524 140263261390720 learning.py:507] global step 3797: loss = 4.3694 (0.320 sec/step)\n",
            "I0707 17:08:11.052450 140263261390720 learning.py:507] global step 3798: loss = 2.9856 (0.296 sec/step)\n",
            "I0707 17:08:11.402081 140263261390720 learning.py:507] global step 3799: loss = 4.6506 (0.348 sec/step)\n",
            "I0707 17:08:11.688967 140263261390720 learning.py:507] global step 3800: loss = 4.4803 (0.285 sec/step)\n",
            "I0707 17:08:12.011897 140263261390720 learning.py:507] global step 3801: loss = 3.0156 (0.321 sec/step)\n",
            "I0707 17:08:12.695711 140263261390720 learning.py:507] global step 3802: loss = 5.3629 (0.682 sec/step)\n",
            "I0707 17:08:13.001017 140263261390720 learning.py:507] global step 3803: loss = 3.7795 (0.304 sec/step)\n",
            "I0707 17:08:13.323705 140263261390720 learning.py:507] global step 3804: loss = 3.6283 (0.321 sec/step)\n",
            "I0707 17:08:13.659938 140263261390720 learning.py:507] global step 3805: loss = 3.5268 (0.335 sec/step)\n",
            "I0707 17:08:13.991947 140263261390720 learning.py:507] global step 3806: loss = 3.9942 (0.330 sec/step)\n",
            "I0707 17:08:14.322145 140263261390720 learning.py:507] global step 3807: loss = 3.6296 (0.329 sec/step)\n",
            "I0707 17:08:15.021079 140263261390720 learning.py:507] global step 3808: loss = 3.5743 (0.697 sec/step)\n",
            "I0707 17:08:15.331174 140263261390720 learning.py:507] global step 3809: loss = 2.9630 (0.308 sec/step)\n",
            "I0707 17:08:15.674232 140263261390720 learning.py:507] global step 3810: loss = 4.8629 (0.342 sec/step)\n",
            "I0707 17:08:16.028330 140263261390720 learning.py:507] global step 3811: loss = 4.7093 (0.352 sec/step)\n",
            "I0707 17:08:16.364267 140263261390720 learning.py:507] global step 3812: loss = 3.8917 (0.334 sec/step)\n",
            "I0707 17:08:16.651010 140263261390720 learning.py:507] global step 3813: loss = 4.7974 (0.285 sec/step)\n",
            "I0707 17:08:17.085248 140263261390720 learning.py:507] global step 3814: loss = 4.7115 (0.433 sec/step)\n",
            "I0707 17:08:17.392873 140263261390720 learning.py:507] global step 3815: loss = 3.3705 (0.306 sec/step)\n",
            "I0707 17:08:17.769576 140263261390720 learning.py:507] global step 3816: loss = 5.2800 (0.375 sec/step)\n",
            "I0707 17:08:18.070302 140263261390720 learning.py:507] global step 3817: loss = 2.9246 (0.299 sec/step)\n",
            "I0707 17:08:18.387257 140263261390720 learning.py:507] global step 3818: loss = 4.9320 (0.315 sec/step)\n",
            "I0707 17:08:18.696859 140263261390720 learning.py:507] global step 3819: loss = 5.0070 (0.308 sec/step)\n",
            "I0707 17:08:19.123794 140263261390720 learning.py:507] global step 3820: loss = 2.5077 (0.425 sec/step)\n",
            "I0707 17:08:19.426368 140263261390720 learning.py:507] global step 3821: loss = 3.3053 (0.301 sec/step)\n",
            "I0707 17:08:19.823450 140263261390720 learning.py:507] global step 3822: loss = 2.9249 (0.396 sec/step)\n",
            "I0707 17:08:20.149370 140263261390720 learning.py:507] global step 3823: loss = 4.6735 (0.324 sec/step)\n",
            "I0707 17:08:20.458260 140263261390720 learning.py:507] global step 3824: loss = 3.2393 (0.307 sec/step)\n",
            "I0707 17:08:20.786603 140263261390720 learning.py:507] global step 3825: loss = 2.4915 (0.327 sec/step)\n",
            "I0707 17:08:21.093328 140263261390720 learning.py:507] global step 3826: loss = 6.0650 (0.305 sec/step)\n",
            "I0707 17:08:21.401434 140263261390720 learning.py:507] global step 3827: loss = 2.7430 (0.306 sec/step)\n",
            "I0707 17:08:21.768065 140263261390720 learning.py:507] global step 3828: loss = 5.4934 (0.365 sec/step)\n",
            "I0707 17:08:22.077822 140263261390720 learning.py:507] global step 3829: loss = 3.7750 (0.308 sec/step)\n",
            "I0707 17:08:22.391334 140263261390720 learning.py:507] global step 3830: loss = 4.6541 (0.312 sec/step)\n",
            "I0707 17:08:22.709474 140263261390720 learning.py:507] global step 3831: loss = 3.5708 (0.316 sec/step)\n",
            "I0707 17:08:23.029984 140263261390720 learning.py:507] global step 3832: loss = 3.4380 (0.318 sec/step)\n",
            "I0707 17:08:23.319553 140263261390720 learning.py:507] global step 3833: loss = 4.1653 (0.288 sec/step)\n",
            "I0707 17:08:23.688604 140263261390720 learning.py:507] global step 3834: loss = 4.4344 (0.367 sec/step)\n",
            "I0707 17:08:23.977248 140263261390720 learning.py:507] global step 3835: loss = 5.1558 (0.287 sec/step)\n",
            "I0707 17:08:24.306994 140263261390720 learning.py:507] global step 3836: loss = 4.2783 (0.328 sec/step)\n",
            "I0707 17:08:24.633547 140263261390720 learning.py:507] global step 3837: loss = 2.9919 (0.325 sec/step)\n",
            "I0707 17:08:24.963847 140263261390720 learning.py:507] global step 3838: loss = 3.0342 (0.328 sec/step)\n",
            "I0707 17:08:25.320218 140263261390720 learning.py:507] global step 3839: loss = 2.9708 (0.354 sec/step)\n",
            "I0707 17:08:25.727980 140263261390720 learning.py:507] global step 3840: loss = 3.8593 (0.406 sec/step)\n",
            "I0707 17:08:26.030141 140263261390720 learning.py:507] global step 3841: loss = 4.1871 (0.300 sec/step)\n",
            "I0707 17:08:26.479809 140263261390720 learning.py:507] global step 3842: loss = 5.1954 (0.447 sec/step)\n",
            "I0707 17:08:26.772869 140263261390720 learning.py:507] global step 3843: loss = 2.6252 (0.291 sec/step)\n",
            "I0707 17:08:27.147694 140263261390720 learning.py:507] global step 3844: loss = 4.0736 (0.373 sec/step)\n",
            "I0707 17:08:27.517398 140263261390720 learning.py:507] global step 3845: loss = 4.9519 (0.366 sec/step)\n",
            "I0707 17:08:27.854843 140263261390720 learning.py:507] global step 3846: loss = 3.2256 (0.334 sec/step)\n",
            "I0707 17:08:28.154223 140263261390720 learning.py:507] global step 3847: loss = 3.8341 (0.298 sec/step)\n",
            "I0707 17:08:28.938708 140263261390720 learning.py:507] global step 3848: loss = 3.5882 (0.783 sec/step)\n",
            "I0707 17:08:29.234941 140263261390720 learning.py:507] global step 3849: loss = 4.1967 (0.294 sec/step)\n",
            "I0707 17:08:29.645789 140263261390720 learning.py:507] global step 3850: loss = 2.9089 (0.409 sec/step)\n",
            "I0707 17:08:29.946551 140263261390720 learning.py:507] global step 3851: loss = 3.6268 (0.299 sec/step)\n",
            "I0707 17:08:30.255400 140263261390720 learning.py:507] global step 3852: loss = 3.5911 (0.307 sec/step)\n",
            "I0707 17:08:30.559277 140263261390720 learning.py:507] global step 3853: loss = 3.5441 (0.302 sec/step)\n",
            "I0707 17:08:31.266558 140263261390720 learning.py:507] global step 3854: loss = 3.1593 (0.706 sec/step)\n",
            "I0707 17:08:31.570657 140263261390720 learning.py:507] global step 3855: loss = 4.1712 (0.302 sec/step)\n",
            "I0707 17:08:31.941866 140263261390720 learning.py:507] global step 3856: loss = 3.2295 (0.370 sec/step)\n",
            "I0707 17:08:32.297228 140263261390720 learning.py:507] global step 3857: loss = 3.5538 (0.354 sec/step)\n",
            "I0707 17:08:32.593605 140263261390720 learning.py:507] global step 3858: loss = 2.9345 (0.295 sec/step)\n",
            "I0707 17:08:32.905773 140263261390720 learning.py:507] global step 3859: loss = 2.7847 (0.311 sec/step)\n",
            "I0707 17:08:33.229293 140263261390720 learning.py:507] global step 3860: loss = 3.4934 (0.322 sec/step)\n",
            "I0707 17:08:33.530319 140263261390720 learning.py:507] global step 3861: loss = 3.6501 (0.299 sec/step)\n",
            "I0707 17:08:33.846496 140263261390720 learning.py:507] global step 3862: loss = 3.2030 (0.315 sec/step)\n",
            "I0707 17:08:34.252042 140263261390720 learning.py:507] global step 3863: loss = 3.8077 (0.404 sec/step)\n",
            "I0707 17:08:34.567514 140263261390720 learning.py:507] global step 3864: loss = 3.8216 (0.314 sec/step)\n",
            "I0707 17:08:34.931980 140263261390720 learning.py:507] global step 3865: loss = 3.9160 (0.363 sec/step)\n",
            "I0707 17:08:35.293599 140263261390720 learning.py:507] global step 3866: loss = 2.4783 (0.360 sec/step)\n",
            "I0707 17:08:35.665854 140263261390720 learning.py:507] global step 3867: loss = 3.9304 (0.370 sec/step)\n",
            "I0707 17:08:36.012748 140263261390720 learning.py:507] global step 3868: loss = 3.8175 (0.345 sec/step)\n",
            "I0707 17:08:36.333158 140263261390720 learning.py:507] global step 3869: loss = 3.3811 (0.319 sec/step)\n",
            "I0707 17:08:36.663489 140263261390720 learning.py:507] global step 3870: loss = 5.2950 (0.329 sec/step)\n",
            "I0707 17:08:36.997943 140263261390720 learning.py:507] global step 3871: loss = 4.1285 (0.333 sec/step)\n",
            "I0707 17:08:37.424618 140263261390720 learning.py:507] global step 3872: loss = 3.2131 (0.425 sec/step)\n",
            "I0707 17:08:37.833384 140263261390720 learning.py:507] global step 3873: loss = 3.5916 (0.405 sec/step)\n",
            "I0707 17:08:38.160502 140263261390720 learning.py:507] global step 3874: loss = 3.3054 (0.325 sec/step)\n",
            "I0707 17:08:38.466485 140263261390720 learning.py:507] global step 3875: loss = 3.1872 (0.304 sec/step)\n",
            "I0707 17:08:38.844663 140263261390720 learning.py:507] global step 3876: loss = 5.3836 (0.376 sec/step)\n",
            "I0707 17:08:39.175783 140263261390720 learning.py:507] global step 3877: loss = 4.4016 (0.330 sec/step)\n",
            "I0707 17:08:39.547570 140263261390720 learning.py:507] global step 3878: loss = 5.0555 (0.370 sec/step)\n",
            "I0707 17:08:39.874657 140263261390720 learning.py:507] global step 3879: loss = 3.1541 (0.325 sec/step)\n",
            "I0707 17:08:40.270145 140263261390720 learning.py:507] global step 3880: loss = 4.7387 (0.394 sec/step)\n",
            "I0707 17:08:40.570577 140263261390720 learning.py:507] global step 3881: loss = 3.1030 (0.299 sec/step)\n",
            "I0707 17:08:40.965683 140263261390720 learning.py:507] global step 3882: loss = 4.1999 (0.393 sec/step)\n",
            "I0707 17:08:41.267481 140263261390720 learning.py:507] global step 3883: loss = 3.2328 (0.300 sec/step)\n",
            "I0707 17:08:41.610169 140263261390720 learning.py:507] global step 3884: loss = 3.8320 (0.341 sec/step)\n",
            "I0707 17:08:41.889548 140263261390720 learning.py:507] global step 3885: loss = 4.5989 (0.278 sec/step)\n",
            "I0707 17:08:42.323538 140263261390720 learning.py:507] global step 3886: loss = 3.9234 (0.432 sec/step)\n",
            "I0707 17:08:42.633815 140263261390720 learning.py:507] global step 3887: loss = 3.1173 (0.309 sec/step)\n",
            "I0707 17:08:42.970890 140263261390720 learning.py:507] global step 3888: loss = 4.4151 (0.335 sec/step)\n",
            "I0707 17:08:43.290866 140263261390720 learning.py:507] global step 3889: loss = 4.1808 (0.318 sec/step)\n",
            "I0707 17:08:43.662411 140263261390720 learning.py:507] global step 3890: loss = 4.5048 (0.370 sec/step)\n",
            "I0707 17:08:44.020244 140263261390720 learning.py:507] global step 3891: loss = 3.2999 (0.356 sec/step)\n",
            "I0707 17:08:44.334417 140263261390720 learning.py:507] global step 3892: loss = 3.8993 (0.312 sec/step)\n",
            "I0707 17:08:44.646989 140263261390720 learning.py:507] global step 3893: loss = 4.0786 (0.311 sec/step)\n",
            "I0707 17:08:45.009415 140263261390720 learning.py:507] global step 3894: loss = 3.2861 (0.361 sec/step)\n",
            "I0707 17:08:45.323101 140263261390720 learning.py:507] global step 3895: loss = 3.9134 (0.312 sec/step)\n",
            "I0707 17:08:45.624209 140263261390720 learning.py:507] global step 3896: loss = 4.8901 (0.299 sec/step)\n",
            "I0707 17:08:46.018253 140263261390720 learning.py:507] global step 3897: loss = 3.1482 (0.392 sec/step)\n",
            "I0707 17:08:46.327272 140263261390720 learning.py:507] global step 3898: loss = 3.6490 (0.307 sec/step)\n",
            "I0707 17:08:46.643906 140263261390720 learning.py:507] global step 3899: loss = 3.1596 (0.315 sec/step)\n",
            "I0707 17:08:47.002642 140263261390720 learning.py:507] global step 3900: loss = 4.4540 (0.357 sec/step)\n",
            "I0707 17:08:47.366551 140263261390720 learning.py:507] global step 3901: loss = 3.9766 (0.362 sec/step)\n",
            "I0707 17:08:47.685582 140263261390720 learning.py:507] global step 3902: loss = 3.7736 (0.317 sec/step)\n",
            "I0707 17:08:48.045344 140263261390720 learning.py:507] global step 3903: loss = 3.5866 (0.358 sec/step)\n",
            "I0707 17:08:48.353497 140263261390720 learning.py:507] global step 3904: loss = 5.1269 (0.306 sec/step)\n",
            "I0707 17:08:48.669529 140263261390720 learning.py:507] global step 3905: loss = 3.6225 (0.314 sec/step)\n",
            "I0707 17:08:49.119271 140263261390720 learning.py:507] global step 3906: loss = 2.7865 (0.448 sec/step)\n",
            "I0707 17:08:49.520080 140263261390720 learning.py:507] global step 3907: loss = 3.3897 (0.399 sec/step)\n",
            "I0707 17:08:49.832803 140263261390720 learning.py:507] global step 3908: loss = 4.0824 (0.311 sec/step)\n",
            "I0707 17:08:50.235447 140263261390720 learning.py:507] global step 3909: loss = 2.8279 (0.401 sec/step)\n",
            "I0707 17:08:50.532978 140263261390720 learning.py:507] global step 3910: loss = 5.1829 (0.296 sec/step)\n",
            "I0707 17:08:50.892694 140263261390720 learning.py:507] global step 3911: loss = 6.0579 (0.358 sec/step)\n",
            "I0707 17:08:51.249274 140263261390720 learning.py:507] global step 3912: loss = 4.3063 (0.355 sec/step)\n",
            "I0707 17:08:51.554063 140263261390720 learning.py:507] global step 3913: loss = 3.7956 (0.302 sec/step)\n",
            "I0707 17:08:52.106912 140263261390720 learning.py:507] global step 3914: loss = 2.8775 (0.551 sec/step)\n",
            "I0707 17:08:52.436527 140263261390720 learning.py:507] global step 3915: loss = 3.3257 (0.328 sec/step)\n",
            "I0707 17:08:52.755508 140263261390720 learning.py:507] global step 3916: loss = 3.1594 (0.317 sec/step)\n",
            "I0707 17:08:53.207074 140263261390720 learning.py:507] global step 3917: loss = 3.0486 (0.450 sec/step)\n",
            "I0707 17:08:53.543808 140263261390720 learning.py:507] global step 3918: loss = 4.1812 (0.335 sec/step)\n",
            "I0707 17:08:53.852316 140263261390720 learning.py:507] global step 3919: loss = 4.6679 (0.307 sec/step)\n",
            "I0707 17:08:54.578193 140263261390720 learning.py:507] global step 3920: loss = 3.0145 (0.724 sec/step)\n",
            "I0707 17:08:54.889332 140263261390720 learning.py:507] global step 3921: loss = 6.2652 (0.310 sec/step)\n",
            "I0707 17:08:55.179346 140263261390720 learning.py:507] global step 3922: loss = 4.4866 (0.288 sec/step)\n",
            "I0707 17:08:55.485450 140263261390720 learning.py:507] global step 3923: loss = 4.1740 (0.304 sec/step)\n",
            "I0707 17:08:55.781201 140263261390720 learning.py:507] global step 3924: loss = 3.9792 (0.294 sec/step)\n",
            "I0707 17:08:56.084242 140263261390720 learning.py:507] global step 3925: loss = 5.0248 (0.301 sec/step)\n",
            "I0707 17:08:56.452940 140263261390720 learning.py:507] global step 3926: loss = 2.4809 (0.367 sec/step)\n",
            "I0707 17:08:56.793538 140263261390720 learning.py:507] global step 3927: loss = 3.0246 (0.339 sec/step)\n",
            "I0707 17:08:57.202029 140263261390720 learning.py:507] global step 3928: loss = 2.9748 (0.407 sec/step)\n",
            "I0707 17:08:57.503256 140263261390720 learning.py:507] global step 3929: loss = 3.7887 (0.299 sec/step)\n",
            "I0707 17:08:57.795462 140263261390720 learning.py:507] global step 3930: loss = 4.6871 (0.290 sec/step)\n",
            "I0707 17:08:58.151276 140263261390720 learning.py:507] global step 3931: loss = 3.6343 (0.354 sec/step)\n",
            "I0707 17:08:58.464436 140263261390720 learning.py:507] global step 3932: loss = 2.9328 (0.311 sec/step)\n",
            "I0707 17:08:58.766240 140263261390720 learning.py:507] global step 3933: loss = 3.0563 (0.300 sec/step)\n",
            "I0707 17:08:59.093355 140263261390720 learning.py:507] global step 3934: loss = 3.7126 (0.325 sec/step)\n",
            "I0707 17:08:59.440777 140263261390720 learning.py:507] global step 3935: loss = 3.1043 (0.346 sec/step)\n",
            "I0707 17:08:59.874512 140263261390720 learning.py:507] global step 3936: loss = 3.2080 (0.432 sec/step)\n",
            "I0707 17:09:00.323049 140263261390720 learning.py:507] global step 3937: loss = 3.4157 (0.447 sec/step)\n",
            "I0707 17:09:00.644747 140263261390720 learning.py:507] global step 3938: loss = 3.0224 (0.320 sec/step)\n",
            "I0707 17:09:00.947803 140263261390720 learning.py:507] global step 3939: loss = 4.2913 (0.302 sec/step)\n",
            "I0707 17:09:01.236902 140263261390720 learning.py:507] global step 3940: loss = 3.0749 (0.287 sec/step)\n",
            "I0707 17:09:01.556193 140263261390720 learning.py:507] global step 3941: loss = 3.6101 (0.317 sec/step)\n",
            "I0707 17:09:01.892056 140263261390720 learning.py:507] global step 3942: loss = 3.4089 (0.334 sec/step)\n",
            "I0707 17:09:02.263163 140263261390720 learning.py:507] global step 3943: loss = 4.4906 (0.369 sec/step)\n",
            "I0707 17:09:02.576301 140263261390720 learning.py:507] global step 3944: loss = 4.0169 (0.312 sec/step)\n",
            "I0707 17:09:02.939699 140263261390720 learning.py:507] global step 3945: loss = 2.9273 (0.362 sec/step)\n",
            "I0707 17:09:03.265620 140263261390720 learning.py:507] global step 3946: loss = 4.7723 (0.324 sec/step)\n",
            "I0707 17:09:03.581858 140263261390720 learning.py:507] global step 3947: loss = 4.0923 (0.315 sec/step)\n",
            "I0707 17:09:03.887120 140263261390720 learning.py:507] global step 3948: loss = 3.9863 (0.304 sec/step)\n",
            "I0707 17:09:04.184675 140263261390720 learning.py:507] global step 3949: loss = 4.3207 (0.296 sec/step)\n",
            "I0707 17:09:04.548408 140263261390720 learning.py:507] global step 3950: loss = 3.9963 (0.362 sec/step)\n",
            "I0707 17:09:04.852400 140263261390720 learning.py:507] global step 3951: loss = 4.1259 (0.302 sec/step)\n",
            "I0707 17:09:05.166495 140263261390720 learning.py:507] global step 3952: loss = 3.7289 (0.312 sec/step)\n",
            "I0707 17:09:05.482984 140263261390720 learning.py:507] global step 3953: loss = 4.2603 (0.315 sec/step)\n",
            "I0707 17:09:05.858779 140263261390720 learning.py:507] global step 3954: loss = 2.9055 (0.374 sec/step)\n",
            "I0707 17:09:06.203173 140263261390720 learning.py:507] global step 3955: loss = 3.0457 (0.343 sec/step)\n",
            "I0707 17:09:06.568449 140263261390720 learning.py:507] global step 3956: loss = 4.5767 (0.364 sec/step)\n",
            "I0707 17:09:06.872394 140263261390720 learning.py:507] global step 3957: loss = 3.3687 (0.302 sec/step)\n",
            "I0707 17:09:07.190767 140263261390720 learning.py:507] global step 3958: loss = 3.1692 (0.317 sec/step)\n",
            "I0707 17:09:07.499527 140263261390720 learning.py:507] global step 3959: loss = 2.6583 (0.307 sec/step)\n",
            "I0707 17:09:07.876931 140263261390720 learning.py:507] global step 3960: loss = 4.0167 (0.376 sec/step)\n",
            "I0707 17:09:08.191525 140263261390720 learning.py:507] global step 3961: loss = 4.3267 (0.313 sec/step)\n",
            "I0707 17:09:08.604337 140263261390720 learning.py:507] global step 3962: loss = 3.1374 (0.411 sec/step)\n",
            "I0707 17:09:08.962637 140263261390720 learning.py:507] global step 3963: loss = 4.1207 (0.357 sec/step)\n",
            "I0707 17:09:09.278085 140263261390720 learning.py:507] global step 3964: loss = 4.5837 (0.314 sec/step)\n",
            "I0707 17:09:09.591525 140263261390720 learning.py:507] global step 3965: loss = 4.0734 (0.311 sec/step)\n",
            "I0707 17:09:09.903421 140263261390720 learning.py:507] global step 3966: loss = 4.9354 (0.310 sec/step)\n",
            "I0707 17:09:10.227737 140263261390720 learning.py:507] global step 3967: loss = 3.7788 (0.323 sec/step)\n",
            "I0707 17:09:10.559691 140263261390720 learning.py:507] global step 3968: loss = 4.2241 (0.330 sec/step)\n",
            "I0707 17:09:10.863653 140263261390720 learning.py:507] global step 3969: loss = 4.7037 (0.302 sec/step)\n",
            "I0707 17:09:11.162924 140263261390720 learning.py:507] global step 3970: loss = 3.3539 (0.297 sec/step)\n",
            "I0707 17:09:11.478680 140263261390720 learning.py:507] global step 3971: loss = 4.5471 (0.314 sec/step)\n",
            "I0707 17:09:11.784818 140263261390720 learning.py:507] global step 3972: loss = 3.1495 (0.304 sec/step)\n",
            "I0707 17:09:12.122058 140263261390720 learning.py:507] global step 3973: loss = 3.6836 (0.336 sec/step)\n",
            "I0707 17:09:12.450531 140263261390720 learning.py:507] global step 3974: loss = 2.5983 (0.327 sec/step)\n",
            "I0707 17:09:12.751891 140263261390720 learning.py:507] global step 3975: loss = 6.0609 (0.300 sec/step)\n",
            "I0707 17:09:13.104112 140263261390720 learning.py:507] global step 3976: loss = 4.6541 (0.351 sec/step)\n",
            "I0707 17:09:13.400675 140263261390720 learning.py:507] global step 3977: loss = 4.1929 (0.295 sec/step)\n",
            "I0707 17:09:13.705033 140263261390720 learning.py:507] global step 3978: loss = 4.3065 (0.303 sec/step)\n",
            "I0707 17:09:14.012991 140263261390720 learning.py:507] global step 3979: loss = 3.4435 (0.306 sec/step)\n",
            "I0707 17:09:14.356580 140263261390720 learning.py:507] global step 3980: loss = 5.4309 (0.342 sec/step)\n",
            "I0707 17:09:14.702399 140263261390720 learning.py:507] global step 3981: loss = 2.7495 (0.344 sec/step)\n",
            "I0707 17:09:14.996732 140263261390720 learning.py:507] global step 3982: loss = 3.2314 (0.292 sec/step)\n",
            "I0707 17:09:15.326001 140263261390720 learning.py:507] global step 3983: loss = 3.8217 (0.327 sec/step)\n",
            "I0707 17:09:15.699616 140263261390720 learning.py:507] global step 3984: loss = 3.2511 (0.372 sec/step)\n",
            "I0707 17:09:15.993255 140263261390720 learning.py:507] global step 3985: loss = 3.5341 (0.292 sec/step)\n",
            "I0707 17:09:16.316780 140263261390720 learning.py:507] global step 3986: loss = 3.6971 (0.322 sec/step)\n",
            "I0707 17:09:16.662236 140263261390720 learning.py:507] global step 3987: loss = 3.3605 (0.344 sec/step)\n",
            "I0707 17:09:16.957295 140263261390720 learning.py:507] global step 3988: loss = 3.8501 (0.294 sec/step)\n",
            "I0707 17:09:17.308722 140263261390720 learning.py:507] global step 3989: loss = 3.9850 (0.350 sec/step)\n",
            "I0707 17:09:17.627902 140263261390720 learning.py:507] global step 3990: loss = 3.0386 (0.317 sec/step)\n",
            "I0707 17:09:17.930422 140263261390720 learning.py:507] global step 3991: loss = 3.7973 (0.301 sec/step)\n",
            "I0707 17:09:18.260813 140263261390720 learning.py:507] global step 3992: loss = 3.4338 (0.329 sec/step)\n",
            "I0707 17:09:18.643179 140263261390720 learning.py:507] global step 3993: loss = 5.4536 (0.381 sec/step)\n",
            "I0707 17:09:18.965937 140263261390720 learning.py:507] global step 3994: loss = 4.5215 (0.321 sec/step)\n",
            "I0707 17:09:19.348665 140263261390720 learning.py:507] global step 3995: loss = 3.1460 (0.381 sec/step)\n",
            "I0707 17:09:19.673475 140263261390720 learning.py:507] global step 3996: loss = 4.4318 (0.323 sec/step)\n",
            "I0707 17:09:20.062987 140263261390720 learning.py:507] global step 3997: loss = 3.0519 (0.388 sec/step)\n",
            "I0707 17:09:20.372313 140263261390720 learning.py:507] global step 3998: loss = 6.9615 (0.308 sec/step)\n",
            "I0707 17:09:20.685402 140263261390720 learning.py:507] global step 3999: loss = 5.4531 (0.311 sec/step)\n",
            "I0707 17:09:20.981901 140263261390720 learning.py:507] global step 4000: loss = 5.4848 (0.295 sec/step)\n",
            "I0707 17:09:21.367815 140263261390720 learning.py:507] global step 4001: loss = 3.3545 (0.384 sec/step)\n",
            "I0707 17:09:21.663941 140263261390720 learning.py:507] global step 4002: loss = 4.6549 (0.295 sec/step)\n",
            "I0707 17:09:21.974234 140263261390720 learning.py:507] global step 4003: loss = 2.9737 (0.309 sec/step)\n",
            "I0707 17:09:22.353473 140263261390720 learning.py:507] global step 4004: loss = 4.3437 (0.378 sec/step)\n",
            "I0707 17:09:22.655949 140263261390720 learning.py:507] global step 4005: loss = 3.6531 (0.301 sec/step)\n",
            "I0707 17:09:22.954562 140263261390720 learning.py:507] global step 4006: loss = 4.6700 (0.297 sec/step)\n",
            "I0707 17:09:23.314168 140263261390720 learning.py:507] global step 4007: loss = 3.9652 (0.358 sec/step)\n",
            "I0707 17:09:23.651150 140263261390720 learning.py:507] global step 4008: loss = 3.9200 (0.335 sec/step)\n",
            "I0707 17:09:23.971084 140263261390720 learning.py:507] global step 4009: loss = 3.2546 (0.318 sec/step)\n",
            "I0707 17:09:24.381919 140263261390720 learning.py:507] global step 4010: loss = 3.8863 (0.409 sec/step)\n",
            "I0707 17:09:24.726607 140263261390720 learning.py:507] global step 4011: loss = 3.7511 (0.343 sec/step)\n",
            "I0707 17:09:25.074520 140263261390720 learning.py:507] global step 4012: loss = 3.9014 (0.346 sec/step)\n",
            "I0707 17:09:25.376936 140263261390720 learning.py:507] global step 4013: loss = 3.6292 (0.301 sec/step)\n",
            "I0707 17:09:25.686583 140263261390720 learning.py:507] global step 4014: loss = 4.2589 (0.308 sec/step)\n",
            "I0707 17:09:26.066848 140263261390720 learning.py:507] global step 4015: loss = 3.0195 (0.378 sec/step)\n",
            "I0707 17:09:26.409082 140263261390720 learning.py:507] global step 4016: loss = 4.1824 (0.340 sec/step)\n",
            "I0707 17:09:26.743289 140263261390720 learning.py:507] global step 4017: loss = 4.2765 (0.333 sec/step)\n",
            "I0707 17:09:27.057503 140263261390720 learning.py:507] global step 4018: loss = 3.3637 (0.313 sec/step)\n",
            "I0707 17:09:27.395791 140263261390720 learning.py:507] global step 4019: loss = 3.9905 (0.337 sec/step)\n",
            "I0707 17:09:27.738645 140263261390720 learning.py:507] global step 4020: loss = 3.6957 (0.341 sec/step)\n",
            "I0707 17:09:28.029768 140263261390720 learning.py:507] global step 4021: loss = 3.5485 (0.289 sec/step)\n",
            "I0707 17:09:28.342676 140263261390720 learning.py:507] global step 4022: loss = 3.8222 (0.311 sec/step)\n",
            "I0707 17:09:28.671093 140263261390720 learning.py:507] global step 4023: loss = 5.7542 (0.327 sec/step)\n",
            "I0707 17:09:28.979737 140263261390720 learning.py:507] global step 4024: loss = 4.4675 (0.307 sec/step)\n",
            "I0707 17:09:29.306004 140263261390720 learning.py:507] global step 4025: loss = 3.5205 (0.325 sec/step)\n",
            "I0707 17:09:29.640705 140263261390720 learning.py:507] global step 4026: loss = 3.4778 (0.333 sec/step)\n",
            "I0707 17:09:29.936665 140263261390720 learning.py:507] global step 4027: loss = 5.1809 (0.294 sec/step)\n",
            "I0707 17:09:30.274003 140263261390720 learning.py:507] global step 4028: loss = 3.9123 (0.336 sec/step)\n",
            "I0707 17:09:30.593325 140263261390720 learning.py:507] global step 4029: loss = 4.2628 (0.318 sec/step)\n",
            "I0707 17:09:30.912143 140263261390720 learning.py:507] global step 4030: loss = 2.8036 (0.317 sec/step)\n",
            "I0707 17:09:31.227739 140263261390720 learning.py:507] global step 4031: loss = 4.4892 (0.314 sec/step)\n",
            "I0707 17:09:31.622057 140263261390720 learning.py:507] global step 4032: loss = 5.1710 (0.393 sec/step)\n",
            "I0707 17:09:31.926534 140263261390720 learning.py:507] global step 4033: loss = 4.1547 (0.302 sec/step)\n",
            "I0707 17:09:32.227125 140263261390720 learning.py:507] global step 4034: loss = 3.2727 (0.299 sec/step)\n",
            "I0707 17:09:32.567525 140263261390720 learning.py:507] global step 4035: loss = 3.4952 (0.339 sec/step)\n",
            "I0707 17:09:32.945036 140263261390720 learning.py:507] global step 4036: loss = 3.4535 (0.376 sec/step)\n",
            "I0707 17:09:33.245156 140263261390720 learning.py:507] global step 4037: loss = 4.5433 (0.298 sec/step)\n",
            "I0707 17:09:33.669816 140263261390720 learning.py:507] global step 4038: loss = 4.5302 (0.423 sec/step)\n",
            "I0707 17:09:34.018613 140263261390720 learning.py:507] global step 4039: loss = 4.2562 (0.347 sec/step)\n",
            "I0707 17:09:34.349706 140263261390720 learning.py:507] global step 4040: loss = 3.0067 (0.329 sec/step)\n",
            "I0707 17:09:34.723230 140263261390720 learning.py:507] global step 4041: loss = 3.3402 (0.372 sec/step)\n",
            "I0707 17:09:35.040809 140263261390720 learning.py:507] global step 4042: loss = 4.0615 (0.316 sec/step)\n",
            "I0707 17:09:35.344138 140263261390720 learning.py:507] global step 4043: loss = 4.2334 (0.302 sec/step)\n",
            "I0707 17:09:35.647605 140263261390720 learning.py:507] global step 4044: loss = 4.2663 (0.302 sec/step)\n",
            "I0707 17:09:35.955567 140263261390720 learning.py:507] global step 4045: loss = 3.6864 (0.306 sec/step)\n",
            "I0707 17:09:36.284773 140263261390720 learning.py:507] global step 4046: loss = 4.4623 (0.328 sec/step)\n",
            "I0707 17:09:36.613320 140263261390720 learning.py:507] global step 4047: loss = 4.1652 (0.327 sec/step)\n",
            "I0707 17:09:36.945517 140263261390720 learning.py:507] global step 4048: loss = 3.9751 (0.330 sec/step)\n",
            "I0707 17:09:37.310175 140263261390720 learning.py:507] global step 4049: loss = 4.6011 (0.362 sec/step)\n",
            "I0707 17:09:37.605448 140263261390720 learning.py:507] global step 4050: loss = 4.2782 (0.294 sec/step)\n",
            "I0707 17:09:37.943722 140263261390720 learning.py:507] global step 4051: loss = 3.7045 (0.337 sec/step)\n",
            "I0707 17:09:38.259695 140263261390720 learning.py:507] global step 4052: loss = 4.4913 (0.314 sec/step)\n",
            "I0707 17:09:38.578288 140263261390720 learning.py:507] global step 4053: loss = 3.5737 (0.317 sec/step)\n",
            "I0707 17:09:38.904362 140263261390720 learning.py:507] global step 4054: loss = 3.4217 (0.324 sec/step)\n",
            "I0707 17:09:39.342132 140263261390720 learning.py:507] global step 4055: loss = 3.3625 (0.436 sec/step)\n",
            "I0707 17:09:39.647905 140263261390720 learning.py:507] global step 4056: loss = 4.1296 (0.304 sec/step)\n",
            "I0707 17:09:39.963319 140263261390720 learning.py:507] global step 4057: loss = 3.7845 (0.314 sec/step)\n",
            "I0707 17:09:40.267616 140263261390720 learning.py:507] global step 4058: loss = 3.0523 (0.303 sec/step)\n",
            "I0707 17:09:40.608645 140263261390720 learning.py:507] global step 4059: loss = 6.1119 (0.339 sec/step)\n",
            "I0707 17:09:40.939415 140263261390720 learning.py:507] global step 4060: loss = 4.0565 (0.329 sec/step)\n",
            "I0707 17:09:41.250878 140263261390720 learning.py:507] global step 4061: loss = 4.7035 (0.310 sec/step)\n",
            "I0707 17:09:41.554089 140263261390720 learning.py:507] global step 4062: loss = 3.4699 (0.302 sec/step)\n",
            "I0707 17:09:41.869855 140263261390720 learning.py:507] global step 4063: loss = 4.2518 (0.314 sec/step)\n",
            "I0707 17:09:42.269476 140263261390720 learning.py:507] global step 4064: loss = 4.4898 (0.398 sec/step)\n",
            "I0707 17:09:42.627999 140263261390720 learning.py:507] global step 4065: loss = 3.2603 (0.357 sec/step)\n",
            "I0707 17:09:42.960908 140263261390720 learning.py:507] global step 4066: loss = 4.1996 (0.331 sec/step)\n",
            "I0707 17:09:43.280576 140263261390720 learning.py:507] global step 4067: loss = 3.3133 (0.318 sec/step)\n",
            "I0707 17:09:43.572035 140263261390720 learning.py:507] global step 4068: loss = 3.2740 (0.290 sec/step)\n",
            "I0707 17:09:43.875206 140263261390720 learning.py:507] global step 4069: loss = 3.0652 (0.301 sec/step)\n",
            "I0707 17:09:44.177866 140263261390720 learning.py:507] global step 4070: loss = 3.5473 (0.299 sec/step)\n",
            "I0707 17:09:44.473427 140263261390720 learning.py:507] global step 4071: loss = 4.4461 (0.294 sec/step)\n",
            "I0707 17:09:44.790641 140263261390720 learning.py:507] global step 4072: loss = 4.4581 (0.315 sec/step)\n",
            "I0707 17:09:45.111749 140263261390720 learning.py:507] global step 4073: loss = 4.7906 (0.319 sec/step)\n",
            "I0707 17:09:45.424917 140263261390720 learning.py:507] global step 4074: loss = 2.9476 (0.312 sec/step)\n",
            "I0707 17:09:45.761109 140263261390720 learning.py:507] global step 4075: loss = 3.2632 (0.334 sec/step)\n",
            "I0707 17:09:46.112771 140263261390720 learning.py:507] global step 4076: loss = 3.6428 (0.350 sec/step)\n",
            "I0707 17:09:46.414135 140263261390720 learning.py:507] global step 4077: loss = 4.1722 (0.300 sec/step)\n",
            "I0707 17:09:46.710303 140263261390720 learning.py:507] global step 4078: loss = 4.6058 (0.294 sec/step)\n",
            "I0707 17:09:47.040208 140263261390720 learning.py:507] global step 4079: loss = 3.2174 (0.328 sec/step)\n",
            "I0707 17:09:47.344168 140263261390720 learning.py:507] global step 4080: loss = 2.9760 (0.302 sec/step)\n",
            "I0707 17:09:47.645361 140263261390720 learning.py:507] global step 4081: loss = 3.7614 (0.300 sec/step)\n",
            "I0707 17:09:47.953929 140263261390720 learning.py:507] global step 4082: loss = 2.4114 (0.307 sec/step)\n",
            "I0707 17:09:48.337944 140263261390720 learning.py:507] global step 4083: loss = 3.4979 (0.382 sec/step)\n",
            "I0707 17:09:48.718395 140263261390720 learning.py:507] global step 4084: loss = 5.2376 (0.379 sec/step)\n",
            "I0707 17:09:49.045050 140263261390720 learning.py:507] global step 4085: loss = 3.7887 (0.325 sec/step)\n",
            "I0707 17:09:49.475025 140263261390720 learning.py:507] global step 4086: loss = 3.7999 (0.428 sec/step)\n",
            "I0707 17:09:49.776818 140263261390720 learning.py:507] global step 4087: loss = 4.5845 (0.300 sec/step)\n",
            "I0707 17:09:50.082201 140263261390720 learning.py:507] global step 4088: loss = 4.2292 (0.304 sec/step)\n",
            "I0707 17:09:50.411956 140263261390720 learning.py:507] global step 4089: loss = 3.5149 (0.328 sec/step)\n",
            "I0707 17:09:50.814888 140263261390720 learning.py:507] global step 4090: loss = 3.9769 (0.401 sec/step)\n",
            "I0707 17:09:51.108707 140263261390720 learning.py:507] global step 4091: loss = 4.1453 (0.292 sec/step)\n",
            "I0707 17:09:51.413007 140263261390720 learning.py:507] global step 4092: loss = 3.2415 (0.303 sec/step)\n",
            "I0707 17:09:51.723417 140263261390720 learning.py:507] global step 4093: loss = 4.2938 (0.309 sec/step)\n",
            "I0707 17:09:52.058108 140263261390720 learning.py:507] global step 4094: loss = 3.2816 (0.333 sec/step)\n",
            "I0707 17:09:52.435353 140263261390720 learning.py:507] global step 4095: loss = 3.8787 (0.376 sec/step)\n",
            "I0707 17:09:52.808164 140263261390720 learning.py:507] global step 4096: loss = 3.1303 (0.371 sec/step)\n",
            "I0707 17:09:53.313498 140263261390720 learning.py:507] global step 4097: loss = 3.6332 (0.359 sec/step)\n",
            "I0707 17:09:53.742100 140260287772416 supervisor.py:1050] Recording summary at step 4097.\n",
            "I0707 17:09:53.838012 140263261390720 learning.py:507] global step 4098: loss = 3.4887 (0.523 sec/step)\n",
            "I0707 17:09:54.169308 140263261390720 learning.py:507] global step 4099: loss = 3.8407 (0.330 sec/step)\n",
            "I0707 17:09:54.498756 140263261390720 learning.py:507] global step 4100: loss = 3.9360 (0.328 sec/step)\n",
            "I0707 17:09:54.859880 140263261390720 learning.py:507] global step 4101: loss = 2.6516 (0.360 sec/step)\n",
            "I0707 17:09:55.219022 140263261390720 learning.py:507] global step 4102: loss = 2.3823 (0.358 sec/step)\n",
            "I0707 17:09:55.537505 140263261390720 learning.py:507] global step 4103: loss = 3.7641 (0.317 sec/step)\n",
            "I0707 17:09:55.843615 140263261390720 learning.py:507] global step 4104: loss = 2.8646 (0.304 sec/step)\n",
            "I0707 17:09:56.156929 140263261390720 learning.py:507] global step 4105: loss = 4.6907 (0.311 sec/step)\n",
            "I0707 17:09:56.494073 140263261390720 learning.py:507] global step 4106: loss = 3.8742 (0.336 sec/step)\n",
            "I0707 17:09:56.842107 140263261390720 learning.py:507] global step 4107: loss = 3.6948 (0.346 sec/step)\n",
            "I0707 17:09:57.175837 140263261390720 learning.py:507] global step 4108: loss = 3.4116 (0.332 sec/step)\n",
            "I0707 17:09:57.589268 140263261390720 learning.py:507] global step 4109: loss = 4.5876 (0.412 sec/step)\n",
            "I0707 17:09:57.896409 140263261390720 learning.py:507] global step 4110: loss = 3.9371 (0.306 sec/step)\n",
            "I0707 17:09:58.223752 140263261390720 learning.py:507] global step 4111: loss = 3.9601 (0.326 sec/step)\n",
            "I0707 17:09:58.538714 140263261390720 learning.py:507] global step 4112: loss = 3.5977 (0.313 sec/step)\n",
            "I0707 17:09:58.850149 140263261390720 learning.py:507] global step 4113: loss = 2.7258 (0.310 sec/step)\n",
            "I0707 17:09:59.192174 140263261390720 learning.py:507] global step 4114: loss = 3.6730 (0.340 sec/step)\n",
            "I0707 17:09:59.548105 140263261390720 learning.py:507] global step 4115: loss = 4.2404 (0.354 sec/step)\n",
            "I0707 17:09:59.858088 140263261390720 learning.py:507] global step 4116: loss = 3.4328 (0.308 sec/step)\n",
            "I0707 17:10:00.257272 140263261390720 learning.py:507] global step 4117: loss = 3.3857 (0.397 sec/step)\n",
            "I0707 17:10:00.571606 140263261390720 learning.py:507] global step 4118: loss = 4.5808 (0.313 sec/step)\n",
            "I0707 17:10:00.861194 140263261390720 learning.py:507] global step 4119: loss = 5.4779 (0.288 sec/step)\n",
            "I0707 17:10:01.219443 140263261390720 learning.py:507] global step 4120: loss = 3.0270 (0.356 sec/step)\n",
            "I0707 17:10:01.649575 140263261390720 learning.py:507] global step 4121: loss = 3.5589 (0.428 sec/step)\n",
            "I0707 17:10:01.956795 140263261390720 learning.py:507] global step 4122: loss = 4.1472 (0.305 sec/step)\n",
            "I0707 17:10:02.269862 140263261390720 learning.py:507] global step 4123: loss = 3.1039 (0.311 sec/step)\n",
            "I0707 17:10:02.589202 140263261390720 learning.py:507] global step 4124: loss = 4.0003 (0.317 sec/step)\n",
            "I0707 17:10:02.886906 140263261390720 learning.py:507] global step 4125: loss = 6.2783 (0.296 sec/step)\n",
            "I0707 17:10:03.269887 140263261390720 learning.py:507] global step 4126: loss = 4.1407 (0.381 sec/step)\n",
            "I0707 17:10:03.629931 140263261390720 learning.py:507] global step 4127: loss = 2.5725 (0.358 sec/step)\n",
            "I0707 17:10:03.941525 140263261390720 learning.py:507] global step 4128: loss = 2.7078 (0.310 sec/step)\n",
            "I0707 17:10:04.247181 140263261390720 learning.py:507] global step 4129: loss = 3.1040 (0.304 sec/step)\n",
            "I0707 17:10:04.683747 140263261390720 learning.py:507] global step 4130: loss = 2.8452 (0.435 sec/step)\n",
            "I0707 17:10:05.024710 140263261390720 learning.py:507] global step 4131: loss = 4.7293 (0.339 sec/step)\n",
            "I0707 17:10:05.458284 140263261390720 learning.py:507] global step 4132: loss = 2.9505 (0.432 sec/step)\n",
            "I0707 17:10:05.789457 140263261390720 learning.py:507] global step 4133: loss = 3.1599 (0.329 sec/step)\n",
            "I0707 17:10:06.101669 140263261390720 learning.py:507] global step 4134: loss = 3.2359 (0.310 sec/step)\n",
            "I0707 17:10:06.421457 140263261390720 learning.py:507] global step 4135: loss = 3.3610 (0.318 sec/step)\n",
            "I0707 17:10:06.750492 140263261390720 learning.py:507] global step 4136: loss = 4.5822 (0.327 sec/step)\n",
            "I0707 17:10:07.173557 140263261390720 learning.py:507] global step 4137: loss = 3.4164 (0.421 sec/step)\n",
            "I0707 17:10:07.506577 140263261390720 learning.py:507] global step 4138: loss = 2.8273 (0.331 sec/step)\n",
            "I0707 17:10:07.953407 140263261390720 learning.py:507] global step 4139: loss = 4.7685 (0.445 sec/step)\n",
            "I0707 17:10:08.248363 140263261390720 learning.py:507] global step 4140: loss = 2.8247 (0.293 sec/step)\n",
            "I0707 17:10:08.573791 140263261390720 learning.py:507] global step 4141: loss = 3.6494 (0.324 sec/step)\n",
            "I0707 17:10:08.897454 140263261390720 learning.py:507] global step 4142: loss = 2.9252 (0.322 sec/step)\n",
            "I0707 17:10:09.374452 140263261390720 learning.py:507] global step 4143: loss = 2.5110 (0.476 sec/step)\n",
            "I0707 17:10:09.682973 140263261390720 learning.py:507] global step 4144: loss = 4.4949 (0.307 sec/step)\n",
            "I0707 17:10:10.005484 140263261390720 learning.py:507] global step 4145: loss = 2.8707 (0.321 sec/step)\n",
            "I0707 17:10:10.316089 140263261390720 learning.py:507] global step 4146: loss = 4.1894 (0.309 sec/step)\n",
            "I0707 17:10:10.652610 140263261390720 learning.py:507] global step 4147: loss = 3.7114 (0.335 sec/step)\n",
            "I0707 17:10:10.975328 140263261390720 learning.py:507] global step 4148: loss = 4.1383 (0.321 sec/step)\n",
            "I0707 17:10:11.287231 140263261390720 learning.py:507] global step 4149: loss = 5.2822 (0.310 sec/step)\n",
            "I0707 17:10:11.592244 140263261390720 learning.py:507] global step 4150: loss = 3.1868 (0.303 sec/step)\n",
            "I0707 17:10:11.912559 140263261390720 learning.py:507] global step 4151: loss = 3.4327 (0.319 sec/step)\n",
            "I0707 17:10:12.219361 140263261390720 learning.py:507] global step 4152: loss = 4.1459 (0.305 sec/step)\n",
            "I0707 17:10:12.503492 140263261390720 learning.py:507] global step 4153: loss = 3.8190 (0.283 sec/step)\n",
            "I0707 17:10:12.817366 140263261390720 learning.py:507] global step 4154: loss = 4.6131 (0.311 sec/step)\n",
            "I0707 17:10:13.152715 140263261390720 learning.py:507] global step 4155: loss = 3.7709 (0.333 sec/step)\n",
            "I0707 17:10:13.501050 140263261390720 learning.py:507] global step 4156: loss = 5.1182 (0.347 sec/step)\n",
            "I0707 17:10:13.815567 140263261390720 learning.py:507] global step 4157: loss = 4.2902 (0.313 sec/step)\n",
            "I0707 17:10:14.222752 140263261390720 learning.py:507] global step 4158: loss = 4.4195 (0.405 sec/step)\n",
            "I0707 17:10:14.767328 140263261390720 learning.py:507] global step 4159: loss = 3.1967 (0.543 sec/step)\n",
            "I0707 17:10:15.093618 140263261390720 learning.py:507] global step 4160: loss = 3.1909 (0.325 sec/step)\n",
            "I0707 17:10:15.384379 140263261390720 learning.py:507] global step 4161: loss = 3.3140 (0.289 sec/step)\n",
            "I0707 17:10:15.684134 140263261390720 learning.py:507] global step 4162: loss = 2.7703 (0.298 sec/step)\n",
            "I0707 17:10:16.080277 140263261390720 learning.py:507] global step 4163: loss = 3.5173 (0.394 sec/step)\n",
            "I0707 17:10:16.414163 140263261390720 learning.py:507] global step 4164: loss = 4.5097 (0.332 sec/step)\n",
            "I0707 17:10:17.116554 140263261390720 learning.py:507] global step 4165: loss = 3.4813 (0.701 sec/step)\n",
            "I0707 17:10:17.532163 140263261390720 learning.py:507] global step 4166: loss = 3.8917 (0.414 sec/step)\n",
            "I0707 17:10:17.821971 140263261390720 learning.py:507] global step 4167: loss = 3.9365 (0.288 sec/step)\n",
            "I0707 17:10:18.158689 140263261390720 learning.py:507] global step 4168: loss = 3.7290 (0.335 sec/step)\n",
            "I0707 17:10:18.527789 140263261390720 learning.py:507] global step 4169: loss = 3.1538 (0.368 sec/step)\n",
            "I0707 17:10:18.834181 140263261390720 learning.py:507] global step 4170: loss = 3.3600 (0.305 sec/step)\n",
            "I0707 17:10:19.145961 140263261390720 learning.py:507] global step 4171: loss = 2.9199 (0.310 sec/step)\n",
            "I0707 17:10:19.438248 140263261390720 learning.py:507] global step 4172: loss = 4.0305 (0.291 sec/step)\n",
            "I0707 17:10:19.736996 140263261390720 learning.py:507] global step 4173: loss = 5.9463 (0.297 sec/step)\n",
            "I0707 17:10:20.061320 140263261390720 learning.py:507] global step 4174: loss = 4.1117 (0.323 sec/step)\n",
            "I0707 17:10:20.369779 140263261390720 learning.py:507] global step 4175: loss = 3.4119 (0.307 sec/step)\n",
            "I0707 17:10:20.688925 140263261390720 learning.py:507] global step 4176: loss = 3.3980 (0.317 sec/step)\n",
            "I0707 17:10:21.105175 140263261390720 learning.py:507] global step 4177: loss = 3.7148 (0.415 sec/step)\n",
            "I0707 17:10:21.420964 140263261390720 learning.py:507] global step 4178: loss = 5.8558 (0.314 sec/step)\n",
            "I0707 17:10:21.741735 140263261390720 learning.py:507] global step 4179: loss = 2.8566 (0.319 sec/step)\n",
            "I0707 17:10:22.045002 140263261390720 learning.py:507] global step 4180: loss = 4.1751 (0.301 sec/step)\n",
            "I0707 17:10:22.454568 140263261390720 learning.py:507] global step 4181: loss = 4.9863 (0.408 sec/step)\n",
            "I0707 17:10:22.756875 140263261390720 learning.py:507] global step 4182: loss = 4.0851 (0.300 sec/step)\n",
            "I0707 17:10:23.263874 140263261390720 learning.py:507] global step 4183: loss = 5.5880 (0.505 sec/step)\n",
            "I0707 17:10:23.627943 140263261390720 learning.py:507] global step 4184: loss = 4.3531 (0.362 sec/step)\n",
            "I0707 17:10:24.053926 140263261390720 learning.py:507] global step 4185: loss = 5.7338 (0.424 sec/step)\n",
            "I0707 17:10:24.513540 140263261390720 learning.py:507] global step 4186: loss = 4.8720 (0.458 sec/step)\n",
            "I0707 17:10:24.861985 140263261390720 learning.py:507] global step 4187: loss = 4.7565 (0.347 sec/step)\n",
            "I0707 17:10:25.211038 140263261390720 learning.py:507] global step 4188: loss = 4.3752 (0.345 sec/step)\n",
            "I0707 17:10:25.664598 140263261390720 learning.py:507] global step 4189: loss = 2.9378 (0.451 sec/step)\n",
            "I0707 17:10:25.990079 140263261390720 learning.py:507] global step 4190: loss = 3.9993 (0.324 sec/step)\n",
            "I0707 17:10:26.361782 140263261390720 learning.py:507] global step 4191: loss = 3.9353 (0.370 sec/step)\n",
            "I0707 17:10:26.907896 140263261390720 learning.py:507] global step 4192: loss = 5.9632 (0.544 sec/step)\n",
            "I0707 17:10:27.226996 140263261390720 learning.py:507] global step 4193: loss = 4.7796 (0.317 sec/step)\n",
            "I0707 17:10:27.539982 140263261390720 learning.py:507] global step 4194: loss = 3.8980 (0.311 sec/step)\n",
            "I0707 17:10:27.851027 140263261390720 learning.py:507] global step 4195: loss = 2.7378 (0.309 sec/step)\n",
            "I0707 17:10:28.171777 140263261390720 learning.py:507] global step 4196: loss = 3.4001 (0.319 sec/step)\n",
            "I0707 17:10:28.481981 140263261390720 learning.py:507] global step 4197: loss = 3.0423 (0.308 sec/step)\n",
            "I0707 17:10:28.790777 140263261390720 learning.py:507] global step 4198: loss = 3.5873 (0.307 sec/step)\n",
            "I0707 17:10:29.199917 140263261390720 learning.py:507] global step 4199: loss = 5.7810 (0.407 sec/step)\n",
            "I0707 17:10:29.502932 140263261390720 learning.py:507] global step 4200: loss = 3.1056 (0.301 sec/step)\n",
            "I0707 17:10:29.809268 140263261390720 learning.py:507] global step 4201: loss = 5.4207 (0.305 sec/step)\n",
            "I0707 17:10:30.107372 140263261390720 learning.py:507] global step 4202: loss = 4.5750 (0.296 sec/step)\n",
            "I0707 17:10:30.426139 140263261390720 learning.py:507] global step 4203: loss = 4.6140 (0.317 sec/step)\n",
            "I0707 17:10:30.746543 140263261390720 learning.py:507] global step 4204: loss = 4.9078 (0.319 sec/step)\n",
            "I0707 17:10:31.126524 140263261390720 learning.py:507] global step 4205: loss = 4.7316 (0.378 sec/step)\n",
            "I0707 17:10:31.435066 140263261390720 learning.py:507] global step 4206: loss = 3.6065 (0.307 sec/step)\n",
            "I0707 17:10:31.863566 140263261390720 learning.py:507] global step 4207: loss = 4.6497 (0.427 sec/step)\n",
            "I0707 17:10:32.163978 140263261390720 learning.py:507] global step 4208: loss = 4.8619 (0.299 sec/step)\n",
            "I0707 17:10:32.546055 140263261390720 learning.py:507] global step 4209: loss = 4.5860 (0.380 sec/step)\n",
            "I0707 17:10:32.886820 140263261390720 learning.py:507] global step 4210: loss = 3.2861 (0.339 sec/step)\n",
            "I0707 17:10:33.390897 140263261390720 learning.py:507] global step 4211: loss = 4.2314 (0.502 sec/step)\n",
            "I0707 17:10:33.777901 140263261390720 learning.py:507] global step 4212: loss = 5.9320 (0.385 sec/step)\n",
            "I0707 17:10:34.161972 140263261390720 learning.py:507] global step 4213: loss = 4.0603 (0.382 sec/step)\n",
            "I0707 17:10:34.465943 140263261390720 learning.py:507] global step 4214: loss = 4.7290 (0.302 sec/step)\n",
            "I0707 17:10:34.989086 140263261390720 learning.py:507] global step 4215: loss = 3.2823 (0.521 sec/step)\n",
            "I0707 17:10:35.329382 140263261390720 learning.py:507] global step 4216: loss = 4.7760 (0.339 sec/step)\n",
            "I0707 17:10:35.735282 140263261390720 learning.py:507] global step 4217: loss = 3.4349 (0.404 sec/step)\n",
            "I0707 17:10:36.075563 140263261390720 learning.py:507] global step 4218: loss = 2.9263 (0.339 sec/step)\n",
            "I0707 17:10:36.393861 140263261390720 learning.py:507] global step 4219: loss = 3.3786 (0.317 sec/step)\n",
            "I0707 17:10:36.706347 140263261390720 learning.py:507] global step 4220: loss = 4.1136 (0.311 sec/step)\n",
            "I0707 17:10:37.097929 140263261390720 learning.py:507] global step 4221: loss = 3.8150 (0.390 sec/step)\n",
            "I0707 17:10:37.395390 140263261390720 learning.py:507] global step 4222: loss = 2.8365 (0.296 sec/step)\n",
            "I0707 17:10:37.689571 140263261390720 learning.py:507] global step 4223: loss = 3.7526 (0.293 sec/step)\n",
            "I0707 17:10:38.066071 140263261390720 learning.py:507] global step 4224: loss = 3.5568 (0.375 sec/step)\n",
            "I0707 17:10:38.381228 140263261390720 learning.py:507] global step 4225: loss = 2.8678 (0.314 sec/step)\n",
            "I0707 17:10:38.684427 140263261390720 learning.py:507] global step 4226: loss = 2.7550 (0.302 sec/step)\n",
            "I0707 17:10:39.193872 140263261390720 learning.py:507] global step 4227: loss = 4.0283 (0.508 sec/step)\n",
            "I0707 17:10:39.489802 140263261390720 learning.py:507] global step 4228: loss = 6.8904 (0.294 sec/step)\n",
            "I0707 17:10:39.787306 140263261390720 learning.py:507] global step 4229: loss = 3.9046 (0.296 sec/step)\n",
            "I0707 17:10:40.123919 140263261390720 learning.py:507] global step 4230: loss = 3.9391 (0.335 sec/step)\n",
            "I0707 17:10:40.429569 140263261390720 learning.py:507] global step 4231: loss = 4.3633 (0.304 sec/step)\n",
            "I0707 17:10:40.733165 140263261390720 learning.py:507] global step 4232: loss = 4.7658 (0.302 sec/step)\n",
            "I0707 17:10:41.254388 140263261390720 learning.py:507] global step 4233: loss = 3.3262 (0.520 sec/step)\n",
            "I0707 17:10:41.544651 140263261390720 learning.py:507] global step 4234: loss = 2.4827 (0.289 sec/step)\n",
            "I0707 17:10:41.848473 140263261390720 learning.py:507] global step 4235: loss = 3.2893 (0.302 sec/step)\n",
            "I0707 17:10:42.160015 140263261390720 learning.py:507] global step 4236: loss = 4.9753 (0.310 sec/step)\n",
            "I0707 17:10:42.490037 140263261390720 learning.py:507] global step 4237: loss = 4.0737 (0.328 sec/step)\n",
            "I0707 17:10:42.896094 140263261390720 learning.py:507] global step 4238: loss = 5.5135 (0.404 sec/step)\n",
            "I0707 17:10:43.237921 140263261390720 learning.py:507] global step 4239: loss = 3.4562 (0.340 sec/step)\n",
            "I0707 17:10:43.597537 140263261390720 learning.py:507] global step 4240: loss = 4.2914 (0.358 sec/step)\n",
            "I0707 17:10:43.961670 140263261390720 learning.py:507] global step 4241: loss = 3.5034 (0.363 sec/step)\n",
            "I0707 17:10:44.285424 140263261390720 learning.py:507] global step 4242: loss = 3.5902 (0.322 sec/step)\n",
            "I0707 17:10:44.582194 140263261390720 learning.py:507] global step 4243: loss = 3.3083 (0.295 sec/step)\n",
            "I0707 17:10:45.007305 140263261390720 learning.py:507] global step 4244: loss = 3.5684 (0.423 sec/step)\n",
            "I0707 17:10:45.327687 140263261390720 learning.py:507] global step 4245: loss = 5.0840 (0.318 sec/step)\n",
            "I0707 17:10:45.704944 140263261390720 learning.py:507] global step 4246: loss = 3.2736 (0.375 sec/step)\n",
            "I0707 17:10:46.188619 140263261390720 learning.py:507] global step 4247: loss = 4.9018 (0.482 sec/step)\n",
            "I0707 17:10:46.533902 140263261390720 learning.py:507] global step 4248: loss = 2.7206 (0.344 sec/step)\n",
            "I0707 17:10:46.854885 140263261390720 learning.py:507] global step 4249: loss = 3.6267 (0.319 sec/step)\n",
            "I0707 17:10:47.353967 140263261390720 learning.py:507] global step 4250: loss = 3.3090 (0.497 sec/step)\n",
            "I0707 17:10:47.659137 140263261390720 learning.py:507] global step 4251: loss = 4.5386 (0.304 sec/step)\n",
            "I0707 17:10:48.072147 140263261390720 learning.py:507] global step 4252: loss = 2.8280 (0.411 sec/step)\n",
            "I0707 17:10:48.400551 140263261390720 learning.py:507] global step 4253: loss = 3.6114 (0.327 sec/step)\n",
            "I0707 17:10:48.719136 140263261390720 learning.py:507] global step 4254: loss = 3.7729 (0.317 sec/step)\n",
            "I0707 17:10:49.016094 140263261390720 learning.py:507] global step 4255: loss = 3.2519 (0.295 sec/step)\n",
            "I0707 17:10:49.347902 140263261390720 learning.py:507] global step 4256: loss = 3.8274 (0.330 sec/step)\n",
            "I0707 17:10:49.694550 140263261390720 learning.py:507] global step 4257: loss = 2.8873 (0.345 sec/step)\n",
            "I0707 17:10:50.039578 140263261390720 learning.py:507] global step 4258: loss = 5.6351 (0.343 sec/step)\n",
            "I0707 17:10:50.334628 140263261390720 learning.py:507] global step 4259: loss = 4.9774 (0.293 sec/step)\n",
            "I0707 17:10:50.724094 140263261390720 learning.py:507] global step 4260: loss = 3.5971 (0.388 sec/step)\n",
            "I0707 17:10:51.024297 140263261390720 learning.py:507] global step 4261: loss = 5.5918 (0.299 sec/step)\n",
            "I0707 17:10:51.338194 140263261390720 learning.py:507] global step 4262: loss = 3.5772 (0.312 sec/step)\n",
            "I0707 17:10:51.701429 140263261390720 learning.py:507] global step 4263: loss = 3.5591 (0.361 sec/step)\n",
            "I0707 17:10:52.064109 140263261390720 learning.py:507] global step 4264: loss = 3.2143 (0.360 sec/step)\n",
            "I0707 17:10:52.392896 140263261390720 learning.py:507] global step 4265: loss = 5.3070 (0.326 sec/step)\n",
            "I0707 17:10:52.725017 140263261390720 learning.py:507] global step 4266: loss = 3.4979 (0.330 sec/step)\n",
            "I0707 17:10:53.020636 140263261390720 learning.py:507] global step 4267: loss = 4.3086 (0.294 sec/step)\n",
            "I0707 17:10:53.386307 140263261390720 learning.py:507] global step 4268: loss = 2.2461 (0.364 sec/step)\n",
            "I0707 17:10:53.735348 140263261390720 learning.py:507] global step 4269: loss = 3.6044 (0.347 sec/step)\n",
            "I0707 17:10:54.046225 140263261390720 learning.py:507] global step 4270: loss = 3.9072 (0.309 sec/step)\n",
            "I0707 17:10:54.383394 140263261390720 learning.py:507] global step 4271: loss = 3.3928 (0.335 sec/step)\n",
            "I0707 17:10:54.725841 140263261390720 learning.py:507] global step 4272: loss = 3.4693 (0.341 sec/step)\n",
            "I0707 17:10:55.027104 140263261390720 learning.py:507] global step 4273: loss = 3.4174 (0.300 sec/step)\n",
            "I0707 17:10:55.402899 140263261390720 learning.py:507] global step 4274: loss = 3.1611 (0.374 sec/step)\n",
            "I0707 17:10:55.741353 140263261390720 learning.py:507] global step 4275: loss = 4.1877 (0.337 sec/step)\n",
            "I0707 17:10:56.063077 140263261390720 learning.py:507] global step 4276: loss = 4.6870 (0.320 sec/step)\n",
            "I0707 17:10:56.443270 140263261390720 learning.py:507] global step 4277: loss = 5.2950 (0.379 sec/step)\n",
            "I0707 17:10:56.760681 140263261390720 learning.py:507] global step 4278: loss = 3.7713 (0.316 sec/step)\n",
            "I0707 17:10:57.067383 140263261390720 learning.py:507] global step 4279: loss = 2.6191 (0.305 sec/step)\n",
            "I0707 17:10:57.606508 140263261390720 learning.py:507] global step 4280: loss = 3.5472 (0.537 sec/step)\n",
            "I0707 17:10:57.960957 140263261390720 learning.py:507] global step 4281: loss = 3.7737 (0.353 sec/step)\n",
            "I0707 17:10:58.292111 140263261390720 learning.py:507] global step 4282: loss = 4.8300 (0.330 sec/step)\n",
            "I0707 17:10:58.652717 140263261390720 learning.py:507] global step 4283: loss = 2.7460 (0.359 sec/step)\n",
            "I0707 17:10:58.973127 140263261390720 learning.py:507] global step 4284: loss = 3.3739 (0.319 sec/step)\n",
            "I0707 17:10:59.314811 140263261390720 learning.py:507] global step 4285: loss = 5.1877 (0.340 sec/step)\n",
            "I0707 17:10:59.932454 140263261390720 learning.py:507] global step 4286: loss = 3.5160 (0.616 sec/step)\n",
            "I0707 17:11:00.259098 140263261390720 learning.py:507] global step 4287: loss = 4.6780 (0.325 sec/step)\n",
            "I0707 17:11:00.606841 140263261390720 learning.py:507] global step 4288: loss = 2.4425 (0.346 sec/step)\n",
            "I0707 17:11:00.932035 140263261390720 learning.py:507] global step 4289: loss = 3.7373 (0.324 sec/step)\n",
            "I0707 17:11:01.244513 140263261390720 learning.py:507] global step 4290: loss = 4.9212 (0.311 sec/step)\n",
            "I0707 17:11:01.589605 140263261390720 learning.py:507] global step 4291: loss = 4.0235 (0.343 sec/step)\n",
            "I0707 17:11:01.882520 140263261390720 learning.py:507] global step 4292: loss = 4.2121 (0.291 sec/step)\n",
            "I0707 17:11:02.186578 140263261390720 learning.py:507] global step 4293: loss = 3.0531 (0.302 sec/step)\n",
            "I0707 17:11:02.621032 140263261390720 learning.py:507] global step 4294: loss = 4.0350 (0.433 sec/step)\n",
            "I0707 17:11:02.949590 140263261390720 learning.py:507] global step 4295: loss = 2.8724 (0.327 sec/step)\n",
            "I0707 17:11:03.250125 140263261390720 learning.py:507] global step 4296: loss = 4.4763 (0.299 sec/step)\n",
            "I0707 17:11:03.621896 140263261390720 learning.py:507] global step 4297: loss = 3.1691 (0.370 sec/step)\n",
            "I0707 17:11:03.922844 140263261390720 learning.py:507] global step 4298: loss = 3.3183 (0.299 sec/step)\n",
            "I0707 17:11:04.237202 140263261390720 learning.py:507] global step 4299: loss = 3.8837 (0.313 sec/step)\n",
            "I0707 17:11:04.542471 140263261390720 learning.py:507] global step 4300: loss = 3.7176 (0.303 sec/step)\n",
            "I0707 17:11:04.842501 140263261390720 learning.py:507] global step 4301: loss = 5.1583 (0.298 sec/step)\n",
            "I0707 17:11:05.251576 140263261390720 learning.py:507] global step 4302: loss = 3.0255 (0.407 sec/step)\n",
            "I0707 17:11:05.596000 140263261390720 learning.py:507] global step 4303: loss = 3.1567 (0.343 sec/step)\n",
            "I0707 17:11:05.926319 140263261390720 learning.py:507] global step 4304: loss = 4.2377 (0.329 sec/step)\n",
            "I0707 17:11:06.235121 140263261390720 learning.py:507] global step 4305: loss = 2.9006 (0.307 sec/step)\n",
            "I0707 17:11:06.529053 140263261390720 learning.py:507] global step 4306: loss = 3.9737 (0.292 sec/step)\n",
            "I0707 17:11:06.837120 140263261390720 learning.py:507] global step 4307: loss = 3.7897 (0.306 sec/step)\n",
            "I0707 17:11:07.132795 140263261390720 learning.py:507] global step 4308: loss = 5.0406 (0.294 sec/step)\n",
            "I0707 17:11:07.431582 140263261390720 learning.py:507] global step 4309: loss = 3.8456 (0.297 sec/step)\n",
            "I0707 17:11:07.750746 140263261390720 learning.py:507] global step 4310: loss = 3.5128 (0.317 sec/step)\n",
            "I0707 17:11:08.053199 140263261390720 learning.py:507] global step 4311: loss = 6.7134 (0.301 sec/step)\n",
            "I0707 17:11:08.382373 140263261390720 learning.py:507] global step 4312: loss = 2.9351 (0.327 sec/step)\n",
            "I0707 17:11:08.693526 140263261390720 learning.py:507] global step 4313: loss = 3.6065 (0.309 sec/step)\n",
            "I0707 17:11:08.996775 140263261390720 learning.py:507] global step 4314: loss = 3.1331 (0.302 sec/step)\n",
            "I0707 17:11:09.332398 140263261390720 learning.py:507] global step 4315: loss = 4.5302 (0.334 sec/step)\n",
            "I0707 17:11:09.644192 140263261390720 learning.py:507] global step 4316: loss = 4.5745 (0.310 sec/step)\n",
            "I0707 17:11:09.946559 140263261390720 learning.py:507] global step 4317: loss = 2.8706 (0.301 sec/step)\n",
            "I0707 17:11:10.236876 140263261390720 learning.py:507] global step 4318: loss = 3.1335 (0.289 sec/step)\n",
            "I0707 17:11:10.578815 140263261390720 learning.py:507] global step 4319: loss = 5.0652 (0.340 sec/step)\n",
            "I0707 17:11:10.885201 140263261390720 learning.py:507] global step 4320: loss = 5.5640 (0.305 sec/step)\n",
            "I0707 17:11:11.249254 140263261390720 learning.py:507] global step 4321: loss = 3.8054 (0.362 sec/step)\n",
            "I0707 17:11:11.733097 140263261390720 learning.py:507] global step 4322: loss = 3.5347 (0.482 sec/step)\n",
            "I0707 17:11:12.046059 140263261390720 learning.py:507] global step 4323: loss = 3.6549 (0.311 sec/step)\n",
            "I0707 17:11:12.381321 140263261390720 learning.py:507] global step 4324: loss = 4.0176 (0.334 sec/step)\n",
            "I0707 17:11:12.694618 140263261390720 learning.py:507] global step 4325: loss = 4.5173 (0.312 sec/step)\n",
            "I0707 17:11:13.002066 140263261390720 learning.py:507] global step 4326: loss = 3.8064 (0.306 sec/step)\n",
            "I0707 17:11:13.317554 140263261390720 learning.py:507] global step 4327: loss = 3.2020 (0.314 sec/step)\n",
            "I0707 17:11:13.916154 140263261390720 learning.py:507] global step 4328: loss = 3.5119 (0.597 sec/step)\n",
            "I0707 17:11:14.205947 140263261390720 learning.py:507] global step 4329: loss = 3.7357 (0.288 sec/step)\n",
            "I0707 17:11:14.530819 140263261390720 learning.py:507] global step 4330: loss = 3.2373 (0.323 sec/step)\n",
            "I0707 17:11:14.888169 140263261390720 learning.py:507] global step 4331: loss = 3.2892 (0.356 sec/step)\n",
            "I0707 17:11:15.209332 140263261390720 learning.py:507] global step 4332: loss = 3.0988 (0.319 sec/step)\n",
            "I0707 17:11:15.520648 140263261390720 learning.py:507] global step 4333: loss = 3.7234 (0.309 sec/step)\n",
            "I0707 17:11:15.821703 140263261390720 learning.py:507] global step 4334: loss = 4.3749 (0.300 sec/step)\n",
            "I0707 17:11:16.128080 140263261390720 learning.py:507] global step 4335: loss = 4.2166 (0.305 sec/step)\n",
            "I0707 17:11:16.426283 140263261390720 learning.py:507] global step 4336: loss = 3.9756 (0.296 sec/step)\n",
            "I0707 17:11:16.756104 140263261390720 learning.py:507] global step 4337: loss = 2.4061 (0.328 sec/step)\n",
            "I0707 17:11:17.105093 140263261390720 learning.py:507] global step 4338: loss = 3.6351 (0.347 sec/step)\n",
            "I0707 17:11:17.428688 140263261390720 learning.py:507] global step 4339: loss = 3.7423 (0.322 sec/step)\n",
            "I0707 17:11:17.808919 140263261390720 learning.py:507] global step 4340: loss = 3.1019 (0.378 sec/step)\n",
            "I0707 17:11:18.135245 140263261390720 learning.py:507] global step 4341: loss = 4.7144 (0.324 sec/step)\n",
            "I0707 17:11:18.452074 140263261390720 learning.py:507] global step 4342: loss = 3.4483 (0.315 sec/step)\n",
            "I0707 17:11:18.755326 140263261390720 learning.py:507] global step 4343: loss = 4.9649 (0.302 sec/step)\n",
            "I0707 17:11:19.100680 140263261390720 learning.py:507] global step 4344: loss = 3.6488 (0.344 sec/step)\n",
            "I0707 17:11:19.412657 140263261390720 learning.py:507] global step 4345: loss = 4.0234 (0.310 sec/step)\n",
            "I0707 17:11:19.712516 140263261390720 learning.py:507] global step 4346: loss = 3.9435 (0.298 sec/step)\n",
            "I0707 17:11:20.036407 140263261390720 learning.py:507] global step 4347: loss = 3.1002 (0.322 sec/step)\n",
            "I0707 17:11:20.338049 140263261390720 learning.py:507] global step 4348: loss = 3.9124 (0.300 sec/step)\n",
            "I0707 17:11:20.643772 140263261390720 learning.py:507] global step 4349: loss = 2.8710 (0.304 sec/step)\n",
            "I0707 17:11:20.985192 140263261390720 learning.py:507] global step 4350: loss = 5.6937 (0.340 sec/step)\n",
            "I0707 17:11:21.425551 140263261390720 learning.py:507] global step 4351: loss = 4.3771 (0.438 sec/step)\n",
            "I0707 17:11:21.734449 140263261390720 learning.py:507] global step 4352: loss = 3.8609 (0.307 sec/step)\n",
            "I0707 17:11:22.050184 140263261390720 learning.py:507] global step 4353: loss = 4.3585 (0.314 sec/step)\n",
            "I0707 17:11:22.336165 140263261390720 learning.py:507] global step 4354: loss = 4.7240 (0.284 sec/step)\n",
            "I0707 17:11:22.678194 140263261390720 learning.py:507] global step 4355: loss = 3.7236 (0.340 sec/step)\n",
            "I0707 17:11:22.982483 140263261390720 learning.py:507] global step 4356: loss = 3.6328 (0.303 sec/step)\n",
            "I0707 17:11:23.320797 140263261390720 learning.py:507] global step 4357: loss = 3.0226 (0.337 sec/step)\n",
            "I0707 17:11:23.627612 140263261390720 learning.py:507] global step 4358: loss = 3.7292 (0.305 sec/step)\n",
            "I0707 17:11:24.067117 140263261390720 learning.py:507] global step 4359: loss = 3.9144 (0.438 sec/step)\n",
            "I0707 17:11:24.404366 140263261390720 learning.py:507] global step 4360: loss = 4.5054 (0.335 sec/step)\n",
            "I0707 17:11:24.713519 140263261390720 learning.py:507] global step 4361: loss = 3.9626 (0.308 sec/step)\n",
            "I0707 17:11:25.015342 140263261390720 learning.py:507] global step 4362: loss = 3.6625 (0.300 sec/step)\n",
            "I0707 17:11:25.334172 140263261390720 learning.py:507] global step 4363: loss = 5.0767 (0.317 sec/step)\n",
            "I0707 17:11:25.626601 140263261390720 learning.py:507] global step 4364: loss = 3.0139 (0.291 sec/step)\n",
            "I0707 17:11:25.952524 140263261390720 learning.py:507] global step 4365: loss = 4.7173 (0.324 sec/step)\n",
            "I0707 17:11:26.274925 140263261390720 learning.py:507] global step 4366: loss = 3.5602 (0.321 sec/step)\n",
            "I0707 17:11:26.569391 140263261390720 learning.py:507] global step 4367: loss = 4.0527 (0.293 sec/step)\n",
            "I0707 17:11:26.912537 140263261390720 learning.py:507] global step 4368: loss = 4.8149 (0.341 sec/step)\n",
            "I0707 17:11:27.256933 140263261390720 learning.py:507] global step 4369: loss = 2.7213 (0.343 sec/step)\n",
            "I0707 17:11:27.699234 140263261390720 learning.py:507] global step 4370: loss = 5.2562 (0.440 sec/step)\n",
            "I0707 17:11:28.008040 140263261390720 learning.py:507] global step 4371: loss = 3.2576 (0.307 sec/step)\n",
            "I0707 17:11:28.320108 140263261390720 learning.py:507] global step 4372: loss = 3.6772 (0.310 sec/step)\n",
            "I0707 17:11:28.677857 140263261390720 learning.py:507] global step 4373: loss = 3.9951 (0.356 sec/step)\n",
            "I0707 17:11:28.981812 140263261390720 learning.py:507] global step 4374: loss = 3.5471 (0.302 sec/step)\n",
            "I0707 17:11:29.390167 140263261390720 learning.py:507] global step 4375: loss = 2.8507 (0.407 sec/step)\n",
            "I0707 17:11:29.772449 140263261390720 learning.py:507] global step 4376: loss = 4.5340 (0.381 sec/step)\n",
            "I0707 17:11:30.100669 140263261390720 learning.py:507] global step 4377: loss = 3.4266 (0.326 sec/step)\n",
            "I0707 17:11:30.527855 140263261390720 learning.py:507] global step 4378: loss = 5.0351 (0.426 sec/step)\n",
            "I0707 17:11:31.051871 140263261390720 learning.py:507] global step 4379: loss = 4.2362 (0.522 sec/step)\n",
            "I0707 17:11:31.349014 140263261390720 learning.py:507] global step 4380: loss = 5.1450 (0.295 sec/step)\n",
            "I0707 17:11:31.725756 140263261390720 learning.py:507] global step 4381: loss = 3.1435 (0.375 sec/step)\n",
            "I0707 17:11:32.029169 140263261390720 learning.py:507] global step 4382: loss = 2.7032 (0.302 sec/step)\n",
            "I0707 17:11:32.367948 140263261390720 learning.py:507] global step 4383: loss = 3.2593 (0.337 sec/step)\n",
            "I0707 17:11:32.696369 140263261390720 learning.py:507] global step 4384: loss = 4.3985 (0.327 sec/step)\n",
            "I0707 17:11:33.220532 140263261390720 learning.py:507] global step 4385: loss = 3.7151 (0.523 sec/step)\n",
            "I0707 17:11:33.527884 140263261390720 learning.py:507] global step 4386: loss = 3.8597 (0.306 sec/step)\n",
            "I0707 17:11:33.951688 140263261390720 learning.py:507] global step 4387: loss = 5.0571 (0.422 sec/step)\n",
            "I0707 17:11:34.262643 140263261390720 learning.py:507] global step 4388: loss = 3.5292 (0.309 sec/step)\n",
            "I0707 17:11:34.579416 140263261390720 learning.py:507] global step 4389: loss = 4.0990 (0.315 sec/step)\n",
            "I0707 17:11:34.886943 140263261390720 learning.py:507] global step 4390: loss = 3.2172 (0.306 sec/step)\n",
            "I0707 17:11:35.187676 140263261390720 learning.py:507] global step 4391: loss = 3.4686 (0.299 sec/step)\n",
            "I0707 17:11:35.486460 140263261390720 learning.py:507] global step 4392: loss = 4.8441 (0.297 sec/step)\n",
            "I0707 17:11:35.854007 140263261390720 learning.py:507] global step 4393: loss = 3.4328 (0.366 sec/step)\n",
            "I0707 17:11:36.171584 140263261390720 learning.py:507] global step 4394: loss = 4.3611 (0.316 sec/step)\n",
            "I0707 17:11:36.490540 140263261390720 learning.py:507] global step 4395: loss = 3.2534 (0.317 sec/step)\n",
            "I0707 17:11:36.778575 140263261390720 learning.py:507] global step 4396: loss = 3.8442 (0.286 sec/step)\n",
            "I0707 17:11:37.143844 140263261390720 learning.py:507] global step 4397: loss = 3.0787 (0.364 sec/step)\n",
            "I0707 17:11:37.614653 140263261390720 learning.py:507] global step 4398: loss = 4.8529 (0.469 sec/step)\n",
            "I0707 17:11:37.959172 140263261390720 learning.py:507] global step 4399: loss = 3.0486 (0.343 sec/step)\n",
            "I0707 17:11:38.268181 140263261390720 learning.py:507] global step 4400: loss = 3.4196 (0.307 sec/step)\n",
            "I0707 17:11:38.616921 140263261390720 learning.py:507] global step 4401: loss = 5.9785 (0.347 sec/step)\n",
            "I0707 17:11:38.936069 140263261390720 learning.py:507] global step 4402: loss = 3.0719 (0.317 sec/step)\n",
            "I0707 17:11:39.263995 140263261390720 learning.py:507] global step 4403: loss = 3.3939 (0.326 sec/step)\n",
            "I0707 17:11:39.796803 140263261390720 learning.py:507] global step 4404: loss = 2.9886 (0.531 sec/step)\n",
            "I0707 17:11:40.094121 140263261390720 learning.py:507] global step 4405: loss = 4.3513 (0.295 sec/step)\n",
            "I0707 17:11:40.478997 140263261390720 learning.py:507] global step 4406: loss = 4.8317 (0.383 sec/step)\n",
            "I0707 17:11:40.988990 140263261390720 learning.py:507] global step 4407: loss = 5.1686 (0.508 sec/step)\n",
            "I0707 17:11:41.299066 140263261390720 learning.py:507] global step 4408: loss = 3.2846 (0.308 sec/step)\n",
            "I0707 17:11:41.594187 140263261390720 learning.py:507] global step 4409: loss = 3.8236 (0.290 sec/step)\n",
            "I0707 17:11:41.947356 140263261390720 learning.py:507] global step 4410: loss = 2.7097 (0.351 sec/step)\n",
            "I0707 17:11:42.249130 140263261390720 learning.py:507] global step 4411: loss = 4.4800 (0.300 sec/step)\n",
            "I0707 17:11:42.640338 140263261390720 learning.py:507] global step 4412: loss = 2.8494 (0.389 sec/step)\n",
            "I0707 17:11:43.294251 140263261390720 learning.py:507] global step 4413: loss = 4.0160 (0.652 sec/step)\n",
            "I0707 17:11:43.615611 140263261390720 learning.py:507] global step 4414: loss = 3.3844 (0.320 sec/step)\n",
            "I0707 17:11:43.927588 140263261390720 learning.py:507] global step 4415: loss = 2.8649 (0.310 sec/step)\n",
            "I0707 17:11:44.221180 140263261390720 learning.py:507] global step 4416: loss = 3.6635 (0.292 sec/step)\n",
            "I0707 17:11:44.557132 140263261390720 learning.py:507] global step 4417: loss = 4.0721 (0.334 sec/step)\n",
            "I0707 17:11:44.883981 140263261390720 learning.py:507] global step 4418: loss = 6.4416 (0.325 sec/step)\n",
            "I0707 17:11:45.302786 140263261390720 learning.py:507] global step 4419: loss = 5.4501 (0.417 sec/step)\n",
            "I0707 17:11:45.648178 140263261390720 learning.py:507] global step 4420: loss = 4.1692 (0.344 sec/step)\n",
            "I0707 17:11:46.087591 140263261390720 learning.py:507] global step 4421: loss = 3.3304 (0.438 sec/step)\n",
            "I0707 17:11:46.374581 140263261390720 learning.py:507] global step 4422: loss = 2.5071 (0.285 sec/step)\n",
            "I0707 17:11:46.727545 140263261390720 learning.py:507] global step 4423: loss = 3.4986 (0.351 sec/step)\n",
            "I0707 17:11:47.031924 140263261390720 learning.py:507] global step 4424: loss = 2.7566 (0.303 sec/step)\n",
            "I0707 17:11:47.368226 140263261390720 learning.py:507] global step 4425: loss = 3.4975 (0.335 sec/step)\n",
            "I0707 17:11:47.668547 140263261390720 learning.py:507] global step 4426: loss = 4.0383 (0.299 sec/step)\n",
            "I0707 17:11:48.127574 140263261390720 learning.py:507] global step 4427: loss = 3.4644 (0.457 sec/step)\n",
            "I0707 17:11:48.430113 140263261390720 learning.py:507] global step 4428: loss = 6.2131 (0.301 sec/step)\n",
            "I0707 17:11:48.762780 140263261390720 learning.py:507] global step 4429: loss = 4.7699 (0.330 sec/step)\n",
            "I0707 17:11:49.206610 140263261390720 learning.py:507] global step 4430: loss = 3.5866 (0.441 sec/step)\n",
            "I0707 17:11:49.528583 140263261390720 learning.py:507] global step 4431: loss = 2.9576 (0.320 sec/step)\n",
            "I0707 17:11:49.830648 140263261390720 learning.py:507] global step 4432: loss = 2.3300 (0.300 sec/step)\n",
            "I0707 17:11:50.132161 140263261390720 learning.py:507] global step 4433: loss = 3.7073 (0.300 sec/step)\n",
            "I0707 17:11:50.456480 140263261390720 learning.py:507] global step 4434: loss = 3.2944 (0.323 sec/step)\n",
            "I0707 17:11:50.737468 140263261390720 learning.py:507] global step 4435: loss = 4.2454 (0.279 sec/step)\n",
            "I0707 17:11:51.095520 140263261390720 learning.py:507] global step 4436: loss = 3.2743 (0.356 sec/step)\n",
            "I0707 17:11:51.397627 140263261390720 learning.py:507] global step 4437: loss = 2.7938 (0.300 sec/step)\n",
            "I0707 17:11:51.711067 140263261390720 learning.py:507] global step 4438: loss = 3.9860 (0.312 sec/step)\n",
            "I0707 17:11:52.035760 140263261390720 learning.py:507] global step 4439: loss = 3.9161 (0.323 sec/step)\n",
            "I0707 17:11:52.371512 140263261390720 learning.py:507] global step 4440: loss = 4.6665 (0.334 sec/step)\n",
            "I0707 17:11:52.706061 140263261390720 learning.py:507] global step 4441: loss = 1.6545 (0.333 sec/step)\n",
            "I0707 17:11:53.016939 140263261390720 learning.py:507] global step 4442: loss = 5.2977 (0.307 sec/step)\n",
            "I0707 17:11:53.674588 140260287772416 supervisor.py:1050] Recording summary at step 4443.\n",
            "I0707 17:11:53.691339 140263261390720 learning.py:507] global step 4443: loss = 3.9862 (0.616 sec/step)\n",
            "I0707 17:11:54.010736 140263261390720 learning.py:507] global step 4444: loss = 3.9905 (0.318 sec/step)\n",
            "I0707 17:11:54.391766 140263261390720 learning.py:507] global step 4445: loss = 4.3663 (0.379 sec/step)\n",
            "I0707 17:11:54.930282 140263261390720 learning.py:507] global step 4446: loss = 3.4964 (0.537 sec/step)\n",
            "I0707 17:11:55.258753 140263261390720 learning.py:507] global step 4447: loss = 3.9300 (0.327 sec/step)\n",
            "I0707 17:11:55.597364 140263261390720 learning.py:507] global step 4448: loss = 3.4715 (0.337 sec/step)\n",
            "I0707 17:11:55.930949 140263261390720 learning.py:507] global step 4449: loss = 3.2713 (0.332 sec/step)\n",
            "I0707 17:11:56.244009 140263261390720 learning.py:507] global step 4450: loss = 2.5838 (0.311 sec/step)\n",
            "I0707 17:11:56.576292 140263261390720 learning.py:507] global step 4451: loss = 4.2315 (0.331 sec/step)\n",
            "I0707 17:11:57.119398 140263261390720 learning.py:507] global step 4452: loss = 3.2144 (0.541 sec/step)\n",
            "I0707 17:11:57.419417 140263261390720 learning.py:507] global step 4453: loss = 3.8937 (0.298 sec/step)\n",
            "I0707 17:11:57.754406 140263261390720 learning.py:507] global step 4454: loss = 2.6432 (0.333 sec/step)\n",
            "I0707 17:11:58.120374 140263261390720 learning.py:507] global step 4455: loss = 2.9288 (0.364 sec/step)\n",
            "I0707 17:11:58.420101 140263261390720 learning.py:507] global step 4456: loss = 4.3787 (0.298 sec/step)\n",
            "I0707 17:11:58.833137 140263261390720 learning.py:507] global step 4457: loss = 3.7700 (0.411 sec/step)\n",
            "I0707 17:11:59.148059 140263261390720 learning.py:507] global step 4458: loss = 4.6082 (0.311 sec/step)\n",
            "I0707 17:11:59.470425 140263261390720 learning.py:507] global step 4459: loss = 3.9989 (0.320 sec/step)\n",
            "I0707 17:11:59.780388 140263261390720 learning.py:507] global step 4460: loss = 5.0416 (0.308 sec/step)\n",
            "I0707 17:12:00.093383 140263261390720 learning.py:507] global step 4461: loss = 4.2247 (0.311 sec/step)\n",
            "I0707 17:12:00.402437 140263261390720 learning.py:507] global step 4462: loss = 3.7479 (0.307 sec/step)\n",
            "I0707 17:12:00.701364 140263261390720 learning.py:507] global step 4463: loss = 3.3766 (0.297 sec/step)\n",
            "I0707 17:12:01.025878 140263261390720 learning.py:507] global step 4464: loss = 4.2273 (0.323 sec/step)\n",
            "I0707 17:12:01.512569 140263261390720 learning.py:507] global step 4465: loss = 4.7352 (0.485 sec/step)\n",
            "I0707 17:12:01.830794 140263261390720 learning.py:507] global step 4466: loss = 3.4362 (0.316 sec/step)\n",
            "I0707 17:12:02.146838 140263261390720 learning.py:507] global step 4467: loss = 6.2958 (0.314 sec/step)\n",
            "I0707 17:12:02.482377 140263261390720 learning.py:507] global step 4468: loss = 4.1542 (0.334 sec/step)\n",
            "I0707 17:12:02.795075 140263261390720 learning.py:507] global step 4469: loss = 3.5314 (0.311 sec/step)\n",
            "I0707 17:12:03.115502 140263261390720 learning.py:507] global step 4470: loss = 3.8524 (0.319 sec/step)\n",
            "I0707 17:12:03.497372 140263261390720 learning.py:507] global step 4471: loss = 3.6310 (0.380 sec/step)\n",
            "I0707 17:12:03.811252 140263261390720 learning.py:507] global step 4472: loss = 4.3295 (0.312 sec/step)\n",
            "I0707 17:12:04.236672 140263261390720 learning.py:507] global step 4473: loss = 3.8522 (0.424 sec/step)\n",
            "I0707 17:12:04.607712 140263261390720 learning.py:507] global step 4474: loss = 3.3373 (0.369 sec/step)\n",
            "I0707 17:12:04.945129 140263261390720 learning.py:507] global step 4475: loss = 3.3226 (0.336 sec/step)\n",
            "I0707 17:12:05.236020 140263261390720 learning.py:507] global step 4476: loss = 2.6557 (0.289 sec/step)\n",
            "I0707 17:12:05.649847 140263261390720 learning.py:507] global step 4477: loss = 3.0320 (0.412 sec/step)\n",
            "I0707 17:12:05.941520 140263261390720 learning.py:507] global step 4478: loss = 3.5812 (0.290 sec/step)\n",
            "I0707 17:12:06.322152 140263261390720 learning.py:507] global step 4479: loss = 5.1240 (0.379 sec/step)\n",
            "I0707 17:12:06.673364 140263261390720 learning.py:507] global step 4480: loss = 3.8809 (0.350 sec/step)\n",
            "I0707 17:12:07.031663 140263261390720 learning.py:507] global step 4481: loss = 4.8880 (0.357 sec/step)\n",
            "I0707 17:12:07.323931 140263261390720 learning.py:507] global step 4482: loss = 4.5083 (0.290 sec/step)\n",
            "I0707 17:12:07.641863 140263261390720 learning.py:507] global step 4483: loss = 3.2821 (0.316 sec/step)\n",
            "I0707 17:12:07.960030 140263261390720 learning.py:507] global step 4484: loss = 5.0969 (0.316 sec/step)\n",
            "I0707 17:12:08.324932 140263261390720 learning.py:507] global step 4485: loss = 4.1138 (0.363 sec/step)\n",
            "I0707 17:12:08.645512 140263261390720 learning.py:507] global step 4486: loss = 2.3909 (0.319 sec/step)\n",
            "I0707 17:12:09.045626 140263261390720 learning.py:507] global step 4487: loss = 3.0918 (0.398 sec/step)\n",
            "I0707 17:12:09.350617 140263261390720 learning.py:507] global step 4488: loss = 3.2381 (0.303 sec/step)\n",
            "I0707 17:12:09.675997 140263261390720 learning.py:507] global step 4489: loss = 2.9737 (0.324 sec/step)\n",
            "I0707 17:12:09.983483 140263261390720 learning.py:507] global step 4490: loss = 4.0502 (0.306 sec/step)\n",
            "I0707 17:12:10.369693 140263261390720 learning.py:507] global step 4491: loss = 3.0762 (0.384 sec/step)\n",
            "I0707 17:12:10.689183 140263261390720 learning.py:507] global step 4492: loss = 5.0746 (0.318 sec/step)\n",
            "I0707 17:12:11.197115 140263261390720 learning.py:507] global step 4493: loss = 3.4782 (0.506 sec/step)\n",
            "I0707 17:12:11.494470 140263261390720 learning.py:507] global step 4494: loss = 3.3950 (0.296 sec/step)\n",
            "I0707 17:12:11.829840 140263261390720 learning.py:507] global step 4495: loss = 3.2541 (0.334 sec/step)\n",
            "I0707 17:12:12.119435 140263261390720 learning.py:507] global step 4496: loss = 2.6278 (0.288 sec/step)\n",
            "I0707 17:12:12.446066 140263261390720 learning.py:507] global step 4497: loss = 3.2878 (0.325 sec/step)\n",
            "I0707 17:12:12.757719 140263261390720 learning.py:507] global step 4498: loss = 3.7727 (0.310 sec/step)\n",
            "I0707 17:12:13.235746 140263261390720 learning.py:507] global step 4499: loss = 3.3815 (0.476 sec/step)\n",
            "I0707 17:12:13.568870 140263261390720 learning.py:507] global step 4500: loss = 4.5087 (0.331 sec/step)\n",
            "I0707 17:12:13.877590 140263261390720 learning.py:507] global step 4501: loss = 3.8624 (0.307 sec/step)\n",
            "I0707 17:12:14.198877 140263261390720 learning.py:507] global step 4502: loss = 3.6506 (0.319 sec/step)\n",
            "I0707 17:12:14.520842 140263261390720 learning.py:507] global step 4503: loss = 2.8164 (0.320 sec/step)\n",
            "I0707 17:12:14.852529 140263261390720 learning.py:507] global step 4504: loss = 3.9798 (0.330 sec/step)\n",
            "I0707 17:12:15.161866 140263261390720 learning.py:507] global step 4505: loss = 5.3182 (0.307 sec/step)\n",
            "I0707 17:12:15.466913 140263261390720 learning.py:507] global step 4506: loss = 4.3772 (0.303 sec/step)\n",
            "I0707 17:12:15.863526 140263261390720 learning.py:507] global step 4507: loss = 6.4165 (0.395 sec/step)\n",
            "I0707 17:12:16.164294 140263261390720 learning.py:507] global step 4508: loss = 3.2613 (0.299 sec/step)\n",
            "I0707 17:12:16.476508 140263261390720 learning.py:507] global step 4509: loss = 3.0845 (0.311 sec/step)\n",
            "I0707 17:12:16.930510 140263261390720 learning.py:507] global step 4510: loss = 4.0686 (0.452 sec/step)\n",
            "I0707 17:12:17.258742 140263261390720 learning.py:507] global step 4511: loss = 4.4050 (0.326 sec/step)\n",
            "I0707 17:12:17.577960 140263261390720 learning.py:507] global step 4512: loss = 5.3746 (0.317 sec/step)\n",
            "I0707 17:12:18.022370 140263261390720 learning.py:507] global step 4513: loss = 2.9620 (0.443 sec/step)\n",
            "I0707 17:12:18.327608 140263261390720 learning.py:507] global step 4514: loss = 3.3814 (0.304 sec/step)\n",
            "I0707 17:12:18.646363 140263261390720 learning.py:507] global step 4515: loss = 4.9574 (0.317 sec/step)\n",
            "I0707 17:12:18.954428 140263261390720 learning.py:507] global step 4516: loss = 3.7817 (0.306 sec/step)\n",
            "I0707 17:12:19.291286 140263261390720 learning.py:507] global step 4517: loss = 4.4078 (0.335 sec/step)\n",
            "I0707 17:12:19.601140 140263261390720 learning.py:507] global step 4518: loss = 4.6999 (0.308 sec/step)\n",
            "I0707 17:12:20.007190 140263261390720 learning.py:507] global step 4519: loss = 3.4996 (0.404 sec/step)\n",
            "I0707 17:12:20.302663 140263261390720 learning.py:507] global step 4520: loss = 4.2263 (0.294 sec/step)\n",
            "I0707 17:12:20.621693 140263261390720 learning.py:507] global step 4521: loss = 3.7975 (0.317 sec/step)\n",
            "I0707 17:12:20.920433 140263261390720 learning.py:507] global step 4522: loss = 2.2296 (0.297 sec/step)\n",
            "I0707 17:12:21.252485 140263261390720 learning.py:507] global step 4523: loss = 3.5576 (0.330 sec/step)\n",
            "I0707 17:12:21.570439 140263261390720 learning.py:507] global step 4524: loss = 4.3792 (0.316 sec/step)\n",
            "I0707 17:12:21.917081 140263261390720 learning.py:507] global step 4525: loss = 3.2802 (0.345 sec/step)\n",
            "I0707 17:12:22.219742 140263261390720 learning.py:507] global step 4526: loss = 3.8950 (0.301 sec/step)\n",
            "I0707 17:12:22.522153 140263261390720 learning.py:507] global step 4527: loss = 5.6304 (0.301 sec/step)\n",
            "I0707 17:12:22.823178 140263261390720 learning.py:507] global step 4528: loss = 3.6769 (0.299 sec/step)\n",
            "I0707 17:12:23.130667 140263261390720 learning.py:507] global step 4529: loss = 4.3254 (0.306 sec/step)\n",
            "I0707 17:12:23.445544 140263261390720 learning.py:507] global step 4530: loss = 1.8102 (0.313 sec/step)\n",
            "I0707 17:12:23.795197 140263261390720 learning.py:507] global step 4531: loss = 3.0572 (0.348 sec/step)\n",
            "I0707 17:12:24.112154 140263261390720 learning.py:507] global step 4532: loss = 2.4342 (0.315 sec/step)\n",
            "I0707 17:12:24.410692 140263261390720 learning.py:507] global step 4533: loss = 3.0865 (0.297 sec/step)\n",
            "I0707 17:12:24.749281 140263261390720 learning.py:507] global step 4534: loss = 3.4505 (0.337 sec/step)\n",
            "I0707 17:12:25.123428 140263261390720 learning.py:507] global step 4535: loss = 5.0621 (0.372 sec/step)\n",
            "I0707 17:12:25.481450 140263261390720 learning.py:507] global step 4536: loss = 5.1327 (0.356 sec/step)\n",
            "I0707 17:12:25.797141 140263261390720 learning.py:507] global step 4537: loss = 2.7218 (0.314 sec/step)\n",
            "I0707 17:12:26.104815 140263261390720 learning.py:507] global step 4538: loss = 4.5110 (0.306 sec/step)\n",
            "I0707 17:12:26.530548 140263261390720 learning.py:507] global step 4539: loss = 3.4223 (0.424 sec/step)\n",
            "I0707 17:12:26.859568 140263261390720 learning.py:507] global step 4540: loss = 3.5062 (0.327 sec/step)\n",
            "I0707 17:12:27.176985 140263261390720 learning.py:507] global step 4541: loss = 3.3008 (0.316 sec/step)\n",
            "I0707 17:12:27.527941 140263261390720 learning.py:507] global step 4542: loss = 3.9537 (0.349 sec/step)\n",
            "I0707 17:12:27.860304 140263261390720 learning.py:507] global step 4543: loss = 3.4420 (0.331 sec/step)\n",
            "I0707 17:12:28.224679 140263261390720 learning.py:507] global step 4544: loss = 5.6873 (0.363 sec/step)\n",
            "I0707 17:12:28.666042 140263261390720 learning.py:507] global step 4545: loss = 3.3470 (0.440 sec/step)\n",
            "I0707 17:12:28.952632 140263261390720 learning.py:507] global step 4546: loss = 5.1910 (0.285 sec/step)\n",
            "I0707 17:12:29.309821 140263261390720 learning.py:507] global step 4547: loss = 5.3141 (0.355 sec/step)\n",
            "I0707 17:12:29.626569 140263261390720 learning.py:507] global step 4548: loss = 3.4658 (0.315 sec/step)\n",
            "I0707 17:12:29.927500 140263261390720 learning.py:507] global step 4549: loss = 5.4077 (0.299 sec/step)\n",
            "I0707 17:12:30.242700 140263261390720 learning.py:507] global step 4550: loss = 5.2291 (0.313 sec/step)\n",
            "I0707 17:12:30.552504 140263261390720 learning.py:507] global step 4551: loss = 7.1554 (0.308 sec/step)\n",
            "I0707 17:12:30.886445 140263261390720 learning.py:507] global step 4552: loss = 3.3093 (0.332 sec/step)\n",
            "I0707 17:12:31.171432 140263261390720 learning.py:507] global step 4553: loss = 3.8783 (0.283 sec/step)\n",
            "I0707 17:12:31.491561 140263261390720 learning.py:507] global step 4554: loss = 3.8321 (0.318 sec/step)\n",
            "I0707 17:12:31.841436 140263261390720 learning.py:507] global step 4555: loss = 2.2932 (0.348 sec/step)\n",
            "I0707 17:12:32.138788 140263261390720 learning.py:507] global step 4556: loss = 4.4583 (0.296 sec/step)\n",
            "I0707 17:12:32.475955 140263261390720 learning.py:507] global step 4557: loss = 2.5737 (0.336 sec/step)\n",
            "I0707 17:12:32.774045 140263261390720 learning.py:507] global step 4558: loss = 3.7346 (0.297 sec/step)\n",
            "I0707 17:12:33.064233 140263261390720 learning.py:507] global step 4559: loss = 3.4371 (0.288 sec/step)\n",
            "I0707 17:12:33.383012 140263261390720 learning.py:507] global step 4560: loss = 4.6477 (0.316 sec/step)\n",
            "I0707 17:12:33.688337 140263261390720 learning.py:507] global step 4561: loss = 3.8233 (0.304 sec/step)\n",
            "I0707 17:12:34.054954 140263261390720 learning.py:507] global step 4562: loss = 3.1441 (0.365 sec/step)\n",
            "I0707 17:12:34.434346 140263261390720 learning.py:507] global step 4563: loss = 3.1667 (0.378 sec/step)\n",
            "I0707 17:12:34.720673 140263261390720 learning.py:507] global step 4564: loss = 3.5366 (0.285 sec/step)\n",
            "I0707 17:12:35.025030 140263261390720 learning.py:507] global step 4565: loss = 3.4122 (0.303 sec/step)\n",
            "I0707 17:12:35.352480 140263261390720 learning.py:507] global step 4566: loss = 2.2566 (0.326 sec/step)\n",
            "I0707 17:12:35.679219 140263261390720 learning.py:507] global step 4567: loss = 4.3980 (0.325 sec/step)\n",
            "I0707 17:12:36.083518 140263261390720 learning.py:507] global step 4568: loss = 3.1133 (0.403 sec/step)\n",
            "I0707 17:12:36.423283 140263261390720 learning.py:507] global step 4569: loss = 4.0842 (0.338 sec/step)\n",
            "I0707 17:12:36.748914 140263261390720 learning.py:507] global step 4570: loss = 4.8477 (0.324 sec/step)\n",
            "I0707 17:12:37.051664 140263261390720 learning.py:507] global step 4571: loss = 4.6860 (0.301 sec/step)\n",
            "I0707 17:12:37.431267 140263261390720 learning.py:507] global step 4572: loss = 5.7185 (0.378 sec/step)\n",
            "I0707 17:12:37.785593 140263261390720 learning.py:507] global step 4573: loss = 5.4531 (0.353 sec/step)\n",
            "I0707 17:12:38.098072 140263261390720 learning.py:507] global step 4574: loss = 4.0253 (0.310 sec/step)\n",
            "I0707 17:12:38.462646 140263261390720 learning.py:507] global step 4575: loss = 3.3947 (0.363 sec/step)\n",
            "I0707 17:12:38.835262 140263261390720 learning.py:507] global step 4576: loss = 2.7252 (0.371 sec/step)\n",
            "I0707 17:12:39.127456 140263261390720 learning.py:507] global step 4577: loss = 2.5381 (0.291 sec/step)\n",
            "I0707 17:12:39.518481 140263261390720 learning.py:507] global step 4578: loss = 3.0766 (0.389 sec/step)\n",
            "I0707 17:12:39.837048 140263261390720 learning.py:507] global step 4579: loss = 3.1178 (0.317 sec/step)\n",
            "I0707 17:12:40.133942 140263261390720 learning.py:507] global step 4580: loss = 3.1693 (0.295 sec/step)\n",
            "I0707 17:12:40.425760 140263261390720 learning.py:507] global step 4581: loss = 3.0438 (0.289 sec/step)\n",
            "I0707 17:12:40.755517 140263261390720 learning.py:507] global step 4582: loss = 3.6234 (0.326 sec/step)\n",
            "I0707 17:12:41.133511 140263261390720 learning.py:507] global step 4583: loss = 3.8043 (0.377 sec/step)\n",
            "I0707 17:12:41.445462 140263261390720 learning.py:507] global step 4584: loss = 3.5217 (0.309 sec/step)\n",
            "I0707 17:12:41.773162 140263261390720 learning.py:507] global step 4585: loss = 3.0491 (0.326 sec/step)\n",
            "I0707 17:12:42.093610 140263261390720 learning.py:507] global step 4586: loss = 4.2159 (0.319 sec/step)\n",
            "I0707 17:12:42.399765 140263261390720 learning.py:507] global step 4587: loss = 4.1840 (0.304 sec/step)\n",
            "I0707 17:12:42.699660 140263261390720 learning.py:507] global step 4588: loss = 5.2555 (0.298 sec/step)\n",
            "I0707 17:12:43.006363 140263261390720 learning.py:507] global step 4589: loss = 4.2874 (0.305 sec/step)\n",
            "I0707 17:12:43.367239 140263261390720 learning.py:507] global step 4590: loss = 4.0885 (0.359 sec/step)\n",
            "I0707 17:12:43.686686 140263261390720 learning.py:507] global step 4591: loss = 3.8535 (0.318 sec/step)\n",
            "I0707 17:12:44.075567 140263261390720 learning.py:507] global step 4592: loss = 2.0765 (0.387 sec/step)\n",
            "I0707 17:12:44.378325 140263261390720 learning.py:507] global step 4593: loss = 3.3103 (0.301 sec/step)\n",
            "I0707 17:12:44.685467 140263261390720 learning.py:507] global step 4594: loss = 4.0045 (0.305 sec/step)\n",
            "I0707 17:12:45.001060 140263261390720 learning.py:507] global step 4595: loss = 4.2450 (0.314 sec/step)\n",
            "I0707 17:12:45.356805 140263261390720 learning.py:507] global step 4596: loss = 2.9747 (0.354 sec/step)\n",
            "I0707 17:12:45.701116 140263261390720 learning.py:507] global step 4597: loss = 3.9069 (0.343 sec/step)\n",
            "I0707 17:12:46.118160 140263261390720 learning.py:507] global step 4598: loss = 2.3999 (0.415 sec/step)\n",
            "I0707 17:12:46.441581 140263261390720 learning.py:507] global step 4599: loss = 3.9313 (0.322 sec/step)\n",
            "I0707 17:12:46.789339 140263261390720 learning.py:507] global step 4600: loss = 4.2012 (0.346 sec/step)\n",
            "I0707 17:12:47.115298 140263261390720 learning.py:507] global step 4601: loss = 3.4397 (0.324 sec/step)\n",
            "I0707 17:12:47.524736 140263261390720 learning.py:507] global step 4602: loss = 3.0503 (0.408 sec/step)\n",
            "I0707 17:12:47.880129 140263261390720 learning.py:507] global step 4603: loss = 4.6248 (0.354 sec/step)\n",
            "I0707 17:12:48.176596 140263261390720 learning.py:507] global step 4604: loss = 4.5421 (0.295 sec/step)\n",
            "I0707 17:12:48.510701 140263261390720 learning.py:507] global step 4605: loss = 3.8095 (0.332 sec/step)\n",
            "I0707 17:12:48.998788 140263261390720 learning.py:507] global step 4606: loss = 4.1447 (0.487 sec/step)\n",
            "I0707 17:12:49.306307 140263261390720 learning.py:507] global step 4607: loss = 3.6384 (0.306 sec/step)\n",
            "I0707 17:12:49.603802 140263261390720 learning.py:507] global step 4608: loss = 4.8010 (0.296 sec/step)\n",
            "I0707 17:12:50.012989 140263261390720 learning.py:507] global step 4609: loss = 3.2660 (0.407 sec/step)\n",
            "I0707 17:12:50.336226 140263261390720 learning.py:507] global step 4610: loss = 5.4367 (0.322 sec/step)\n",
            "I0707 17:12:50.646505 140263261390720 learning.py:507] global step 4611: loss = 4.1986 (0.309 sec/step)\n",
            "I0707 17:12:50.956134 140263261390720 learning.py:507] global step 4612: loss = 3.4322 (0.308 sec/step)\n",
            "I0707 17:12:51.270304 140263261390720 learning.py:507] global step 4613: loss = 3.6681 (0.313 sec/step)\n",
            "I0707 17:12:51.716859 140263261390720 learning.py:507] global step 4614: loss = 2.5513 (0.445 sec/step)\n",
            "I0707 17:12:52.070989 140263261390720 learning.py:507] global step 4615: loss = 4.4814 (0.352 sec/step)\n",
            "I0707 17:12:52.373079 140263261390720 learning.py:507] global step 4616: loss = 3.1582 (0.300 sec/step)\n",
            "I0707 17:12:52.796422 140263261390720 learning.py:507] global step 4617: loss = 3.8968 (0.422 sec/step)\n",
            "I0707 17:12:53.120418 140263261390720 learning.py:507] global step 4618: loss = 2.9592 (0.322 sec/step)\n",
            "I0707 17:12:53.446836 140263261390720 learning.py:507] global step 4619: loss = 3.5349 (0.324 sec/step)\n",
            "I0707 17:12:53.799810 140263261390720 learning.py:507] global step 4620: loss = 3.1471 (0.351 sec/step)\n",
            "I0707 17:12:54.105976 140263261390720 learning.py:507] global step 4621: loss = 4.3906 (0.304 sec/step)\n",
            "I0707 17:12:54.420341 140263261390720 learning.py:507] global step 4622: loss = 4.1251 (0.313 sec/step)\n",
            "I0707 17:12:54.739880 140263261390720 learning.py:507] global step 4623: loss = 4.2215 (0.318 sec/step)\n",
            "I0707 17:12:55.077618 140263261390720 learning.py:507] global step 4624: loss = 2.9693 (0.336 sec/step)\n",
            "I0707 17:12:55.483344 140263261390720 learning.py:507] global step 4625: loss = 3.2940 (0.404 sec/step)\n",
            "I0707 17:12:55.888987 140263261390720 learning.py:507] global step 4626: loss = 2.6292 (0.404 sec/step)\n",
            "I0707 17:12:56.230257 140263261390720 learning.py:507] global step 4627: loss = 3.1547 (0.340 sec/step)\n",
            "I0707 17:12:56.624293 140263261390720 learning.py:507] global step 4628: loss = 3.3628 (0.392 sec/step)\n",
            "I0707 17:12:56.957010 140263261390720 learning.py:507] global step 4629: loss = 3.4221 (0.331 sec/step)\n",
            "I0707 17:12:57.300807 140263261390720 learning.py:507] global step 4630: loss = 4.4825 (0.342 sec/step)\n",
            "I0707 17:12:57.652158 140263261390720 learning.py:507] global step 4631: loss = 3.4491 (0.350 sec/step)\n",
            "I0707 17:12:57.990626 140263261390720 learning.py:507] global step 4632: loss = 4.3849 (0.335 sec/step)\n",
            "I0707 17:12:58.300263 140263261390720 learning.py:507] global step 4633: loss = 2.7937 (0.308 sec/step)\n",
            "I0707 17:12:58.727817 140263261390720 learning.py:507] global step 4634: loss = 3.3101 (0.426 sec/step)\n",
            "I0707 17:12:59.099799 140263261390720 learning.py:507] global step 4635: loss = 2.7843 (0.370 sec/step)\n",
            "I0707 17:12:59.546669 140263261390720 learning.py:507] global step 4636: loss = 3.1475 (0.445 sec/step)\n",
            "I0707 17:12:59.865396 140263261390720 learning.py:507] global step 4637: loss = 4.1485 (0.317 sec/step)\n",
            "I0707 17:13:00.187345 140263261390720 learning.py:507] global step 4638: loss = 4.9714 (0.320 sec/step)\n",
            "I0707 17:13:00.516623 140263261390720 learning.py:507] global step 4639: loss = 3.3091 (0.328 sec/step)\n",
            "I0707 17:13:00.993237 140263261390720 learning.py:507] global step 4640: loss = 3.4349 (0.475 sec/step)\n",
            "I0707 17:13:01.374456 140263261390720 learning.py:507] global step 4641: loss = 2.9861 (0.379 sec/step)\n",
            "I0707 17:13:01.780664 140263261390720 learning.py:507] global step 4642: loss = 2.8329 (0.405 sec/step)\n",
            "I0707 17:13:02.110215 140263261390720 learning.py:507] global step 4643: loss = 3.7718 (0.328 sec/step)\n",
            "I0707 17:13:02.422805 140263261390720 learning.py:507] global step 4644: loss = 5.6049 (0.311 sec/step)\n",
            "I0707 17:13:02.766122 140263261390720 learning.py:507] global step 4645: loss = 3.9563 (0.342 sec/step)\n",
            "I0707 17:13:03.137065 140263261390720 learning.py:507] global step 4646: loss = 4.6720 (0.369 sec/step)\n",
            "I0707 17:13:03.467950 140263261390720 learning.py:507] global step 4647: loss = 3.2310 (0.329 sec/step)\n",
            "I0707 17:13:03.770156 140263261390720 learning.py:507] global step 4648: loss = 4.6576 (0.300 sec/step)\n",
            "I0707 17:13:04.085273 140263261390720 learning.py:507] global step 4649: loss = 3.1789 (0.313 sec/step)\n",
            "I0707 17:13:04.499698 140263261390720 learning.py:507] global step 4650: loss = 4.5888 (0.413 sec/step)\n",
            "I0707 17:13:04.925317 140263261390720 learning.py:507] global step 4651: loss = 3.6496 (0.424 sec/step)\n",
            "I0707 17:13:05.350035 140263261390720 learning.py:507] global step 4652: loss = 3.7199 (0.423 sec/step)\n",
            "I0707 17:13:05.650655 140263261390720 learning.py:507] global step 4653: loss = 4.0464 (0.299 sec/step)\n",
            "I0707 17:13:05.938266 140263261390720 learning.py:507] global step 4654: loss = 3.3087 (0.286 sec/step)\n",
            "I0707 17:13:06.276381 140263261390720 learning.py:507] global step 4655: loss = 4.0162 (0.336 sec/step)\n",
            "I0707 17:13:06.679946 140263261390720 learning.py:507] global step 4656: loss = 4.5814 (0.401 sec/step)\n",
            "I0707 17:13:07.040353 140263261390720 learning.py:507] global step 4657: loss = 3.0129 (0.359 sec/step)\n",
            "I0707 17:13:07.395902 140263261390720 learning.py:507] global step 4658: loss = 4.0569 (0.354 sec/step)\n",
            "I0707 17:13:07.692650 140263261390720 learning.py:507] global step 4659: loss = 3.3699 (0.295 sec/step)\n",
            "I0707 17:13:07.999971 140263261390720 learning.py:507] global step 4660: loss = 4.1408 (0.306 sec/step)\n",
            "I0707 17:13:08.381675 140263261390720 learning.py:507] global step 4661: loss = 4.3415 (0.380 sec/step)\n",
            "I0707 17:13:08.865154 140263261390720 learning.py:507] global step 4662: loss = 3.1440 (0.482 sec/step)\n",
            "I0707 17:13:09.325924 140263261390720 learning.py:507] global step 4663: loss = 2.7302 (0.459 sec/step)\n",
            "I0707 17:13:09.628482 140263261390720 learning.py:507] global step 4664: loss = 3.6818 (0.301 sec/step)\n",
            "I0707 17:13:09.938887 140263261390720 learning.py:507] global step 4665: loss = 2.6392 (0.309 sec/step)\n",
            "I0707 17:13:10.271936 140263261390720 learning.py:507] global step 4666: loss = 3.1932 (0.331 sec/step)\n",
            "I0707 17:13:10.632445 140263261390720 learning.py:507] global step 4667: loss = 3.2882 (0.359 sec/step)\n",
            "I0707 17:13:10.930165 140263261390720 learning.py:507] global step 4668: loss = 3.3540 (0.296 sec/step)\n",
            "I0707 17:13:11.281594 140263261390720 learning.py:507] global step 4669: loss = 4.7525 (0.350 sec/step)\n",
            "I0707 17:13:11.682114 140263261390720 learning.py:507] global step 4670: loss = 2.5977 (0.399 sec/step)\n",
            "I0707 17:13:11.985715 140263261390720 learning.py:507] global step 4671: loss = 4.5606 (0.302 sec/step)\n",
            "I0707 17:13:12.403540 140263261390720 learning.py:507] global step 4672: loss = 3.7839 (0.416 sec/step)\n",
            "I0707 17:13:12.765544 140263261390720 learning.py:507] global step 4673: loss = 3.6337 (0.360 sec/step)\n",
            "I0707 17:13:13.105366 140263261390720 learning.py:507] global step 4674: loss = 2.3678 (0.338 sec/step)\n",
            "I0707 17:13:13.423541 140263261390720 learning.py:507] global step 4675: loss = 3.9030 (0.317 sec/step)\n",
            "I0707 17:13:13.890199 140263261390720 learning.py:507] global step 4676: loss = 4.5650 (0.465 sec/step)\n",
            "I0707 17:13:14.186353 140263261390720 learning.py:507] global step 4677: loss = 2.0912 (0.294 sec/step)\n",
            "I0707 17:13:14.612485 140263261390720 learning.py:507] global step 4678: loss = 3.5439 (0.424 sec/step)\n",
            "I0707 17:13:14.953121 140263261390720 learning.py:507] global step 4679: loss = 2.7591 (0.339 sec/step)\n",
            "I0707 17:13:15.355229 140263261390720 learning.py:507] global step 4680: loss = 4.7924 (0.400 sec/step)\n",
            "I0707 17:13:15.665281 140263261390720 learning.py:507] global step 4681: loss = 4.8260 (0.308 sec/step)\n",
            "I0707 17:13:15.985421 140263261390720 learning.py:507] global step 4682: loss = 3.7699 (0.318 sec/step)\n",
            "I0707 17:13:16.269009 140263261390720 learning.py:507] global step 4683: loss = 3.4447 (0.282 sec/step)\n",
            "I0707 17:13:16.602545 140263261390720 learning.py:507] global step 4684: loss = 3.9080 (0.330 sec/step)\n",
            "I0707 17:13:16.889428 140263261390720 learning.py:507] global step 4685: loss = 3.7854 (0.285 sec/step)\n",
            "I0707 17:13:17.381246 140263261390720 learning.py:507] global step 4686: loss = 3.3648 (0.490 sec/step)\n",
            "I0707 17:13:17.712079 140263261390720 learning.py:507] global step 4687: loss = 2.6500 (0.329 sec/step)\n",
            "I0707 17:13:18.091254 140263261390720 learning.py:507] global step 4688: loss = 4.1100 (0.377 sec/step)\n",
            "I0707 17:13:18.381134 140263261390720 learning.py:507] global step 4689: loss = 5.2575 (0.288 sec/step)\n",
            "I0707 17:13:18.718044 140263261390720 learning.py:507] global step 4690: loss = 4.4965 (0.335 sec/step)\n",
            "I0707 17:13:19.005074 140263261390720 learning.py:507] global step 4691: loss = 3.1576 (0.285 sec/step)\n",
            "I0707 17:13:19.474116 140263261390720 learning.py:507] global step 4692: loss = 3.0638 (0.467 sec/step)\n",
            "I0707 17:13:19.788713 140263261390720 learning.py:507] global step 4693: loss = 3.4245 (0.313 sec/step)\n",
            "I0707 17:13:20.118178 140263261390720 learning.py:507] global step 4694: loss = 3.1324 (0.327 sec/step)\n",
            "I0707 17:13:20.463468 140263261390720 learning.py:507] global step 4695: loss = 3.0830 (0.344 sec/step)\n",
            "I0707 17:13:20.869410 140263261390720 learning.py:507] global step 4696: loss = 3.3574 (0.404 sec/step)\n",
            "I0707 17:13:21.173493 140263261390720 learning.py:507] global step 4697: loss = 4.7019 (0.302 sec/step)\n",
            "I0707 17:13:21.522480 140263261390720 learning.py:507] global step 4698: loss = 4.8011 (0.347 sec/step)\n",
            "I0707 17:13:21.943268 140263261390720 learning.py:507] global step 4699: loss = 4.0839 (0.419 sec/step)\n",
            "I0707 17:13:22.307771 140263261390720 learning.py:507] global step 4700: loss = 3.1942 (0.363 sec/step)\n",
            "I0707 17:13:22.620390 140263261390720 learning.py:507] global step 4701: loss = 2.8929 (0.311 sec/step)\n",
            "I0707 17:13:22.934867 140263261390720 learning.py:507] global step 4702: loss = 4.8312 (0.313 sec/step)\n",
            "I0707 17:13:23.308248 140263261390720 learning.py:507] global step 4703: loss = 3.8546 (0.372 sec/step)\n",
            "I0707 17:13:23.733139 140263261390720 learning.py:507] global step 4704: loss = 3.4680 (0.423 sec/step)\n",
            "I0707 17:13:24.124305 140263261390720 learning.py:507] global step 4705: loss = 2.6858 (0.389 sec/step)\n",
            "I0707 17:13:24.485223 140263261390720 learning.py:507] global step 4706: loss = 3.9108 (0.359 sec/step)\n",
            "I0707 17:13:25.072186 140263261390720 learning.py:507] global step 4707: loss = 4.6024 (0.585 sec/step)\n",
            "I0707 17:13:25.502794 140263261390720 learning.py:507] global step 4708: loss = 4.5222 (0.429 sec/step)\n",
            "I0707 17:13:25.801155 140263261390720 learning.py:507] global step 4709: loss = 2.4550 (0.297 sec/step)\n",
            "I0707 17:13:26.163703 140263261390720 learning.py:507] global step 4710: loss = 5.6835 (0.361 sec/step)\n",
            "I0707 17:13:26.489620 140263261390720 learning.py:507] global step 4711: loss = 4.4922 (0.324 sec/step)\n",
            "I0707 17:13:26.904941 140263261390720 learning.py:507] global step 4712: loss = 3.0863 (0.413 sec/step)\n",
            "I0707 17:13:27.215316 140263261390720 learning.py:507] global step 4713: loss = 4.1411 (0.309 sec/step)\n",
            "I0707 17:13:27.696932 140263261390720 learning.py:507] global step 4714: loss = 3.0728 (0.480 sec/step)\n",
            "I0707 17:13:28.087392 140263261390720 learning.py:507] global step 4715: loss = 4.2250 (0.389 sec/step)\n",
            "I0707 17:13:28.480239 140263261390720 learning.py:507] global step 4716: loss = 3.3770 (0.391 sec/step)\n",
            "I0707 17:13:28.792583 140263261390720 learning.py:507] global step 4717: loss = 3.7018 (0.311 sec/step)\n",
            "I0707 17:13:29.147271 140263261390720 learning.py:507] global step 4718: loss = 2.0114 (0.353 sec/step)\n",
            "I0707 17:13:29.463084 140263261390720 learning.py:507] global step 4719: loss = 4.5245 (0.314 sec/step)\n",
            "I0707 17:13:29.759367 140263261390720 learning.py:507] global step 4720: loss = 3.5418 (0.295 sec/step)\n",
            "I0707 17:13:30.122594 140263261390720 learning.py:507] global step 4721: loss = 3.3206 (0.362 sec/step)\n",
            "I0707 17:13:30.441579 140263261390720 learning.py:507] global step 4722: loss = 5.3116 (0.317 sec/step)\n",
            "I0707 17:13:30.799950 140263261390720 learning.py:507] global step 4723: loss = 3.3698 (0.357 sec/step)\n",
            "I0707 17:13:31.172451 140263261390720 learning.py:507] global step 4724: loss = 4.3251 (0.371 sec/step)\n",
            "I0707 17:13:31.486770 140263261390720 learning.py:507] global step 4725: loss = 2.6730 (0.313 sec/step)\n",
            "I0707 17:13:31.798391 140263261390720 learning.py:507] global step 4726: loss = 4.2795 (0.310 sec/step)\n",
            "I0707 17:13:32.207781 140263261390720 learning.py:507] global step 4727: loss = 3.2514 (0.408 sec/step)\n",
            "I0707 17:13:32.520949 140263261390720 learning.py:507] global step 4728: loss = 2.5283 (0.312 sec/step)\n",
            "I0707 17:13:32.919430 140263261390720 learning.py:507] global step 4729: loss = 5.0538 (0.396 sec/step)\n",
            "I0707 17:13:33.327873 140263261390720 learning.py:507] global step 4730: loss = 4.0695 (0.406 sec/step)\n",
            "I0707 17:13:33.634228 140263261390720 learning.py:507] global step 4731: loss = 3.9278 (0.305 sec/step)\n",
            "I0707 17:13:33.973648 140263261390720 learning.py:507] global step 4732: loss = 3.5882 (0.338 sec/step)\n",
            "I0707 17:13:34.448310 140263261390720 learning.py:507] global step 4733: loss = 3.6617 (0.473 sec/step)\n",
            "I0707 17:13:34.783001 140263261390720 learning.py:507] global step 4734: loss = 3.4579 (0.333 sec/step)\n",
            "I0707 17:13:35.151183 140263261390720 learning.py:507] global step 4735: loss = 2.9100 (0.366 sec/step)\n",
            "I0707 17:13:35.471278 140263261390720 learning.py:507] global step 4736: loss = 3.0530 (0.318 sec/step)\n",
            "I0707 17:13:35.782122 140263261390720 learning.py:507] global step 4737: loss = 3.9363 (0.309 sec/step)\n",
            "I0707 17:13:36.091860 140263261390720 learning.py:507] global step 4738: loss = 3.5803 (0.308 sec/step)\n",
            "I0707 17:13:36.445985 140263261390720 learning.py:507] global step 4739: loss = 3.1788 (0.352 sec/step)\n",
            "I0707 17:13:36.745076 140263261390720 learning.py:507] global step 4740: loss = 3.4159 (0.297 sec/step)\n",
            "I0707 17:13:37.151059 140263261390720 learning.py:507] global step 4741: loss = 3.3987 (0.404 sec/step)\n",
            "I0707 17:13:37.490745 140263261390720 learning.py:507] global step 4742: loss = 3.0872 (0.338 sec/step)\n",
            "I0707 17:13:37.800254 140263261390720 learning.py:507] global step 4743: loss = 3.3736 (0.308 sec/step)\n",
            "I0707 17:13:38.124091 140263261390720 learning.py:507] global step 4744: loss = 2.8782 (0.322 sec/step)\n",
            "I0707 17:13:38.459759 140263261390720 learning.py:507] global step 4745: loss = 4.4901 (0.334 sec/step)\n",
            "I0707 17:13:38.768684 140263261390720 learning.py:507] global step 4746: loss = 5.2284 (0.307 sec/step)\n",
            "I0707 17:13:39.078197 140263261390720 learning.py:507] global step 4747: loss = 4.1555 (0.308 sec/step)\n",
            "I0707 17:13:39.489784 140263261390720 learning.py:507] global step 4748: loss = 4.9236 (0.410 sec/step)\n",
            "I0707 17:13:39.780375 140263261390720 learning.py:507] global step 4749: loss = 2.9850 (0.289 sec/step)\n",
            "I0707 17:13:40.099496 140263261390720 learning.py:507] global step 4750: loss = 4.0886 (0.317 sec/step)\n",
            "I0707 17:13:40.393412 140263261390720 learning.py:507] global step 4751: loss = 3.0356 (0.292 sec/step)\n",
            "I0707 17:13:40.761125 140263261390720 learning.py:507] global step 4752: loss = 4.1207 (0.366 sec/step)\n",
            "I0707 17:13:41.104486 140263261390720 learning.py:507] global step 4753: loss = 3.2120 (0.342 sec/step)\n",
            "I0707 17:13:41.434422 140263261390720 learning.py:507] global step 4754: loss = 3.1651 (0.328 sec/step)\n",
            "I0707 17:13:41.742545 140263261390720 learning.py:507] global step 4755: loss = 3.3755 (0.306 sec/step)\n",
            "I0707 17:13:42.043428 140263261390720 learning.py:507] global step 4756: loss = 4.2208 (0.299 sec/step)\n",
            "I0707 17:13:42.486814 140263261390720 learning.py:507] global step 4757: loss = 3.4736 (0.442 sec/step)\n",
            "I0707 17:13:42.839610 140263261390720 learning.py:507] global step 4758: loss = 4.9669 (0.351 sec/step)\n",
            "I0707 17:13:43.161474 140263261390720 learning.py:507] global step 4759: loss = 2.3059 (0.320 sec/step)\n",
            "I0707 17:13:43.501954 140263261390720 learning.py:507] global step 4760: loss = 2.9966 (0.339 sec/step)\n",
            "I0707 17:13:43.818947 140263261390720 learning.py:507] global step 4761: loss = 4.4282 (0.315 sec/step)\n",
            "I0707 17:13:44.129774 140263261390720 learning.py:507] global step 4762: loss = 3.6280 (0.309 sec/step)\n",
            "I0707 17:13:44.475498 140263261390720 learning.py:507] global step 4763: loss = 5.1609 (0.344 sec/step)\n",
            "I0707 17:13:44.786469 140263261390720 learning.py:507] global step 4764: loss = 3.4442 (0.309 sec/step)\n",
            "I0707 17:13:45.085292 140263261390720 learning.py:507] global step 4765: loss = 2.8475 (0.297 sec/step)\n",
            "I0707 17:13:45.444848 140263261390720 learning.py:507] global step 4766: loss = 3.8407 (0.358 sec/step)\n",
            "I0707 17:13:45.774301 140263261390720 learning.py:507] global step 4767: loss = 4.1816 (0.325 sec/step)\n",
            "I0707 17:13:46.065409 140263261390720 learning.py:507] global step 4768: loss = 4.6526 (0.289 sec/step)\n",
            "I0707 17:13:46.397854 140263261390720 learning.py:507] global step 4769: loss = 3.7296 (0.331 sec/step)\n",
            "I0707 17:13:46.735141 140263261390720 learning.py:507] global step 4770: loss = 2.8641 (0.335 sec/step)\n",
            "I0707 17:13:47.042497 140263261390720 learning.py:507] global step 4771: loss = 4.3585 (0.306 sec/step)\n",
            "I0707 17:13:47.461318 140263261390720 learning.py:507] global step 4772: loss = 4.5909 (0.417 sec/step)\n",
            "I0707 17:13:47.810926 140263261390720 learning.py:507] global step 4773: loss = 6.2609 (0.348 sec/step)\n",
            "I0707 17:13:48.127107 140263261390720 learning.py:507] global step 4774: loss = 2.8644 (0.314 sec/step)\n",
            "I0707 17:13:48.437965 140263261390720 learning.py:507] global step 4775: loss = 3.3095 (0.309 sec/step)\n",
            "I0707 17:13:48.736628 140263261390720 learning.py:507] global step 4776: loss = 6.5049 (0.296 sec/step)\n",
            "I0707 17:13:49.084313 140263261390720 learning.py:507] global step 4777: loss = 3.3742 (0.346 sec/step)\n",
            "I0707 17:13:49.458255 140263261390720 learning.py:507] global step 4778: loss = 4.1981 (0.372 sec/step)\n",
            "I0707 17:13:49.837477 140263261390720 learning.py:507] global step 4779: loss = 3.0211 (0.378 sec/step)\n",
            "I0707 17:13:50.182046 140263261390720 learning.py:507] global step 4780: loss = 3.5260 (0.343 sec/step)\n",
            "I0707 17:13:50.520866 140263261390720 learning.py:507] global step 4781: loss = 3.5299 (0.337 sec/step)\n",
            "I0707 17:13:50.843918 140263261390720 learning.py:507] global step 4782: loss = 2.9253 (0.321 sec/step)\n",
            "I0707 17:13:51.240732 140263261390720 learning.py:507] global step 4783: loss = 4.0336 (0.395 sec/step)\n",
            "I0707 17:13:51.562277 140263261390720 learning.py:507] global step 4784: loss = 3.6358 (0.320 sec/step)\n",
            "I0707 17:13:51.891396 140263261390720 learning.py:507] global step 4785: loss = 2.8009 (0.327 sec/step)\n",
            "I0707 17:13:52.223783 140263261390720 learning.py:507] global step 4786: loss = 4.1329 (0.331 sec/step)\n",
            "I0707 17:13:52.551479 140263261390720 learning.py:507] global step 4787: loss = 3.4921 (0.326 sec/step)\n",
            "I0707 17:13:52.949771 140263261390720 learning.py:507] global step 4788: loss = 3.6682 (0.396 sec/step)\n",
            "I0707 17:13:53.480499 140263261390720 learning.py:507] global step 4789: loss = 3.7889 (0.521 sec/step)\n",
            "I0707 17:13:53.675603 140260287772416 supervisor.py:1050] Recording summary at step 4789.\n",
            "I0707 17:13:53.927092 140263261390720 learning.py:507] global step 4790: loss = 5.7240 (0.328 sec/step)\n",
            "I0707 17:13:54.225643 140263261390720 learning.py:507] global step 4791: loss = 2.3859 (0.297 sec/step)\n",
            "I0707 17:13:54.554103 140263261390720 learning.py:507] global step 4792: loss = 3.0619 (0.326 sec/step)\n",
            "I0707 17:13:54.930032 140263261390720 learning.py:507] global step 4793: loss = 3.9491 (0.374 sec/step)\n",
            "I0707 17:13:55.214182 140263261390720 learning.py:507] global step 4794: loss = 3.3949 (0.283 sec/step)\n",
            "I0707 17:13:55.519555 140263261390720 learning.py:507] global step 4795: loss = 2.4500 (0.304 sec/step)\n",
            "I0707 17:13:55.951259 140263261390720 learning.py:507] global step 4796: loss = 3.9249 (0.430 sec/step)\n",
            "I0707 17:13:56.252055 140263261390720 learning.py:507] global step 4797: loss = 3.3547 (0.299 sec/step)\n",
            "I0707 17:13:56.565893 140263261390720 learning.py:507] global step 4798: loss = 4.8181 (0.312 sec/step)\n",
            "I0707 17:13:56.925601 140263261390720 learning.py:507] global step 4799: loss = 3.0975 (0.358 sec/step)\n",
            "I0707 17:13:57.460030 140263261390720 learning.py:507] global step 4800: loss = 3.0815 (0.533 sec/step)\n",
            "I0707 17:13:57.863995 140263261390720 learning.py:507] global step 4801: loss = 3.2684 (0.402 sec/step)\n",
            "I0707 17:13:58.169013 140263261390720 learning.py:507] global step 4802: loss = 4.9027 (0.303 sec/step)\n",
            "I0707 17:13:58.490168 140263261390720 learning.py:507] global step 4803: loss = 2.4415 (0.319 sec/step)\n",
            "I0707 17:13:58.822546 140263261390720 learning.py:507] global step 4804: loss = 3.7093 (0.331 sec/step)\n",
            "I0707 17:13:59.225306 140263261390720 learning.py:507] global step 4805: loss = 4.5664 (0.401 sec/step)\n",
            "I0707 17:14:00.071183 140263261390720 learning.py:507] global step 4806: loss = 3.6841 (0.844 sec/step)\n",
            "I0707 17:14:00.395020 140263261390720 learning.py:507] global step 4807: loss = 3.5647 (0.322 sec/step)\n",
            "I0707 17:14:00.695726 140263261390720 learning.py:507] global step 4808: loss = 4.5655 (0.299 sec/step)\n",
            "I0707 17:14:00.996812 140263261390720 learning.py:507] global step 4809: loss = 4.4209 (0.299 sec/step)\n",
            "I0707 17:14:01.410081 140263261390720 learning.py:507] global step 4810: loss = 4.4253 (0.411 sec/step)\n",
            "I0707 17:14:01.727799 140263261390720 learning.py:507] global step 4811: loss = 2.8370 (0.316 sec/step)\n",
            "I0707 17:14:02.061857 140263261390720 learning.py:507] global step 4812: loss = 2.9665 (0.332 sec/step)\n",
            "I0707 17:14:02.426029 140263261390720 learning.py:507] global step 4813: loss = 4.2051 (0.362 sec/step)\n",
            "I0707 17:14:02.824506 140263261390720 learning.py:507] global step 4814: loss = 4.0362 (0.397 sec/step)\n",
            "I0707 17:14:03.172857 140263261390720 learning.py:507] global step 4815: loss = 4.6029 (0.347 sec/step)\n",
            "I0707 17:14:03.480319 140263261390720 learning.py:507] global step 4816: loss = 3.8504 (0.306 sec/step)\n",
            "I0707 17:14:03.783744 140263261390720 learning.py:507] global step 4817: loss = 3.6627 (0.302 sec/step)\n",
            "I0707 17:14:04.113359 140263261390720 learning.py:507] global step 4818: loss = 4.7008 (0.328 sec/step)\n",
            "I0707 17:14:04.503453 140263261390720 learning.py:507] global step 4819: loss = 4.1914 (0.388 sec/step)\n",
            "I0707 17:14:04.802294 140263261390720 learning.py:507] global step 4820: loss = 3.8030 (0.297 sec/step)\n",
            "I0707 17:14:05.109572 140263261390720 learning.py:507] global step 4821: loss = 4.2511 (0.306 sec/step)\n",
            "I0707 17:14:05.400803 140263261390720 learning.py:507] global step 4822: loss = 5.0993 (0.289 sec/step)\n",
            "I0707 17:14:05.823044 140263261390720 learning.py:507] global step 4823: loss = 5.3542 (0.420 sec/step)\n",
            "I0707 17:14:06.143242 140263261390720 learning.py:507] global step 4824: loss = 4.8480 (0.319 sec/step)\n",
            "I0707 17:14:06.547001 140263261390720 learning.py:507] global step 4825: loss = 4.9752 (0.402 sec/step)\n",
            "I0707 17:14:06.853691 140263261390720 learning.py:507] global step 4826: loss = 6.8125 (0.305 sec/step)\n",
            "I0707 17:14:07.150153 140263261390720 learning.py:507] global step 4827: loss = 3.9941 (0.295 sec/step)\n",
            "I0707 17:14:07.449234 140263261390720 learning.py:507] global step 4828: loss = 4.1599 (0.297 sec/step)\n",
            "I0707 17:14:07.742126 140263261390720 learning.py:507] global step 4829: loss = 4.3826 (0.291 sec/step)\n",
            "I0707 17:14:08.230067 140263261390720 learning.py:507] global step 4830: loss = 3.8369 (0.486 sec/step)\n",
            "I0707 17:14:08.732464 140263261390720 learning.py:507] global step 4831: loss = 4.8193 (0.501 sec/step)\n",
            "I0707 17:14:09.148794 140263261390720 learning.py:507] global step 4832: loss = 5.5155 (0.414 sec/step)\n",
            "I0707 17:14:09.460142 140263261390720 learning.py:507] global step 4833: loss = 2.7847 (0.310 sec/step)\n",
            "I0707 17:14:09.883039 140263261390720 learning.py:507] global step 4834: loss = 3.8011 (0.421 sec/step)\n",
            "I0707 17:14:10.170279 140263261390720 learning.py:507] global step 4835: loss = 3.8948 (0.286 sec/step)\n",
            "I0707 17:14:10.778485 140263261390720 learning.py:507] global step 4836: loss = 3.0463 (0.607 sec/step)\n",
            "I0707 17:14:11.068711 140263261390720 learning.py:507] global step 4837: loss = 3.1466 (0.289 sec/step)\n",
            "I0707 17:14:11.378639 140263261390720 learning.py:507] global step 4838: loss = 3.6867 (0.308 sec/step)\n",
            "I0707 17:14:11.723806 140263261390720 learning.py:507] global step 4839: loss = 3.9124 (0.343 sec/step)\n",
            "I0707 17:14:12.218290 140263261390720 learning.py:507] global step 4840: loss = 5.2134 (0.493 sec/step)\n",
            "I0707 17:14:12.539983 140263261390720 learning.py:507] global step 4841: loss = 3.4743 (0.320 sec/step)\n",
            "I0707 17:14:12.855701 140263261390720 learning.py:507] global step 4842: loss = 4.1663 (0.314 sec/step)\n",
            "I0707 17:14:13.168982 140263261390720 learning.py:507] global step 4843: loss = 5.6921 (0.311 sec/step)\n",
            "I0707 17:14:13.512048 140263261390720 learning.py:507] global step 4844: loss = 2.8346 (0.341 sec/step)\n",
            "I0707 17:14:13.859333 140263261390720 learning.py:507] global step 4845: loss = 3.6110 (0.346 sec/step)\n",
            "I0707 17:14:14.159908 140263261390720 learning.py:507] global step 4846: loss = 4.5878 (0.299 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 184, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"train.py\", line 180, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/legacy/trainer.py\", line 416, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 490, in train_step\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}